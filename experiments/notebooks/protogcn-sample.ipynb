{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport os.path as osp\nimport torch\nfrom torch_geometric.data import Dataset, Data\nfrom torch_geometric.loader import DataLoader\nimport cv2\nimport mediapipe as mp\nimport numpy as np\nfrom typing import List, Tuple\nimport json\nimport warnings\n\n# Suppress MediaPipe warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n\ndef compute_edge_features(x: torch.Tensor, edge_index: torch.Tensor, \n                         availability_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute edge features (distance) and filter edges based on availability.\n    \n    Args:\n        x: Node features of shape (T, N, 3) where 3 = (x, y, confidence)\n        edge_index: Edge connectivity of shape (2, num_edges)\n        availability_mask: Availability mask of shape (T, N)\n        \n    Returns:\n        filtered_edge_index: Edge index with unavailable nodes removed, shape (2, num_valid_edges)\n        edge_attr: Edge features (distance) of shape (num_valid_edges, T)\n    \"\"\"\n    T, N, _ = x.shape\n    num_edges = edge_index.size(1)\n    \n    # Extract source and target nodes\n    source_nodes = edge_index[0]  # (num_edges,)\n    target_nodes = edge_index[1]  # (num_edges,)\n    \n    # Get coordinates for all frames\n    source_coords = x[:, source_nodes, :2]  # (T, num_edges, 2)\n    target_coords = x[:, target_nodes, :2]  # (T, num_edges, 2)\n    \n    # Compute Euclidean distance for each edge at each frame\n    distances = torch.norm(target_coords - source_coords, dim=2)  # (T, num_edges)\n    \n    # Check availability: edge is valid only if both nodes are available\n    source_available = availability_mask[:, source_nodes]  # (T, num_edges)\n    target_available = availability_mask[:, target_nodes]  # (T, num_edges)\n    edge_available = source_available * target_available  # (T, num_edges)\n    \n    # An edge is valid if it's available in at least some frames\n    edge_valid = edge_available.sum(dim=0) > 0  # (num_edges,)\n    \n    # Filter edges\n    filtered_edge_index = edge_index[:, edge_valid]  # (2, num_valid_edges)\n    filtered_distances = distances[:, edge_valid]  # (T, num_valid_edges)\n    filtered_availability = edge_available[:, edge_valid]  # (T, num_valid_edges)\n    \n    # Set distance to 0 where edge is not available in that frame\n    filtered_distances = filtered_distances * filtered_availability\n    \n    # Transpose to get (num_valid_edges, T)\n    edge_attr = filtered_distances.t()  # (num_valid_edges, T)\n    \n    return filtered_edge_index, edge_attr\n\n\ndef create_edge_index(n_pose: int, n_face: int, n_hand: int) -> torch.Tensor:\n    \"\"\"\n    Create edge index based on actual landmark counts.\n    \n    Args:\n        n_pose: Number of pose landmarks (12 for upper body)\n        n_face: Number of face landmarks (lips + eyebrows)\n        n_hand: Number of hand landmarks (21 per hand)\n        \n    Returns:\n        edge_index: Tensor of shape (2, num_edges)\n    \"\"\"\n    edges = []\n    \n    # POSE CONNECTIONS (upper body: shoulders, elbows, wrists, hips)\n    pose_connections = [\n        (0, 1),   # Left shoulder to right shoulder\n        (0, 2),   # Left shoulder to left elbow  \n        (2, 4),   # Left elbow to left wrist\n        (1, 3),   # Right shoulder to right elbow\n        (3, 5),   # Right elbow to right wrist\n        (0, 6),   # Left shoulder to left hip\n        (1, 7),   # Right shoulder to right hip\n        (6, 7),   # Left hip to right hip\n        (6, 8),   # Left hip to left knee\n        (7, 9),   # Right hip to right knee\n        (8, 10),  # Left knee to left ankle\n        (9, 11),  # Right knee to right ankle\n    ]\n    \n    for i, j in pose_connections:\n        if i < n_pose and j < n_pose:\n            edges.append((i, j))\n    \n    # FACE CONNECTIONS (sequential for lips and eyebrows)\n    face_start = n_pose\n    for i in range(n_face - 1):\n        edges.append((face_start + i, face_start + i + 1))\n    # Close the loop\n    if n_face > 0:\n        edges.append((face_start, face_start + n_face - 1))\n    \n    # LEFT HAND CONNECTIONS\n    left_hand_start = n_pose + n_face\n    hand_connections = [\n        (0, 1), (1, 2), (2, 3), (3, 4),          # Thumb\n        (0, 5), (5, 6), (6, 7), (7, 8),          # Index\n        (0, 9), (9, 10), (10, 11), (11, 12),     # Middle\n        (0, 13), (13, 14), (14, 15), (15, 16),   # Ring\n        (0, 17), (17, 18), (18, 19), (19, 20),   # Pinky\n        (5, 9), (9, 13), (13, 17),               # Palm\n    ]\n    \n    for i, j in hand_connections:\n        edges.append((left_hand_start + i, left_hand_start + j))\n    \n    # RIGHT HAND CONNECTIONS\n    right_hand_start = left_hand_start + n_hand\n    for i, j in hand_connections:\n        edges.append((right_hand_start + i, right_hand_start + j))\n    \n    # Convert to bidirectional edges\n    bidirectional_edges = []\n    for i, j in edges:\n        bidirectional_edges.append([i, j])\n        bidirectional_edges.append([j, i])\n    \n    edge_index = torch.tensor(bidirectional_edges, dtype=torch.long).t().contiguous()\n    return edge_index\n\n\ndef extract_features_from_video(video_path: str) -> Tuple[np.ndarray, np.ndarray, Tuple[int, int, int]]:\n    \"\"\"\n    Extract pose features from a video file (optimized version).\n    \n    Returns:\n        features: Array of shape (T, N, 3) - x, y, confidence\n        availability_mask: Array of shape (T, N) - 1 if keypoint available, 0 if missing\n        counts: Tuple of (n_pose, n_face, n_hand)\n    \"\"\"\n    mp_holistic = mp.solutions.holistic\n    mp_face_mesh = mp.solutions.face_mesh\n    \n    # Define landmark indices\n    POSE_INDICES = list(range(11, 23))  # Upper body\n    \n    face_connections = (\n        mp_face_mesh.FACEMESH_LIPS | \n        mp_face_mesh.FACEMESH_LEFT_EYEBROW | \n        mp_face_mesh.FACEMESH_RIGHT_EYEBROW\n    )\n    FACE_INDICES = sorted(set([idx for pair in face_connections for idx in pair]))\n    HAND_INDICES = list(range(21))\n    \n    N_POSE = len(POSE_INDICES)\n    N_FACE = len(FACE_INDICES)\n    N_HAND = len(HAND_INDICES)\n    TOTAL_N = N_POSE + N_FACE + (N_HAND * 2)\n    \n    # Load video - optimized to read frame count first\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise ValueError(f\"Cannot open video: {video_path}\")\n    \n    # Get frame count and pre-allocate\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Pre-allocate arrays for all frames\n    features = np.zeros((frame_count, TOTAL_N, 3), dtype=np.float32)\n    availability_mask = np.zeros((frame_count, TOTAL_N), dtype=np.float32)\n    \n    # Process frames with holistic model\n    holistic = mp_holistic.Holistic(\n        static_image_mode=False,\n        model_complexity=1,\n        min_detection_confidence=0.5,\n        min_tracking_confidence=0.5,\n        enable_segmentation=False,  # Disable segmentation for speed\n        refine_face_landmarks=False  # Disable face refinement for speed\n    )\n    \n    t = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Convert to RGB once\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = holistic.process(rgb_frame)\n        \n        idx = 0\n        \n        # Pose - vectorized approach\n        if results.pose_landmarks:\n            for pose_idx in POSE_INDICES:\n                lm = results.pose_landmarks.landmark[pose_idx]\n                features[t, idx] = [lm.x, lm.y, lm.visibility]\n                availability_mask[t, idx] = 1.0 if lm.visibility > 0.5 else 0.0\n                idx += 1\n        else:\n            idx += N_POSE\n        \n        # Face\n        if results.face_landmarks:\n            for face_idx in FACE_INDICES:\n                lm = results.face_landmarks.landmark[face_idx]\n                features[t, idx] = [lm.x, lm.y, 1.0]\n                availability_mask[t, idx] = 1.0\n                idx += 1\n        else:\n            idx += N_FACE\n        \n        # Left hand\n        if results.left_hand_landmarks:\n            for hand_idx in HAND_INDICES:\n                lm = results.left_hand_landmarks.landmark[hand_idx]\n                features[t, idx] = [lm.x, lm.y, 1.0]\n                availability_mask[t, idx] = 1.0\n                idx += 1\n        else:\n            idx += N_HAND\n        \n        # Right hand\n        if results.right_hand_landmarks:\n            for hand_idx in HAND_INDICES:\n                lm = results.right_hand_landmarks.landmark[hand_idx]\n                features[t, idx] = [lm.x, lm.y, 1.0]\n                availability_mask[t, idx] = 1.0\n                idx += 1\n        else:\n            idx += N_HAND\n        \n        t += 1\n    \n    cap.release()\n    holistic.close()\n    \n    # Trim arrays if actual frame count differs\n    if t < frame_count:\n        features = features[:t]\n        availability_mask = availability_mask[:t]\n    \n    return features, availability_mask, (N_POSE, N_FACE, N_HAND)\n\n\nclass SignLanguageDataset(Dataset):\n    \"\"\"\n    PyTorch Geometric Dataset for Sign Language Recognition.\n    \n    Each sample is a temporal graph with:\n    - x: Node features of shape (T, N, 3) where T=frames, N=keypoints, 3=(x,y,confidence)\n    - edge_index: Graph connectivity of shape (2, num_edges)\n    - y: Label (sign class)\n    \n    Directory structure:\n        root/\n            raw/\n                video1.mp4\n                video2.mp4\n                ...\n                labels.json  # {\"video1.mp4\": 0, \"video2.mp4\": 1, ...}\n            processed/\n                data_0.pt\n                data_1.pt\n                ...\n    \"\"\"\n    \n    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n        super().__init__(root, transform, pre_transform, pre_filter)\n        \n    @property\n    def raw_file_names(self) -> List[str]:\n        \"\"\"Return list of raw video files.\"\"\"\n        raw_dir = self.raw_dir\n        if not osp.exists(raw_dir):\n            return []\n        \n        video_files = [f for f in os.listdir(raw_dir) \n                      if f.endswith(('.mp4', '.avi', '.mov'))]\n        return sorted(video_files)\n    \n    @property\n    def processed_file_names(self) -> List[str]:\n        \"\"\"Return list of processed .pt files.\"\"\"\n        return [f'data_{i}.pt' for i in range(len(self.raw_file_names))]\n    \n    def download(self):\n        \"\"\"Download dataset (implement if needed).\"\"\"\n        pass\n    \n    def process(self):\n        \"\"\"Process raw videos into graph data.\"\"\"\n        # Load labels\n        labels_path = osp.join(self.raw_dir, 'labels.json')\n        if osp.exists(labels_path):\n            with open(labels_path, 'r') as f:\n                labels = json.load(f)\n        else:\n            print(\"Warning: labels.json not found, using default label 0\")\n            labels = {}\n        \n        idx = 0\n        for raw_path in self.raw_paths:\n            if raw_path.endswith('labels.json'):\n                continue\n            \n            print(f\"Processing {osp.basename(raw_path)}...\")\n            \n            try:\n                # Extract features\n                features, availability_mask, (n_pose, n_face, n_hand) = extract_features_from_video(raw_path)\n                \n                # Create initial edge index (all possible edges)\n                edge_index = create_edge_index(n_pose, n_face, n_hand)\n                \n                # Convert to tensors\n                x = torch.from_numpy(features).float()  # Shape: (T, N, 3)\n                availability_mask_tensor = torch.from_numpy(availability_mask).float()  # (T, N)\n                \n                # Compute edge features and filter unavailable edges\n                filtered_edge_index, edge_attr = compute_edge_features(\n                    x, edge_index, availability_mask_tensor\n                )\n                \n                # Get label\n                video_name = osp.basename(raw_path)\n                y = torch.tensor([labels.get(video_name, 0)], dtype=torch.long)\n                \n                # Create Data object\n                data = Data(\n                    x=x,  # (T, N, 3) - node features: x, y, confidence\n                    edge_index=filtered_edge_index,  # (2, num_valid_edges)\n                    edge_attr=edge_attr,  # (num_valid_edges, T) - distance features\n                    availability_mask=availability_mask_tensor,  # (T, N) - 1 if available, 0 if missing\n                    y=y,  # (1,)\n                    num_frames=x.size(0),\n                    num_nodes=x.size(1)\n                )\n                \n                # Apply filters and transforms\n                if self.pre_filter is not None and not self.pre_filter(data):\n                    continue\n                \n                if self.pre_transform is not None:\n                    data = self.pre_transform(data)\n                \n                # Save with optimized settings\n                torch.save(\n                    data, \n                    osp.join(self.processed_dir, f'data_{idx}.pt'),\n                    _use_new_zipfile_serialization=True  # Use optimized serialization\n                )\n                idx += 1\n                \n            except Exception as e:\n                print(f\"Error processing {raw_path}: {e}\")\n                continue\n    \n    def len(self) -> int:\n        \"\"\"Return number of samples.\"\"\"\n        return len(self.processed_file_names)\n    \n    def get(self, idx: int) -> Data:\n        \"\"\"Load and return a single graph.\"\"\"\n        data = torch.load(\n            osp.join(self.processed_dir, f'data_{idx}.pt'),\n            weights_only=False  # Required for PyG Data objects\n        )\n        return data","metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, global_mean_pool, global_add_pool\nfrom torch_geometric.data import Data, Batch\nimport math\n\n\nclass MotionTopologyEnhancement(nn.Module):\n    \"\"\"\n    Motion Topology Enhancement (MTE) module to capture rich motion representations.\n    Learns dynamic graph topology based on motion features.\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        \n        # Learnable matrices for topology modeling\n        self.theta = nn.Linear(in_channels, out_channels)\n        self.phi = nn.Linear(in_channels, out_channels)\n        \n        # Graph convolution\n        self.gcn = GCNConv(in_channels, out_channels)\n        \n    def forward(self, x, edge_index):\n        \"\"\"\n        Args:\n            x: Node features (num_nodes, in_channels)\n            edge_index: Graph connectivity (2, num_edges)\n        \"\"\"\n        # Compute attention-based topology\n        theta_x = self.theta(x)  # (num_nodes, out_channels)\n        phi_x = self.phi(x)      # (num_nodes, out_channels)\n        \n        # Apply GCN with original topology\n        out = self.gcn(x, edge_index)\n        \n        return out\n\n\nclass PrototypeReconstructionNetwork(nn.Module):\n    \"\"\"\n    Prototype Reconstruction Network (PRN) that decomposes features\n    into learnable prototypes representing motion patterns.\n    \"\"\"\n    def __init__(self, feature_dim, num_prototypes=128, prototype_dim=256):\n        super().__init__()\n        self.num_prototypes = num_prototypes\n        self.prototype_dim = prototype_dim\n        \n        # Learnable prototype memory\n        self.prototypes = nn.Parameter(torch.randn(num_prototypes, prototype_dim))\n        nn.init.xavier_uniform_(self.prototypes)\n        \n        # Feature projection\n        self.feature_proj = nn.Sequential(\n            nn.Linear(feature_dim, prototype_dim),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Reconstruction projection\n        self.recon_proj = nn.Linear(prototype_dim, feature_dim)\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Input features (batch_size, feature_dim)\n        Returns:\n            reconstructed: Reconstructed features\n            proto_scores: Prototype assignment scores\n        \"\"\"\n        # Handle single sample case\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n        \n        batch_size = x.size(0)\n        \n        # Project features to prototype space\n        x_proj = self.feature_proj(x)  # (batch_size, prototype_dim)\n        \n        # Compute similarity to prototypes (cosine similarity)\n        x_norm = F.normalize(x_proj, p=2, dim=1)\n        proto_norm = F.normalize(self.prototypes, p=2, dim=1)\n        \n        # Similarity matrix: (batch_size, num_prototypes)\n        proto_scores = torch.matmul(x_norm, proto_norm.t())\n        \n        # Soft assignment using softmax\n        proto_weights = F.softmax(proto_scores / 0.1, dim=1)  # Temperature = 0.1\n        \n        # Reconstruct features as weighted combination of prototypes\n        reconstructed_proj = torch.matmul(proto_weights, self.prototypes)\n        \n        # Project back to original feature space\n        reconstructed = self.recon_proj(reconstructed_proj)\n        \n        return reconstructed, proto_scores\n\n\nclass TemporalConvNet(nn.Module):\n    \"\"\"\n    Temporal convolution to aggregate features across time.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=9):\n        super().__init__()\n        padding = (kernel_size - 1) // 2\n        self.conv = nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding),\n            nn.BatchNorm1d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (batch_size, channels, time_steps)\n        \"\"\"\n        return self.conv(x)\n\n\nclass ProtoGCN(nn.Module):\n    \"\"\"\n    ProtoGCN: Prototype-based Graph Convolutional Network for Sign Language Recognition.\n    \n    The model consists of:\n    1. Motion Topology Enhancement (MTE) - learns dynamic graph topology\n    2. Spatial-Temporal GCN layers - extract spatio-temporal features\n    3. Prototype Reconstruction Network (PRN) - decomposes into motion prototypes\n    4. Classification head with contrastive learning support\n    \"\"\"\n    def __init__(\n        self,\n        num_nodes,\n        in_channels=3,  # x, y, confidence\n        hidden_channels=64,\n        num_classes=100,\n        num_gcn_layers=4,\n        num_prototypes=128,\n        dropout=0.5,\n        temporal_kernel_size=9\n    ):\n        super().__init__()\n        \n        self.num_nodes = num_nodes\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.num_classes = num_classes\n        self.num_gcn_layers = num_gcn_layers\n        \n        # Input projection (process each node independently)\n        self.input_proj = nn.Linear(in_channels, hidden_channels)\n        \n        # Spatial GCN layers with MTE\n        self.gcn_layers = nn.ModuleList()\n        self.mte_layers = nn.ModuleList()\n        self.temporal_convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        \n        for i in range(num_gcn_layers):\n            self.gcn_layers.append(GCNConv(hidden_channels, hidden_channels))\n            self.mte_layers.append(MotionTopologyEnhancement(hidden_channels, hidden_channels))\n            self.temporal_convs.append(TemporalConvNet(hidden_channels, hidden_channels, temporal_kernel_size))\n            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # Prototype Reconstruction Network\n        self.prn = PrototypeReconstructionNetwork(\n            feature_dim=hidden_channels,\n            num_prototypes=num_prototypes,\n            prototype_dim=256\n        )\n        \n        # Classification head\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_channels, hidden_channels // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_channels // 2, num_classes)\n        )\n        \n        # Projection head for contrastive learning\n        self.projector = nn.Sequential(\n            nn.Linear(hidden_channels, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, 256)\n        )\n        \n    def forward(self, data, return_embedding=False):\n        \"\"\"\n        Args:\n            data: PyG Data object with:\n                - x: (T, N, 3) node features for single sample\n                - edge_index: (2, num_edges) graph connectivity\n                - num_frames: T\n                - num_nodes: N\n        Returns:\n            logits: Class predictions\n            embedding: Feature embedding (if return_embedding=True)\n            proto_scores: Prototype scores (for contrastive learning)\n        \"\"\"\n        x = data.x  # (T, N, 3)\n        edge_index = data.edge_index  # (2, num_edges)\n        \n        # Get dimensions\n        if x.dim() == 3:\n            T, N, C = x.shape\n        else:\n            # Batched case would be handled differently\n            raise NotImplementedError(\"Batched processing not yet implemented\")\n        \n        # Input projection: (T, N, 3) -> (T, N, hidden_channels)\n        x = self.input_proj(x)  # (T, N, hidden_channels)\n        \n        # Process through GCN layers with temporal convolution\n        for i in range(self.num_gcn_layers):\n            # Spatial processing: apply GCN at each timestep\n            # Reshape: (T, N, C) -> (T*N, C)\n            x_flat = x.reshape(T * N, self.hidden_channels)\n            \n            # Expand edge_index for all timesteps\n            # Create edge_index for each timestep: add offset for each frame\n            edge_index_expanded = []\n            for t in range(T):\n                edge_index_t = edge_index + (t * N)\n                edge_index_expanded.append(edge_index_t)\n            edge_index_temporal = torch.cat(edge_index_expanded, dim=1)  # (2, T*num_edges)\n            \n            # Apply GCN and MTE\n            x_gcn = self.gcn_layers[i](x_flat, edge_index_temporal)\n            x_mte = self.mte_layers[i](x_flat, edge_index_temporal)\n            \n            # Combine and normalize\n            x_spatial = x_gcn + x_mte  # (T*N, hidden_channels)\n            x_spatial = self.batch_norms[i](x_spatial)\n            x_spatial = F.relu(x_spatial)\n            x_spatial = self.dropout(x_spatial)\n            \n            # Reshape back: (T*N, C) -> (T, N, C)\n            x = x_spatial.reshape(T, N, self.hidden_channels)\n            \n            # Temporal convolution: (T, N, C) -> (N, C, T)\n            x = x.permute(1, 2, 0).contiguous()  # (N, hidden_channels, T)\n            x = self.temporal_convs[i](x)  # (N, hidden_channels, T)\n            \n            # Reshape back: (N, C, T) -> (T, N, C)\n            x = x.permute(2, 0, 1).contiguous()  # (T, N, hidden_channels)\n        \n        # Global pooling: average over time and nodes\n        x = x.mean(dim=[0, 1])  # (hidden_channels,)\n        \n        # Prototype reconstruction\n        x_recon, proto_scores = self.prn(x)\n        \n        # Handle batch dimension\n        if x_recon.dim() == 2:\n            x_recon = x_recon.squeeze(0)\n        \n        # Classification\n        logits = self.fc(x_recon.unsqueeze(0)).squeeze(0)\n        \n        # Projection for contrastive learning\n        embedding = self.projector(x_recon.unsqueeze(0)).squeeze(0)\n        \n        if return_embedding:\n            return logits, embedding, proto_scores\n        return logits\n\n\nclass ClassSpecificContrastiveLoss(nn.Module):\n    \"\"\"\n    Class-Specific Contrastive (CSC) Loss to enhance inter-class distinction.\n    \"\"\"\n    def __init__(self, temperature=0.1):\n        super().__init__()\n        self.temperature = temperature\n        \n    def forward(self, embeddings, labels):\n        \"\"\"\n        Args:\n            embeddings: (batch_size, embedding_dim)\n            labels: (batch_size,)\n        \"\"\"\n        # Handle single sample\n        if embeddings.dim() == 1:\n            embeddings = embeddings.unsqueeze(0)\n        \n        # Normalize embeddings\n        embeddings = F.normalize(embeddings, p=2, dim=1)\n        \n        # Compute similarity matrix\n        sim_matrix = torch.matmul(embeddings, embeddings.t()) / self.temperature\n        \n        # Create positive and negative masks\n        labels = labels.view(-1, 1)\n        pos_mask = (labels == labels.t()).float()\n        neg_mask = (labels != labels.t()).float()\n        \n        # Remove diagonal\n        pos_mask = pos_mask - torch.eye(pos_mask.size(0), device=pos_mask.device)\n        \n        # Compute loss\n        exp_sim = torch.exp(sim_matrix)\n        pos_sim = (exp_sim * pos_mask).sum(dim=1)\n        neg_sim = (exp_sim * neg_mask).sum(dim=1)\n        \n        loss = -torch.log(pos_sim / (pos_sim + neg_sim + 1e-8))\n        return loss.mean()","metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nfrom torch_geometric.loader import DataLoader\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport json\nfrom tqdm import tqdm\nimport time\nfrom typing import Dict, Tuple, Optional, List\nimport random\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score, \n    confusion_matrix, classification_report,\n    top_k_accuracy_score\n)\nimport seaborn as sns\n\n\nclass DataAugmentation:\n    \"\"\"\n    On-the-fly data augmentation for temporal graph data.\n    \"\"\"\n    def __init__(\n        self,\n        drop_frame_prob=0.2,\n        drop_node_prob=0.1,\n        noise_std=0.01,\n        temporal_crop_ratio=0.8\n    ):\n        self.drop_frame_prob = drop_frame_prob\n        self.drop_node_prob = drop_node_prob\n        self.noise_std = noise_std\n        self.temporal_crop_ratio = temporal_crop_ratio\n    \n    def __call__(self, data, training=True):\n        \"\"\"Apply augmentation to data.\"\"\"\n        if not training:\n            return data\n        \n        x = data.x.clone()  # (T, N, 3)\n        T, N, C = x.shape\n        \n        # 1. Random frame dropping (temporal augmentation)\n        if random.random() < self.drop_frame_prob and T > 10:\n            num_frames_to_keep = max(10, int(T * self.temporal_crop_ratio))\n            frame_indices = sorted(random.sample(range(T), num_frames_to_keep))\n            x = x[frame_indices]\n            data.num_frames = len(frame_indices)\n        \n        # 2. Random node dropping (spatial augmentation)\n        if random.random() < self.drop_node_prob:\n            num_nodes_to_drop = max(1, int(N * 0.1))\n            nodes_to_drop = random.sample(range(N), num_nodes_to_drop)\n            \n            # Set dropped nodes to zero and update availability mask\n            x[:, nodes_to_drop, :] = 0\n            if hasattr(data, 'availability_mask'):\n                availability_mask = data.availability_mask.clone()\n                availability_mask[:, nodes_to_drop] = 0\n                data.availability_mask = availability_mask\n        \n        # 3. Add Gaussian noise to node coordinates\n        if self.noise_std > 0:\n            noise = torch.randn_like(x[:, :, :2]) * self.noise_std\n            x[:, :, :2] = x[:, :, :2] + noise\n            # Clip to valid range [0, 1]\n            x[:, :, :2] = torch.clamp(x[:, :, :2], 0.0, 1.0)\n        \n        # 4. Random horizontal flip\n        if random.random() < 0.5:\n            x[:, :, 0] = 1.0 - x[:, :, 0]  # Flip x-coordinates\n        \n        data.x = x\n        return data\n\n\nclass EarlyStopping:\n    \"\"\"Early stopping to stop training when validation loss doesn't improve.\"\"\"\n    def __init__(self, patience=10, min_delta=0.001, mode='min'):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.mode = mode\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.best_epoch = 0\n        \n        if mode == 'min':\n            self.monitor_op = np.less\n            self.min_delta *= -1\n        else:\n            self.monitor_op = np.greater\n            self.min_delta *= 1\n    \n    def __call__(self, score, epoch):\n        if self.best_score is None:\n            self.best_score = score\n            self.best_epoch = epoch\n        elif self.monitor_op(score, self.best_score + self.min_delta):\n            self.best_score = score\n            self.best_epoch = epoch\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        \n        return self.early_stop\n\n\nclass MetricsTracker:\n    \"\"\"Track and store training metrics including detailed classification metrics.\"\"\"\n    def __init__(self):\n        self.metrics = {\n            'train_loss': [],\n            'train_acc': [],\n            'train_f1': [],\n            'train_precision': [],\n            'train_recall': [],\n            'val_loss': [],\n            'val_acc': [],\n            'val_f1': [],\n            'val_precision': [],\n            'val_recall': [],\n            'val_top5_acc': [],\n            'learning_rates': []\n        }\n        self.best_confusion_matrix = None\n        self.best_classification_report = None\n    \n    def update(self, **kwargs):\n        for key, value in kwargs.items():\n            if key in self.metrics:\n                self.metrics[key].append(value)\n    \n    def save(self, path):\n        # Save numerical metrics\n        metrics_to_save = {k: v for k, v in self.metrics.items() \n                          if k not in ['best_confusion_matrix', 'best_classification_report']}\n        with open(path, 'w') as f:\n            json.dump(metrics_to_save, f, indent=2)\n    \n    def plot(self, save_dir):\n        \"\"\"Plot comprehensive training curves.\"\"\"\n        save_dir = Path(save_dir)\n        \n        # Create main training curves plot\n        fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n        \n        # Loss curves\n        axes[0, 0].plot(self.metrics['train_loss'], label='Train Loss', linewidth=2, color='#3498db')\n        axes[0, 0].plot(self.metrics['val_loss'], label='Val Loss', linewidth=2, color='#e74c3c')\n        axes[0, 0].set_xlabel('Epoch', fontsize=12)\n        axes[0, 0].set_ylabel('Loss', fontsize=12)\n        axes[0, 0].set_title('Loss Curves', fontsize=14, fontweight='bold')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n        \n        # Accuracy curves\n        axes[0, 1].plot(self.metrics['train_acc'], label='Train Acc', linewidth=2, color='#3498db')\n        axes[0, 1].plot(self.metrics['val_acc'], label='Val Acc', linewidth=2, color='#e74c3c')\n        if len(self.metrics['val_top5_acc']) > 0:\n            axes[0, 1].plot(self.metrics['val_top5_acc'], label='Val Top-5 Acc', \n                           linewidth=2, color='#2ecc71', linestyle='--')\n        axes[0, 1].set_xlabel('Epoch', fontsize=12)\n        axes[0, 1].set_ylabel('Accuracy (%)', fontsize=12)\n        axes[0, 1].set_title('Accuracy Curves', fontsize=14, fontweight='bold')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n        \n        # F1 Score curves\n        axes[1, 0].plot(self.metrics['train_f1'], label='Train F1', linewidth=2, color='#3498db')\n        axes[1, 0].plot(self.metrics['val_f1'], label='Val F1', linewidth=2, color='#e74c3c')\n        axes[1, 0].set_xlabel('Epoch', fontsize=12)\n        axes[1, 0].set_ylabel('F1 Score', fontsize=12)\n        axes[1, 0].set_title('F1 Score Curves', fontsize=14, fontweight='bold')\n        axes[1, 0].legend()\n        axes[1, 0].grid(True, alpha=0.3)\n        \n        # Precision curves\n        axes[1, 1].plot(self.metrics['train_precision'], label='Train Precision', linewidth=2, color='#3498db')\n        axes[1, 1].plot(self.metrics['val_precision'], label='Val Precision', linewidth=2, color='#e74c3c')\n        axes[1, 1].set_xlabel('Epoch', fontsize=12)\n        axes[1, 1].set_ylabel('Precision', fontsize=12)\n        axes[1, 1].set_title('Precision Curves', fontsize=14, fontweight='bold')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True, alpha=0.3)\n        \n        # Recall curves\n        axes[2, 0].plot(self.metrics['train_recall'], label='Train Recall', linewidth=2, color='#3498db')\n        axes[2, 0].plot(self.metrics['val_recall'], label='Val Recall', linewidth=2, color='#e74c3c')\n        axes[2, 0].set_xlabel('Epoch', fontsize=12)\n        axes[2, 0].set_ylabel('Recall', fontsize=12)\n        axes[2, 0].set_title('Recall Curves', fontsize=14, fontweight='bold')\n        axes[2, 0].legend()\n        axes[2, 0].grid(True, alpha=0.3)\n        \n        # Learning rate\n        axes[2, 1].plot(self.metrics['learning_rates'], linewidth=2, color='#9b59b6')\n        axes[2, 1].set_xlabel('Epoch', fontsize=12)\n        axes[2, 1].set_ylabel('Learning Rate', fontsize=12)\n        axes[2, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n        axes[2, 1].set_yscale('log')\n        axes[2, 1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(save_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        # Create summary statistics plot\n        self._plot_summary(save_dir)\n        \n        # Plot confusion matrix if available\n        if self.best_confusion_matrix is not None:\n            self._plot_confusion_matrix(save_dir)\n        \n        print(f\"Training curves saved to {save_dir / 'training_curves.png'}\")\n    \n    def _plot_summary(self, save_dir):\n        \"\"\"Plot summary statistics.\"\"\"\n        fig, ax = plt.subplots(figsize=(10, 8))\n        \n        best_val_acc = max(self.metrics['val_acc'])\n        best_epoch = self.metrics['val_acc'].index(best_val_acc)\n        best_val_f1 = self.metrics['val_f1'][best_epoch] if len(self.metrics['val_f1']) > best_epoch else 0\n        best_val_precision = self.metrics['val_precision'][best_epoch] if len(self.metrics['val_precision']) > best_epoch else 0\n        best_val_recall = self.metrics['val_recall'][best_epoch] if len(self.metrics['val_recall']) > best_epoch else 0\n        \n        summary_text = f\"\"\"\n        TRAINING SUMMARY\n        {'='*50}\n        \n        Best Validation Metrics (Epoch {best_epoch + 1}):\n          • Accuracy:  {best_val_acc:.2f}%\n          • F1 Score:  {best_val_f1:.4f}\n          • Precision: {best_val_precision:.4f}\n          • Recall:    {best_val_recall:.4f}\n        \n        {'='*50}\n        \n        Final Metrics:\n          • Train Loss: {self.metrics['train_loss'][-1]:.4f}\n          • Val Loss:   {self.metrics['val_loss'][-1]:.4f}\n          • Train Acc:  {self.metrics['train_acc'][-1]:.2f}%\n          • Val Acc:    {self.metrics['val_acc'][-1]:.2f}%\n          • Val F1:     {self.metrics['val_f1'][-1]:.4f}\n        \n        {'='*50}\n        \n        Total Epochs: {len(self.metrics['train_loss'])}\n        \"\"\"\n        \n        ax.text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n               fontfamily='monospace',\n               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n        ax.axis('off')\n        \n        plt.tight_layout()\n        plt.savefig(save_dir / 'training_summary.png', dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _plot_confusion_matrix(self, save_dir):\n        \"\"\"Plot confusion matrix.\"\"\"\n        if self.best_confusion_matrix is None:\n            return\n        \n        cm = self.best_confusion_matrix\n        \n        # If too many classes, show a smaller subset or simplified view\n        if cm.shape[0] > 20:\n            # Show only top 20 most confused classes\n            fig, ax = plt.subplots(figsize=(12, 10))\n            sns.heatmap(cm[:20, :20], annot=False, fmt='d', cmap='Blues', ax=ax)\n            ax.set_title('Confusion Matrix (Top 20 Classes)', fontsize=14, fontweight='bold')\n            ax.set_xlabel('Predicted Label', fontsize=12)\n            ax.set_ylabel('True Label', fontsize=12)\n        else:\n            fig, ax = plt.subplots(figsize=(12, 10))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n            ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n            ax.set_xlabel('Predicted Label', fontsize=12)\n            ax.set_ylabel('True Label', fontsize=12)\n        \n        plt.tight_layout()\n        plt.savefig(save_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"Confusion matrix saved to {save_dir / 'confusion_matrix.png'}\")\n    \n    def save_classification_report(self, save_dir):\n        \"\"\"Save detailed classification report.\"\"\"\n        if self.best_classification_report is not None:\n            report_path = save_dir / 'classification_report.txt'\n            with open(report_path, 'w') as f:\n                f.write(\"BEST MODEL CLASSIFICATION REPORT\\n\")\n                f.write(\"=\"*70 + \"\\n\\n\")\n                f.write(self.best_classification_report)\n            print(f\"Classification report saved to {report_path}\")\n\n\ndef compute_metrics(predictions: List[int], targets: List[int], \n                   probabilities: Optional[np.ndarray] = None) -> Dict[str, float]:\n    \"\"\"\n    Compute comprehensive classification metrics.\n    \n    Args:\n        predictions: List of predicted class indices\n        targets: List of true class indices\n        probabilities: Optional array of class probabilities for top-k accuracy\n        \n    Returns:\n        Dictionary of metrics\n    \"\"\"\n    metrics = {}\n    \n    # Basic metrics\n    metrics['accuracy'] = 100 * np.mean(np.array(predictions) == np.array(targets))\n    \n    # Precision, Recall, F1 (weighted average for multi-class)\n    metrics['precision'] = precision_score(targets, predictions, average='weighted', zero_division=0)\n    metrics['recall'] = recall_score(targets, predictions, average='weighted', zero_division=0)\n    metrics['f1'] = f1_score(targets, predictions, average='weighted', zero_division=0)\n    \n    # Macro-averaged metrics (treats all classes equally)\n    metrics['precision_macro'] = precision_score(targets, predictions, average='macro', zero_division=0)\n    metrics['recall_macro'] = recall_score(targets, predictions, average='macro', zero_division=0)\n    metrics['f1_macro'] = f1_score(targets, predictions, average='macro', zero_division=0)\n    \n    # Top-5 accuracy if probabilities provided\n    if probabilities is not None and probabilities.shape[1] >= 5:\n        metrics['top5_accuracy'] = 100 * top_k_accuracy_score(\n            targets, probabilities, k=5, labels=list(range(probabilities.shape[1]))\n        )\n    \n    return metrics\n\n\ndef train_epoch(model, loader, optimizer, criterion, contrastive_criterion, \n                augmentation, device, lambda_contrast=0.1, lambda_recon=0.1):\n    \"\"\"Train for one epoch with detailed metrics.\"\"\"\n    model.train()\n    total_loss = 0\n    all_predictions = []\n    all_targets = []\n    all_probs = []\n    \n    pbar = tqdm(loader, desc='Training')\n    for data in pbar:\n        # Apply augmentation\n        data = augmentation(data, training=True)\n        data = data.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        logits, embeddings, proto_scores = model(data, return_embedding=True)\n        \n        # Classification loss\n        if len(logits.shape) == 1:\n            logits = logits.unsqueeze(0)\n        loss_cls = criterion(logits, data.y)\n        \n        # Contrastive loss (if batch size > 1)\n        loss_contrast = torch.tensor(0.0, device=device)\n        if embeddings.dim() == 1:\n            embeddings = embeddings.unsqueeze(0)\n        if embeddings.size(0) > 1 and data.y.size(0) > 1:\n            loss_contrast = contrastive_criterion(embeddings, data.y)\n        \n        # Reconstruction loss (prototype diversity)\n        loss_recon = torch.tensor(0.0, device=device)\n        if proto_scores.size(0) > 0:\n            proto_entropy = -(proto_scores.softmax(dim=1) * proto_scores.log_softmax(dim=1)).sum(dim=1).mean()\n            loss_recon = -proto_entropy\n        \n        # Total loss\n        loss = loss_cls + lambda_contrast * loss_contrast + lambda_recon * loss_recon\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        # Collect predictions and targets\n        total_loss += loss.item()\n        pred = logits.argmax(dim=1)\n        probs = F.softmax(logits, dim=1)\n        \n        all_predictions.extend(pred.cpu().numpy())\n        all_targets.extend(data.y.cpu().numpy())\n        all_probs.append(probs.detach().cpu().numpy())\n        \n        # Update progress bar\n        current_acc = 100 * np.mean(np.array(all_predictions) == np.array(all_targets))\n        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{current_acc:.2f}%'})\n    \n    # Compute comprehensive metrics\n    avg_loss = total_loss / len(loader)\n    all_probs = np.vstack(all_probs)\n    metrics = compute_metrics(all_predictions, all_targets, all_probs)\n    \n    return avg_loss, metrics\n\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device, return_detailed=False):\n    \"\"\"\n    Evaluate model on validation/test set with comprehensive metrics.\n    \n    Args:\n        model: Model to evaluate\n        loader: Data loader\n        criterion: Loss function\n        device: Device to use\n        return_detailed: If True, return confusion matrix and classification report\n        \n    Returns:\n        avg_loss: Average loss\n        metrics: Dictionary of metrics\n        confusion_mat: Confusion matrix (if return_detailed=True)\n        class_report: Classification report (if return_detailed=True)\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    all_predictions = []\n    all_targets = []\n    all_probs = []\n    \n    pbar = tqdm(loader, desc='Evaluating')\n    for data in pbar:\n        data = data.to(device)\n        \n        # Forward pass\n        logits = model(data, return_embedding=False)\n        \n        # Loss\n        if len(logits.shape) == 1:\n            logits = logits.unsqueeze(0)\n        loss = criterion(logits, data.y)\n        \n        # Collect predictions and targets\n        total_loss += loss.item()\n        pred = logits.argmax(dim=1)\n        probs = F.softmax(logits, dim=1)\n        \n        all_predictions.extend(pred.cpu().numpy())\n        all_targets.extend(data.y.cpu().numpy())\n        all_probs.append(probs.cpu().numpy())\n        \n        # Update progress bar\n        current_acc = 100 * np.mean(np.array(all_predictions) == np.array(all_targets))\n        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{current_acc:.2f}%'})\n    \n    # Compute comprehensive metrics\n    avg_loss = total_loss / len(loader)\n    all_probs = np.vstack(all_probs)\n    metrics = compute_metrics(all_predictions, all_targets, all_probs)\n    \n    if return_detailed:\n        # Confusion matrix\n        confusion_mat = confusion_matrix(all_targets, all_predictions)\n        \n        # Classification report\n        class_report = classification_report(\n            all_targets, all_predictions,\n            target_names=[f'Class_{i}' for i in range(max(all_targets) + 1)],\n            zero_division=0\n        )\n        \n        return avg_loss, metrics, confusion_mat, class_report\n    \n    return avg_loss, metrics\n\n\ndef train_model(\n    model,\n    train_loader,\n    val_loader,\n    num_epochs=100,\n    learning_rate=0.001,\n    weight_decay=1e-4,\n    patience=15,\n    save_dir='checkpoints',\n    device='cuda',\n    lambda_contrast=0.1,\n    lambda_recon=0.1\n):\n    \"\"\"\n    Complete training pipeline with early stopping and learning rate scheduling.\n    \n    Args:\n        model: ProtoGCN model\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        num_epochs: Maximum number of epochs\n        learning_rate: Initial learning rate\n        weight_decay: L2 regularization\n        patience: Early stopping patience\n        save_dir: Directory to save checkpoints\n        device: Device to train on\n        lambda_contrast: Weight for contrastive loss\n        lambda_recon: Weight for reconstruction loss\n    \"\"\"\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n    \n    # Loss functions\n    criterion = nn.CrossEntropyLoss()\n    contrastive_criterion = ClassSpecificContrastiveLoss(temperature=0.1)\n    \n    # Data augmentation\n    augmentation = DataAugmentation(\n        drop_frame_prob=0.2,\n        drop_node_prob=0.1,\n        noise_std=0.01,\n        temporal_crop_ratio=0.8\n    )\n    \n    # Early stopping and metrics\n    early_stopping = EarlyStopping(patience=patience, mode='max')  # Monitor validation accuracy\n    metrics_tracker = MetricsTracker()\n    \n    best_val_acc = 0.0\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Starting training on {device}\")\n    print(f\"{'='*60}\\n\")\n    \n    start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        print(f\"{'-'*60}\")\n        \n        # Train\n        train_loss, train_metrics = train_epoch(\n            model, train_loader, optimizer, criterion, contrastive_criterion,\n            augmentation, device, lambda_contrast, lambda_recon\n        )\n        \n        # Validate\n        val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n        \n        # Update learning rate\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Track metrics\n        metrics_tracker.update(\n            train_loss=train_loss,\n            train_acc=train_metrics['accuracy'],\n            train_f1=train_metrics['f1'],\n            train_precision=train_metrics['precision'],\n            train_recall=train_metrics['recall'],\n            val_loss=val_loss,\n            val_acc=val_metrics['accuracy'],\n            val_f1=val_metrics['f1'],\n            val_precision=val_metrics['precision'],\n            val_recall=val_metrics['recall'],\n            val_top5_acc=val_metrics.get('top5_accuracy', 0),\n            learning_rates=current_lr\n        )\n        \n        # Print summary\n        print(f\"\\nEpoch {epoch + 1} Summary:\")\n        print(f\"  Train - Loss: {train_loss:.4f} | Acc: {train_metrics['accuracy']:.2f}% | \"\n              f\"F1: {train_metrics['f1']:.4f} | Precision: {train_metrics['precision']:.4f} | \"\n              f\"Recall: {train_metrics['recall']:.4f}\")\n        print(f\"  Val   - Loss: {val_loss:.4f} | Acc: {val_metrics['accuracy']:.2f}% | \"\n              f\"F1: {val_metrics['f1']:.4f} | Precision: {val_metrics['precision']:.4f} | \"\n              f\"Recall: {val_metrics['recall']:.4f}\")\n        if 'top5_accuracy' in val_metrics:\n            print(f\"  Val Top-5 Accuracy: {val_metrics['top5_accuracy']:.2f}%\")\n        print(f\"  Learning Rate: {current_lr:.6f}\")\n        \n        # Save best model and get detailed metrics\n        if val_metrics['accuracy'] > best_val_acc:\n            best_val_acc = val_metrics['accuracy']\n            \n            # Get detailed metrics for best model\n            _, _, confusion_mat, class_report = evaluate(\n                model, val_loader, criterion, device, return_detailed=True\n            )\n            metrics_tracker.best_confusion_matrix = confusion_mat\n            metrics_tracker.best_classification_report = class_report\n            \n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'val_acc': val_metrics['accuracy'],\n                'val_loss': val_loss,\n                'val_f1': val_metrics['f1'],\n                'val_precision': val_metrics['precision'],\n                'val_recall': val_metrics['recall'],\n            }, save_dir / 'best_model.pth')\n            print(f\"  ✓ New best model saved! (Val Acc: {val_metrics['accuracy']:.2f}%, \"\n                  f\"F1: {val_metrics['f1']:.4f})\")\n        \n        # Save checkpoint\n        if (epoch + 1) % 10 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n            }, save_dir / f'checkpoint_epoch_{epoch+1}.pth')\n        \n        # Early stopping\n        if early_stopping(val_metrics['accuracy'], epoch):\n            print(f\"\\n{'='*60}\")\n            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n            print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {early_stopping.best_epoch + 1}\")\n            print(f\"{'='*60}\\n\")\n            break\n    \n    # Training complete\n    training_time = time.time() - start_time\n    print(f\"\\n{'='*60}\")\n    print(f\"Training completed in {training_time / 60:.2f} minutes\")\n    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n    print(f\"{'='*60}\\n\")\n    \n    # Save final model\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n    }, save_dir / 'final_model.pth')\n    \n    # Save metrics and plots\n    metrics_tracker.save(save_dir / 'metrics.json')\n    metrics_tracker.plot(save_dir)\n    metrics_tracker.save_classification_report(save_dir)\n    \n    return model, metrics_tracker\n\n\n@torch.no_grad()\ndef inference(model, data, device='cuda', return_probs=False):\n    \"\"\"\n    Perform inference on a single sample or batch.\n    \n    Args:\n        model: Trained ProtoGCN model\n        data: PyG Data object\n        device: Device to run inference on\n        return_probs: Whether to return class probabilities\n        \n    Returns:\n        prediction: Predicted class index\n        probabilities: Class probabilities (if return_probs=True)\n        confidence: Prediction confidence\n    \"\"\"\n    model.eval()\n    data = data.to(device)\n    \n    # Forward pass\n    logits = model(data, return_embedding=False)\n    \n    # Get probabilities\n    if len(logits.shape) == 1:\n        logits = logits.unsqueeze(0)\n    probs = F.softmax(logits, dim=1)\n    \n    # Get prediction and confidence\n    confidence, prediction = probs.max(dim=1)\n    \n    if return_probs:\n        return prediction.item(), probs.squeeze().cpu().numpy(), confidence.item()\n    return prediction.item(), confidence.item()\n\n\ndef load_model(model, checkpoint_path, device='cuda'):\n    \"\"\"Load model from checkpoint.\"\"\"\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    print(f\"Model loaded from {checkpoint_path}\")\n    if 'val_acc' in checkpoint:\n        print(f\"Checkpoint metrics:\")\n        print(f\"  • Accuracy:  {checkpoint['val_acc']:.2f}%\")\n        if 'val_f1' in checkpoint:\n            print(f\"  • F1 Score:  {checkpoint['val_f1']:.4f}\")\n            print(f\"  • Precision: {checkpoint['val_precision']:.4f}\")\n            print(f\"  • Recall:    {checkpoint['val_recall']:.4f}\")\n    return model\n\n\n@torch.no_grad()\ndef detailed_evaluation(model, loader, device='cuda', save_dir='results'):\n    \"\"\"\n    Perform detailed evaluation with comprehensive metrics and visualizations.\n    \n    Args:\n        model: Trained model\n        loader: Data loader\n        device: Device to use\n        save_dir: Directory to save results\n    \"\"\"\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n    \n    model.eval()\n    all_predictions = []\n    all_targets = []\n    all_probs = []\n    \n    print(\"\\nPerforming detailed evaluation...\")\n    for data in tqdm(loader, desc='Evaluating'):\n        data = data.to(device)\n        logits = model(data, return_embedding=False)\n        \n        if len(logits.shape) == 1:\n            logits = logits.unsqueeze(0)\n        \n        pred = logits.argmax(dim=1)\n        probs = F.softmax(logits, dim=1)\n        \n        all_predictions.extend(pred.cpu().numpy())\n        all_targets.extend(data.y.cpu().numpy())\n        all_probs.append(probs.cpu().numpy())\n    \n    all_probs = np.vstack(all_probs)\n    \n    # Compute all metrics\n    metrics = compute_metrics(all_predictions, all_targets, all_probs)\n    \n    # Print metrics\n    print(\"\\n\" + \"=\"*70)\n    print(\"DETAILED EVALUATION RESULTS\")\n    print(\"=\"*70)\n    print(f\"\\nAccuracy Metrics:\")\n    print(f\"  • Top-1 Accuracy:  {metrics['accuracy']:.2f}%\")\n    if 'top5_accuracy' in metrics:\n        print(f\"  • Top-5 Accuracy:  {metrics['top5_accuracy']:.2f}%\")\n    \n    print(f\"\\nWeighted Average Metrics:\")\n    print(f\"  • Precision:       {metrics['precision']:.4f}\")\n    print(f\"  • Recall:          {metrics['recall']:.4f}\")\n    print(f\"  • F1 Score:        {metrics['f1']:.4f}\")\n    \n    print(f\"\\nMacro Average Metrics:\")\n    print(f\"  • Precision:       {metrics['precision_macro']:.4f}\")\n    print(f\"  • Recall:          {metrics['recall_macro']:.4f}\")\n    print(f\"  • F1 Score:        {metrics['f1_macro']:.4f}\")\n    print(\"=\"*70 + \"\\n\")\n    \n    # Confusion matrix\n    confusion_mat = confusion_matrix(all_targets, all_predictions)\n    \n    # Classification report\n    num_classes = len(np.unique(all_targets))\n    class_report = classification_report(\n        all_targets, all_predictions,\n        target_names=[f'Class_{i}' for i in range(num_classes)],\n        zero_division=0\n    )\n    \n    # Save results\n    with open(save_dir / 'evaluation_metrics.json', 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    with open(save_dir / 'classification_report.txt', 'w') as f:\n        f.write(\"EVALUATION CLASSIFICATION REPORT\\n\")\n        f.write(\"=\"*70 + \"\\n\\n\")\n        f.write(class_report)\n    \n    # Plot confusion matrix\n    if num_classes <= 20:\n        fig, ax = plt.subplots(figsize=(12, 10))\n        sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', ax=ax)\n        ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n        ax.set_xlabel('Predicted Label', fontsize=12)\n        ax.set_ylabel('True Label', fontsize=12)\n    else:\n        # Show subset for many classes\n        fig, ax = plt.subplots(figsize=(14, 12))\n        sns.heatmap(confusion_mat[:20, :20], annot=False, fmt='d', cmap='Blues', ax=ax)\n        ax.set_title('Confusion Matrix (Top 20 Classes)', fontsize=14, fontweight='bold')\n        ax.set_xlabel('Predicted Label', fontsize=12)\n        ax.set_ylabel('True Label', fontsize=12)\n    \n    plt.tight_layout()\n    plt.savefig(save_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    print(f\"Results saved to {save_dir}/\")\n    print(f\"  • evaluation_metrics.json\")\n    print(f\"  • classification_report.txt\")\n    print(f\"  • confusion_matrix.png\")\n    \n    return metrics, confusion_mat, class_report\n\n\n# Import the contrastive loss class\n\n\n# Example usage\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load dataset\ndataset = SignLanguageDataset(root='data/sign_language')\n\n# Split dataset\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(\n    dataset, [train_size, val_size]\n)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n\n# Initialize model\nmodel = ProtoGCN(\n    num_nodes=114,\n    in_channels=3,\n    hidden_channels=64,\n    num_classes=2,\n    num_gcn_layers=4,\n    num_prototypes=128,\n    dropout=0.5\n)\n\nprint(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Train model\ntrained_model, metrics = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=100,\n    learning_rate=0.001,\n    weight_decay=1e-4,\n    patience=5,\n    save_dir='checkpoints',\n    device=device,\n    lambda_contrast=0.1,\n    lambda_recon=0.1\n)\n\n# Load best model for inference\nbest_model = ProtoGCN(\n    num_nodes=114,\n    in_channels=3,\n    hidden_channels=64,\n    num_classes=2,\n    num_gcn_layers=4,\n    num_prototypes=128,\n    dropout=0.5\n)\nbest_model = load_model(best_model, 'checkpoints/best_model.pth', device)\n\n# Detailed evaluation\nprint(\"\\n\" + \"=\"*70)\nprint(\"PERFORMING DETAILED EVALUATION ON VALIDATION SET\")\nprint(\"=\"*70)\ndetailed_metrics, conf_matrix, class_rep = detailed_evaluation(\n    best_model, val_loader, device, save_dir='results'\n)\n\n# Test inference\nprint(\"\\n\" + \"=\"*70)\nprint(\"TESTING INFERENCE\")\nprint(\"=\"*70)\ntest_data = dataset[0]\nprediction, confidence = inference(best_model, test_data, device)\nprint(f\"\\nSingle Sample Inference:\")\nprint(f\"  • Prediction: Class {prediction}\")\nprint(f\"  • Confidence: {confidence:.4f}\")\n\n# Test with probabilities\nprediction, probs, confidence = inference(best_model, test_data, device, return_probs=True)\nprint(f\"\\nTop 5 Predictions:\")\ntop5_indices = np.argsort(probs)[-5:][::-1]\nfor i, idx in enumerate(top5_indices):\n    print(f\"  {i+1}. Class {idx}: {probs[idx]:.4f}\")\nprint(\"=\"*70)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","\n","Model parameters: 365,986\n","\n","============================================================\n","Starting training on cuda\n","============================================================\n","\n","\n","Epoch 1/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 10.95it/s, loss=0.2340, acc=25.00%]\n","Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 56.70it/s, loss=0.7379, acc=0.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1 Summary:\n","  Train - Loss: 0.2191 | Acc: 25.00% | F1: 0.1000 | Precision: 0.0625 | Recall: 0.2500\n","  Val   - Loss: 0.7379 | Acc: 0.00% | F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n","  Learning Rate: 0.001000\n","\n","Epoch 2/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 55.66it/s, loss=0.1846, acc=50.00%]\n","Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 85.81it/s, loss=0.7232, acc=0.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 2 Summary:\n","  Train - Loss: 0.2311 | Acc: 50.00% | F1: 0.5000 | Precision: 0.8333 | Recall: 0.5000\n","  Val   - Loss: 0.7232 | Acc: 0.00% | F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n","  Learning Rate: 0.000999\n","\n","Epoch 3/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 59.29it/s, loss=0.2259, acc=25.00%]\n","Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 90.25it/s, loss=0.7135, acc=0.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 3 Summary:\n","  Train - Loss: 0.2263 | Acc: 25.00% | F1: 0.1000 | Precision: 0.0625 | Recall: 0.2500\n","  Val   - Loss: 0.7135 | Acc: 0.00% | F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n","  Learning Rate: 0.000998\n","\n","Epoch 4/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 60.69it/s, loss=0.1836, acc=75.00%]\n","Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 86.85it/s, loss=0.6948, acc=0.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 4 Summary:\n","  Train - Loss: 0.1990 | Acc: 75.00% | F1: 0.7667 | Precision: 0.8750 | Recall: 0.7500\n","  Val   - Loss: 0.6948 | Acc: 0.00% | F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n","  Learning Rate: 0.000996\n","\n","Epoch 5/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 54.75it/s, loss=0.1686, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 77.53it/s, loss=0.6729, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 5 Summary:\n","  Train - Loss: 0.1911 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.6729 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000994\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 70.75it/s, loss=0.6729, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["  ✓ New best model saved! (Val Acc: 100.00%, F1: 1.0000)\n","\n","Epoch 6/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 57.85it/s, loss=0.2100, acc=50.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 80.01it/s, loss=0.6508, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 6 Summary:\n","  Train - Loss: 0.2114 | Acc: 50.00% | F1: 0.5000 | Precision: 0.5000 | Recall: 0.5000\n","  Val   - Loss: 0.6508 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000991\n","\n","Epoch 7/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 55.82it/s, loss=0.1240, acc=50.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 88.30it/s, loss=0.6040, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 7 Summary:\n","  Train - Loss: 0.1559 | Acc: 50.00% | F1: 0.5000 | Precision: 0.5000 | Recall: 0.5000\n","  Val   - Loss: 0.6040 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000988\n","\n","Epoch 8/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 65.85it/s, loss=0.2741, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 81.73it/s, loss=0.5516, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 8 Summary:\n","  Train - Loss: 0.1383 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.5516 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000984\n","\n","Epoch 9/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 59.10it/s, loss=0.5709, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 88.05it/s, loss=0.4980, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 9 Summary:\n","  Train - Loss: 0.2032 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.4980 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000980\n","\n","Epoch 10/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 65.35it/s, loss=-0.1128, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 89.76it/s, loss=0.4463, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 10 Summary:\n","  Train - Loss: 0.0456 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.4463 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000976\n","\n","Epoch 11/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 61.96it/s, loss=-0.2486, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 84.49it/s, loss=0.3894, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 11 Summary:\n","  Train - Loss: 0.0605 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.3894 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000970\n","\n","Epoch 12/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 60.94it/s, loss=-0.2733, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 89.41it/s, loss=0.3227, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 12 Summary:\n","  Train - Loss: -0.0887 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.3227 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000965\n","\n","Epoch 13/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 60.03it/s, loss=-0.2852, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 92.40it/s, loss=0.2502, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 13 Summary:\n","  Train - Loss: 0.0053 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.2502 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000959\n","\n","Epoch 14/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 64.45it/s, loss=-0.3332, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 93.45it/s, loss=0.1959, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 14 Summary:\n","  Train - Loss: 0.0790 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.1959 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000952\n","\n","Epoch 15/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 60.12it/s, loss=-0.3991, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 89.81it/s, loss=0.1526, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 15 Summary:\n","  Train - Loss: -0.1196 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.1526 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000946\n","\n","Epoch 16/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 63.18it/s, loss=-0.2370, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 86.27it/s, loss=0.1178, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 16 Summary:\n","  Train - Loss: 0.2665 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.1178 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000938\n","\n","Epoch 17/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 61.65it/s, loss=-0.4420, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 92.41it/s, loss=0.0864, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 17 Summary:\n","  Train - Loss: 0.6400 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.0864 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000930\n","\n","Epoch 18/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 59.88it/s, loss=1.5496, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 86.46it/s, loss=0.0654, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 18 Summary:\n","  Train - Loss: 0.0687 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.0654 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000922\n","\n","Epoch 19/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 63.25it/s, loss=-0.0526, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 90.21it/s, loss=0.0559, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 19 Summary:\n","  Train - Loss: 0.5193 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.0559 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000914\n","\n","Epoch 20/100\n","------------------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 61.64it/s, loss=-0.2974, acc=75.00%]\n","Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 91.67it/s, loss=0.0483, acc=100.00%]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 20 Summary:\n","  Train - Loss: -0.0187 | Acc: 75.00% | F1: 0.6429 | Precision: 0.5625 | Recall: 0.7500\n","  Val   - Loss: 0.0483 | Acc: 100.00% | F1: 1.0000 | Precision: 1.0000 | Recall: 1.0000\n","  Learning Rate: 0.000905\n","\n","============================================================\n","Early stopping triggered at epoch 20\n","Best validation accuracy: 100.00% at epoch 5\n","============================================================\n","\n","\n","============================================================\n","Training completed in 0.04 minutes\n","Best validation accuracy: 100.00%\n","============================================================\n","\n","Confusion matrix saved to checkpoints/confusion_matrix.png\n","Training curves saved to checkpoints/training_curves.png\n","Classification report saved to checkpoints/classification_report.txt\n","Model loaded from checkpoints/best_model.pth\n","Checkpoint metrics:\n","  • Accuracy:  100.00%\n","  • F1 Score:  1.0000\n","  • Precision: 1.0000\n","  • Recall:    1.0000\n","\n","======================================================================\n","PERFORMING DETAILED EVALUATION ON VALIDATION SET\n","======================================================================\n","\n","Performing detailed evaluation...\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 91.74it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","======================================================================\n","DETAILED EVALUATION RESULTS\n","======================================================================\n","\n","Accuracy Metrics:\n","  • Top-1 Accuracy:  100.00%\n","\n","Weighted Average Metrics:\n","  • Precision:       1.0000\n","  • Recall:          1.0000\n","  • F1 Score:        1.0000\n","\n","Macro Average Metrics:\n","  • Precision:       1.0000\n","  • Recall:          1.0000\n","  • F1 Score:        1.0000\n","======================================================================\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Results saved to results/\n","  • evaluation_metrics.json\n","  • classification_report.txt\n","  • confusion_matrix.png\n","\n","======================================================================\n","TESTING INFERENCE\n","======================================================================\n","\n","Single Sample Inference:\n","  • Prediction: Class 0\n","  • Confidence: 0.5103\n","\n","Top 5 Predictions:\n","  1. Class 0: 0.5103\n","  2. Class 1: 0.4897\n","======================================================================\n"]}],"execution_count":3}]}