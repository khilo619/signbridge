{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "print(\"âœ… Imports loaded successfully!\")\n",
    "print(f\"ðŸ“Š TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ðŸŽ® GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (64 frames for complete signs)\n",
    "TARGET_FRAMES = 64\n",
    "N_FACE = 468\n",
    "N_POSE = 33\n",
    "N_LEFT_HAND = 21\n",
    "N_RIGHT_HAND = 21\n",
    "TOTAL_LANDMARKS = N_FACE + N_POSE + N_LEFT_HAND + N_RIGHT_HAND  # 543\n",
    "INPUT_SHAPE = (TARGET_FRAMES, TOTAL_LANDMARKS * 3)  # (64, 1629)\n",
    "\n",
    "# Dataset Paths (ISLR ONLY)\n",
    "ISLR_TRAIN_PATH = \"/kaggle/input/wlasl2000-landmarks/train.tfrecord\"  # 23.36 GB\n",
    "ISLR_VAL_PATH = \"/kaggle/input/wlasl2000-landmarks/val.tfrecord\"      # 7.96 GB\n",
    "ISLR_MAPPING_PATH = \"/kaggle/input/islr-mappings/sign_to_prediction_index_map.json\"  # 250 words\n",
    "\n",
    "print(f\"âœ… Configuration:\")\n",
    "print(f\"   Target frames: {TARGET_FRAMES} ({TARGET_FRAMES/30:.1f}s @ 30fps)\")\n",
    "print(f\"   Total landmarks: {TOTAL_LANDMARKS}\")\n",
    "print(f\"   Input shape: {INPUT_SHAPE}\")\n",
    "print(f\"   Training on: ISLR 200 words ONLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load ISLR 250-word Mapping\n",
    "with open(ISLR_MAPPING_PATH, 'r') as f:\n",
    "    islr_full_mapping = json.load(f)  # word -> label (0-249)\n",
    "\n",
    "print(f\"âœ… ISLR mapping loaded: {len(islr_full_mapping)} words\")\n",
    "print(f\"   First 10 words: {list(islr_full_mapping.keys())[:10]}\")\n",
    "print(f\"   Label range: {min(islr_full_mapping.values())} - {max(islr_full_mapping.values())}\")\n",
    "\n",
    "# Create reverse mapping (label -> word)\n",
    "islr_label_to_word = {v: k for k, v in islr_full_mapping.items()}\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Will train on 200 words present in TFRecord files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: TFRecord Parsing Functions\n",
    "def parse_tfrecord_example(example_proto):\n",
    "    \"\"\"Parse a single TFRecord example from ISLR dataset\"\"\"\n",
    "    feature_description = {\n",
    "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Decode landmarks: variable frames x 543 landmarks x 3 coords\n",
    "    landmarks = tf.io.decode_raw(parsed['landmarks'], tf.float32)\n",
    "    landmarks = tf.reshape(landmarks, [-1, TOTAL_LANDMARKS, 3])\n",
    "    \n",
    "    label = parsed['label']\n",
    "    \n",
    "    return landmarks, label\n",
    "\n",
    "def resample_to_target_frames(landmarks, target_frames=TARGET_FRAMES):\n",
    "    \"\"\"Resample variable-length sequence to fixed 64 frames\"\"\"\n",
    "    current_frames = tf.shape(landmarks)[0]\n",
    "    \n",
    "    # If already correct length\n",
    "    if current_frames == target_frames:\n",
    "        return tf.reshape(landmarks, [target_frames, -1])\n",
    "    \n",
    "    # If longer: sample evenly spaced frames\n",
    "    if current_frames >= target_frames:\n",
    "        indices = tf.linspace(0.0, tf.cast(current_frames-1, tf.float32), target_frames)\n",
    "        indices = tf.cast(indices, tf.int32)\n",
    "        landmarks = tf.gather(landmarks, indices)\n",
    "    else:\n",
    "        # If shorter: pad with last frame\n",
    "        padding = target_frames - current_frames\n",
    "        last_frame = landmarks[-1:]\n",
    "        padding_frames = tf.tile(last_frame, [padding, 1, 1])\n",
    "        landmarks = tf.concat([landmarks, padding_frames], axis=0)\n",
    "    \n",
    "    # Flatten: (64, 543, 3) -> (64, 1629)\n",
    "    return tf.reshape(landmarks, [target_frames, -1])\n",
    "\n",
    "print(\"âœ… TFRecord parsing functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load ISLR Training Data (10-15 minutes)\n",
    "def load_islr_tfrecord(tfrecord_path, dataset_name=\"dataset\"):\n",
    "    \"\"\"Load and process ISLR TFRecord file\"\"\"\n",
    "    print(f\"\\nðŸ“¦ Loading {dataset_name} from {tfrecord_path}...\")\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    label_counts = {}\n",
    "    skipped = 0\n",
    "    \n",
    "    for record in tqdm(dataset, desc=f\"Processing {dataset_name}\"):\n",
    "        try:\n",
    "            landmarks, label = parse_tfrecord_example(record)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            landmarks_np = landmarks.numpy()\n",
    "            label_np = int(label.numpy())\n",
    "            \n",
    "            # Resample to 64 frames\n",
    "            processed = resample_to_target_frames(tf.constant(landmarks_np)).numpy()\n",
    "            \n",
    "            X_data.append(processed)\n",
    "            y_data.append(label_np)\n",
    "            \n",
    "            # Track label distribution\n",
    "            label_counts[label_np] = label_counts.get(label_np, 0) + 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    X_data = np.array(X_data, dtype=np.float32)\n",
    "    y_data = np.array(y_data, dtype=np.int32)\n",
    "    \n",
    "    print(f\"   âœ… Loaded: {len(X_data):,} samples\")\n",
    "    print(f\"   âš ï¸  Skipped: {skipped} samples\")\n",
    "    print(f\"   ðŸ“Š Unique labels: {len(label_counts)}\")\n",
    "    print(f\"   ðŸ“Š Shape: {X_data.shape}\")\n",
    "    \n",
    "    return X_data, y_data, label_counts\n",
    "\n",
    "# Load training data\n",
    "print(\"ðŸš€ Loading ISLR training data (this will take 10-15 minutes)...\")\n",
    "X_train, y_train, train_label_counts = load_islr_tfrecord(ISLR_TRAIN_PATH, \"train\")\n",
    "\n",
    "# Load validation data\n",
    "print(\"\\nðŸš€ Loading ISLR validation data...\")\n",
    "X_val, y_val, val_label_counts = load_islr_tfrecord(ISLR_VAL_PATH, \"val\")\n",
    "\n",
    "# Combine label counts\n",
    "all_labels = set(train_label_counts.keys()).union(set(val_label_counts.keys()))\n",
    "num_classes = len(all_labels)\n",
    "\n",
    "print(f\"\\nâœ… ISLR DATASET LOADED:\")\n",
    "print(f\"   Train: {len(X_train):,} samples\")\n",
    "print(f\"   Val: {len(X_val):,} samples\")\n",
    "print(f\"   Total: {len(X_train) + len(X_val):,} samples\")\n",
    "print(f\"   Classes: {num_classes} words\")\n",
    "\n",
    "# Show samples per class\n",
    "all_samples = np.concatenate([y_train, y_val])\n",
    "unique_labels, counts = np.unique(all_samples, return_counts=True)\n",
    "print(f\"\\nðŸ“Š Samples per class:\")\n",
    "print(f\"   Min: {counts.min()}\")\n",
    "print(f\"   Max: {counts.max()}\")\n",
    "print(f\"   Mean: {counts.mean():.1f}\")\n",
    "print(f\"   Median: {np.median(counts):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create Final Label Mapping (Remap to 0-199)\n",
    "# ISLR labels might not be continuous (0-199), so we remap them\n",
    "unique_labels_sorted = sorted(np.unique(np.concatenate([y_train, y_val])))\n",
    "\n",
    "# Create mapping: old_label -> new_label (0 to num_classes-1)\n",
    "old_to_new_label = {old: new for new, old in enumerate(unique_labels_sorted)}\n",
    "new_to_old_label = {new: old for old, new in old_to_new_label.items()}\n",
    "\n",
    "# Remap labels\n",
    "y_train_remapped = np.array([old_to_new_label[label] for label in y_train], dtype=np.int32)\n",
    "y_val_remapped = np.array([old_to_new_label[label] for label in y_val], dtype=np.int32)\n",
    "\n",
    "# Create final word mappings\n",
    "final_label_to_word = {}\n",
    "final_word_to_label = {}\n",
    "\n",
    "for new_label, old_label in new_to_old_label.items():\n",
    "    word = islr_label_to_word.get(old_label, f\"unknown_{old_label}\")\n",
    "    final_label_to_word[str(new_label)] = word\n",
    "    final_word_to_label[word] = new_label\n",
    "\n",
    "print(f\"âœ… Labels remapped to continuous range [0-{num_classes-1}]\")\n",
    "print(f\"   Train labels: {y_train_remapped.min()} - {y_train_remapped.max()}\")\n",
    "print(f\"   Val labels: {y_val_remapped.min()} - {y_val_remapped.max()}\")\n",
    "print(f\"\\nðŸ“š Vocabulary (first 20 words):\")\n",
    "for i in range(min(20, num_classes)):\n",
    "    print(f\"   {i}: {final_label_to_word[str(i)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data Quality Check\n",
    "print(\"ðŸ” Data quality check:\")\n",
    "print(f\"\\nðŸ“Š Training data:\")\n",
    "print(f\"   Shape: {X_train.shape}\")\n",
    "print(f\"   Range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"   Mean: {X_train.mean():.3f}\")\n",
    "print(f\"   Std: {X_train.std():.3f}\")\n",
    "print(f\"   NaN values: {np.isnan(X_train).sum()}\")\n",
    "print(f\"   Inf values: {np.isinf(X_train).sum()}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Validation data:\")\n",
    "print(f\"   Shape: {X_val.shape}\")\n",
    "print(f\"   Range: [{X_val.min():.3f}, {X_val.max():.3f}]\")\n",
    "print(f\"   Mean: {X_val.mean():.3f}\")\n",
    "print(f\"   Std: {X_val.std():.3f}\")\n",
    "print(f\"   NaN values: {np.isnan(X_val).sum()}\")\n",
    "print(f\"   Inf values: {np.isinf(X_val).sum()}\")\n",
    "\n",
    "print(f\"\\nâœ… Data is clean and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Build Enhanced LSTM Model\n",
    "model = Sequential([\n",
    "    # First LSTM layer\n",
    "    LSTM(128, return_sequences=True, activation='tanh', input_shape=INPUT_SHAPE),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    LSTM(256, return_sequences=True, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Third LSTM layer\n",
    "    LSTM(128, return_sequences=False, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"âœ… LSTM Model built:\")\n",
    "print(f\"   Input: {INPUT_SHAPE}\")\n",
    "print(f\"   Output: {num_classes} classes\")\n",
    "print(f\"   Parameters: {model.count_params():,}\")\n",
    "print(f\"   Model size: ~{model.count_params() * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train Model (45-90 minutes)\n",
    "# Setup callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '/kaggle/working/lstm_islr200_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=8,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Validation samples: {len(X_val):,}\")\n",
    "print(f\"   Batch size: 64\")\n",
    "print(f\"   Max epochs: 150\")\n",
    "print(f\"   Estimated time: 45-90 minutes\")\n",
    "print(f\"\\nâ³ Please wait...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_remapped,\n",
    "    validation_data=(X_val, y_val_remapped),\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint, reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"   Total epochs: {len(history.history['loss'])}\")\n",
    "print(f\"   Best val accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"   Final train accuracy: {history.history['accuracy'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluate Model\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model('/kaggle/working/lstm_islr200_best.h5')\n",
    "\n",
    "print(\"ðŸ” Evaluating model performance...\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_acc = best_model.evaluate(X_val, y_val_remapped, verbose=0)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate top-k accuracies\n",
    "top3_acc = top_k_accuracy_score(y_val_remapped, y_pred, k=3)\n",
    "top5_acc = top_k_accuracy_score(y_val_remapped, y_pred, k=5)\n",
    "top10_acc = top_k_accuracy_score(y_val_remapped, y_pred, k=10)\n",
    "\n",
    "print(f\"ðŸŽ¯ FINAL RESULTS:\")\n",
    "print(f\"   ðŸ“Š Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"   ðŸŽ¯ Top-1 Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"   ðŸ¥‰ Top-3 Accuracy: {top3_acc*100:.2f}%\")\n",
    "print(f\"   ðŸ¥‡ Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
    "print(f\"   ðŸ† Top-10 Accuracy: {top10_acc*100:.2f}%\")\n",
    "\n",
    "# Sample predictions\n",
    "print(f\"\\nðŸ” Sample predictions (first 20):\")\n",
    "for i in range(min(20, len(y_val_remapped))):\n",
    "    true_word = final_label_to_word[str(y_val_remapped[i])]\n",
    "    pred_word = final_label_to_word[str(y_pred_classes[i])]\n",
    "    confidence = y_pred[i][y_pred_classes[i]] * 100\n",
    "    status = \"âœ…\" if y_val_remapped[i] == y_pred_classes[i] else \"âŒ\"\n",
    "    print(f\"   {status} True: {true_word:15s} | Pred: {pred_word:15s} ({confidence:.1f}%)\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(f\"\\nðŸ“ˆ Per-class statistics:\")\n",
    "class_accuracies = []\n",
    "poor_classes = []\n",
    "\n",
    "for label in range(num_classes):\n",
    "    mask = y_val_remapped == label\n",
    "    if mask.sum() > 0:\n",
    "        acc = (y_pred_classes[mask] == label).mean()\n",
    "        class_accuracies.append(acc)\n",
    "        word = final_label_to_word[str(label)]\n",
    "        \n",
    "        if acc < 0.5:  # Poor performing classes\n",
    "            poor_classes.append((word, acc, mask.sum()))\n",
    "\n",
    "print(f\"   Best class: {max(class_accuracies)*100:.1f}%\")\n",
    "print(f\"   Worst class: {min(class_accuracies)*100:.1f}%\")\n",
    "print(f\"   Average: {np.mean(class_accuracies)*100:.1f}%\")\n",
    "\n",
    "if poor_classes:\n",
    "    print(f\"\\nâš ï¸  Classes with <50% accuracy:\")\n",
    "    for word, acc, count in sorted(poor_classes, key=lambda x: x[1])[:10]:\n",
    "        print(f\"   {word:15s}: {acc*100:.1f}% ({count} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save Model and Deployment Package\n",
    "print(\"ðŸ’¾ Saving model and creating deployment package...\\n\")\n",
    "\n",
    "# Save model\n",
    "best_model.save('/kaggle/working/lstm_islr200_final.h5')\n",
    "best_model.save('/kaggle/working/lstm_islr200_savedmodel')\n",
    "\n",
    "# Create deployment configuration\n",
    "deployment_config = {\n",
    "    'model_info': {\n",
    "        'name': 'ISLR 200-Word LSTM Model',\n",
    "        'version': '1.0',\n",
    "        'framework': 'TensorFlow/Keras',\n",
    "        'input_shape': list(INPUT_SHAPE),\n",
    "        'target_frames': TARGET_FRAMES,\n",
    "        'total_landmarks': TOTAL_LANDMARKS,\n",
    "        'num_classes': num_classes,\n",
    "        'training_samples': len(X_train),\n",
    "        'validation_samples': len(X_val)\n",
    "    },\n",
    "    'performance': {\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'val_loss': float(val_loss),\n",
    "        'top3_accuracy': float(top3_acc),\n",
    "        'top5_accuracy': float(top5_acc),\n",
    "        'top10_accuracy': float(top10_acc),\n",
    "        'expected_realworld_accuracy': f\"{val_acc*0.8*100:.1f}-{val_acc*0.9*100:.1f}%\"\n",
    "    },\n",
    "    'labels': {\n",
    "        'word_to_label': final_word_to_label,\n",
    "        'label_to_word': final_label_to_word,\n",
    "        'num_classes': num_classes\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'target_frames': TARGET_FRAMES,\n",
    "        'landmarks_per_frame': TOTAL_LANDMARKS,\n",
    "        'coordinates_per_landmark': 3,\n",
    "        'input_flattened': True,\n",
    "        'landmark_order': 'face(468) + left_hand(21) + pose(33) + right_hand(21)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save deployment config\n",
    "with open('/kaggle/working/deployment_config.json', 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "# Save training history\n",
    "history_data = {\n",
    "    'loss': [float(x) for x in history.history['loss']],\n",
    "    'accuracy': [float(x) for x in history.history['accuracy']],\n",
    "    'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "    'val_accuracy': [float(x) for x in history.history['val_accuracy']]\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/training_history.json', 'w') as f:\n",
    "    json.dump(history_data, f, indent=2)\n",
    "\n",
    "# Create README\n",
    "readme = f\"\"\"# ISLR 200-Word Sign Language Recognition Model\n",
    "\n",
    "## Model Performance\n",
    "- **Validation Accuracy:** {val_acc*100:.2f}%\n",
    "- **Top-5 Accuracy:** {top5_acc*100:.2f}%\n",
    "- **Training Samples:** {len(X_train):,}\n",
    "- **Vocabulary:** {num_classes} words\n",
    "\n",
    "## Model Specifications\n",
    "- **Input:** {TARGET_FRAMES} frames Ã— {TOTAL_LANDMARKS} landmarks Ã— 3 coords = {INPUT_SHAPE}\n",
    "- **Architecture:** 3-layer LSTM with BatchNormalization\n",
    "- **Parameters:** {model.count_params():,}\n",
    "- **Model Size:** ~{model.count_params() * 4 / 1024 / 1024:.1f} MB\n",
    "\n",
    "## Usage\n",
    "1. Load model: `model = tf.keras.models.load_model('lstm_islr200_final.h5')`\n",
    "2. Load labels: `with open('deployment_config.json') as f: config = json.load(f)`\n",
    "3. Preprocess: Extract MediaPipe landmarks, resample to 64 frames, flatten to (64, 1629)\n",
    "4. Predict: `prediction = model.predict(landmarks)`\n",
    "\n",
    "## Files\n",
    "- `lstm_islr200_final.h5` - Keras model\n",
    "- `lstm_islr200_savedmodel/` - TensorFlow SavedModel format\n",
    "- `deployment_config.json` - Complete configuration and labels\n",
    "- `training_history.json` - Training metrics\n",
    "- `README.md` - This file\n",
    "\"\"\"\n",
    "\n",
    "with open('/kaggle/working/README.md', 'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\"âœ… DEPLOYMENT PACKAGE CREATED:\")\n",
    "print(\"   ðŸ“¦ lstm_islr200_final.h5\")\n",
    "print(\"   ðŸ“¦ lstm_islr200_savedmodel/\")\n",
    "print(\"   ðŸ“¦ deployment_config.json\")\n",
    "print(\"   ðŸ“¦ training_history.json\")\n",
    "print(\"   ðŸ“¦ README.md\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MODEL SUMMARY:\")\n",
    "print(f\"   ðŸŽª Dataset: ISLR 200 words\")\n",
    "print(f\"   ðŸ“Š Training samples: {len(X_train):,}\")\n",
    "print(f\"   ðŸŽ¯ Validation accuracy: {val_acc*100:.1f}%\")\n",
    "print(f\"   ðŸ¥‡ Top-5 accuracy: {top5_acc*100:.1f}%\")\n",
    "print(f\"   ðŸ“š Vocabulary: {num_classes} words\")\n",
    "print(f\"   âš¡ Input: {TARGET_FRAMES} frames ({TARGET_FRAMES/30:.1f}s @ 30fps)\")\n",
    "print(f\"   ðŸ’¾ Model size: ~{model.count_params() * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(f\"\\nðŸ“¥ DOWNLOAD ALL FILES FROM /kaggle/working/ FOR DEPLOYMENT\")\n",
    "print(f\"\\nðŸŽ‰ TRAINING COMPLETE! Model ready for real-time inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
