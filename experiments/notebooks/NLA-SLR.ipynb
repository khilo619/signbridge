{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbYDDkkRrm5p",
    "outputId": "df93868b-1487-4058-fb4d-3d009ec5f140"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Clone the repository\n",
    "# !git clone https://github.com/FangyunWei/SLRT.git\n",
    "# os.chdir(\"SLRT\")\n",
    "# print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scA37z3fs92A",
    "outputId": "1cc255a8-2037-4f2a-a287-039a67b64694"
   },
   "outputs": [],
   "source": [
    "# !pip install decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "SR5uDmzrseDF",
    "outputId": "2a0befcf-0fe3-4b99-88bb-1c54d508f28c"
   },
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "import warnings\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from modelling.model import build_model\n",
    "from utils.optimizer import build_optimizer, build_scheduler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import shutil\n",
    "import time\n",
    "import queue\n",
    "sys.path.append(os.getcwd())#slt dir\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP, distributed\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.misc import (\n",
    "    load_config,\n",
    "    log_cfg,\n",
    "    load_checkpoint,\n",
    "    make_writer,\n",
    "    set_seed,\n",
    "    symlink_update,\n",
    "    is_main_process, init_DDP, move_to_device,\n",
    "    neq_load_customized,\n",
    "    synchronize,\n",
    ")\n",
    "from utils.metrics import compute_accuracy\n",
    "from dataset.Dataloader import build_dataloader\n",
    "from dataset.Dataset import build_dataset\n",
    "from utils.progressbar import ProgressBar\n",
    "from copy import deepcopy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GK2V9OvW29W7"
   },
   "outputs": [],
   "source": [
    "with open(\"pickle_data/test.pkl\", \"rb\") as f:\n",
    "    test_split = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1HYYRCsj5tjX"
   },
   "outputs": [],
   "source": [
    "vocab_list = sorted({item['label'] for item in test_split})\n",
    "cls_num = len({item['label'] for item in test_split})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5OM8Iqg6QP2",
    "outputId": "0ab09a47-1590-4179-fa0e-282c63f1cda7"
   },
   "outputs": [],
   "source": [
    "cls_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3Q3nIISp6yYI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding matrix for 1999 classes...\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickle_data/wlasl_word_embeddings.pkl\", \"rb\") as f:\n",
    "    word_emb_tab = pickle.load(f)\n",
    "\n",
    "# 2. Determine embedding dimension (check the first item in the dict)\n",
    "first_key = list(word_emb_tab.keys())[0]\n",
    "first_val = word_emb_tab[first_key]\n",
    "embed_dim = len(first_val)\n",
    "\n",
    "# 3. Create the Tensor Matrix\n",
    "# Initialize with zeros or random noise\n",
    "embedding_tensor = torch.zeros((len(vocab_list), embed_dim))\n",
    "\n",
    "print(f\"Building embedding matrix for {len(vocab_list)} classes...\")\n",
    "\n",
    "# 4. Fill the matrix ensuring the order matches the class IDs\n",
    "for i, word in enumerate(vocab_list):\n",
    "    # Strip whitespace just in case\n",
    "    word_clean = word.strip() if isinstance(word, str) else word\n",
    "    \n",
    "    if word_clean in word_emb_tab:\n",
    "        # Convert list/numpy array to torch tensor\n",
    "        vector = torch.tensor(word_emb_tab[word_clean])\n",
    "        embedding_tensor[i] = vector\n",
    "    else:\n",
    "        # Optional: Print warning if a word in your dataset lacks an embedding\n",
    "        # print(f\"Warning: '{word}' not found in embedding dictionary.\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "-CADEj2Wxhy4",
    "outputId": "195be62c-7d6c-4cad-f01b-351423407bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite cfg.model.RecognitionNetwork.keypoint_s3d.in_channel -> 63\n",
      "Load pretrained S3D backbone from checkpoint\n",
      "['checkpoint/S3D_kinetics400.pt']\n",
      "\n",
      "=======Check Weights Loading======\n",
      "Weights not used from pretrained file:\n",
      "backbone.fc.0.weight\n",
      "backbone.fc.0.bias\n",
      "---------------------------\n",
      "Weights not loaded into new model:\n",
      "===================================\n",
      "\n",
      "Load pretrained S3D backbone from checkpoint\n",
      "['checkpoint/S3D_kinetics400.pt']\n",
      "\n",
      "=======Check Weights Loading======\n",
      "Weights not used from pretrained file:\n",
      "backbone.base.0.conv_s.weight\n",
      "backbone.fc.0.weight\n",
      "backbone.fc.0.bias\n",
      "---------------------------\n",
      "Weights not loaded into new model:\n",
      "backbone.base.0.conv_s.weight shape mis-matched, not loaded\n",
      "===================================\n",
      "\n",
      "Load pretrained S3D backbone from checkpoint\n",
      "['checkpoint/S3D_kinetics400.pt']\n",
      "\n",
      "=======Check Weights Loading======\n",
      "Weights not used from pretrained file:\n",
      "backbone.fc.0.weight\n",
      "backbone.fc.0.bias\n",
      "---------------------------\n",
      "Weights not loaded into new model:\n",
      "===================================\n",
      "\n",
      "Load pretrained S3D backbone from checkpoint\n",
      "['checkpoint/S3D_kinetics400.pt']\n",
      "\n",
      "=======Check Weights Loading======\n",
      "Weights not used from pretrained file:\n",
      "backbone.base.0.conv_s.weight\n",
      "backbone.fc.0.weight\n",
      "backbone.fc.0.bias\n",
      "---------------------------\n",
      "Weights not loaded into new model:\n",
      "backbone.base.0.conv_s.weight shape mis-matched, not loaded\n",
      "===================================\n",
      "\n",
      "\n",
      "=======Check Weights Loading======\n",
      "Weights not used from pretrained file:\n",
      "---------------------------\n",
      "Weights not loaded into new model:\n",
      "===================================\n",
      "\n",
      "Load model ckpt from checkpoint/best.ckpt\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\"SLT baseline Testing\")\n",
    "parser.add_argument(\"--config\", default=\"configs/nla_slr_wlasl_100.yaml\", type=str, help=\"Training configuration file (yaml).\")\n",
    "parser.add_argument(\"--save_subdir\", default='prediction', type=str)\n",
    "parser.add_argument('--ckpt_name', default='best.ckpt', type=str)\n",
    "parser.add_argument('--eval_setting', default='origin', type=str)\n",
    "args = parser.parse_args([])\n",
    "cfg = load_config(args.config)\n",
    "\n",
    "# Configure the environment variables for a single process (the notebook)\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355' # Any free port\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "# NOTE: Since you are setting device to 'cpu' in line 12, \n",
    "# you MUST use 'gloo' instead of 'nccl'. 'nccl' only works on CUDA GPUs.\n",
    "backend = \"nccl\" \n",
    "\n",
    "torch.distributed.init_process_group(\n",
    "    backend=backend, \n",
    "    init_method=\"env://\", \n",
    "    timeout=datetime.timedelta(0, 3600)\n",
    ")\n",
    "\n",
    "# Set LOCAL_RANK and WORLD_SIZE environment variables for single-process execution\n",
    "cfg['local_rank'] = 0; cfg['world_size']=1; cfg['device'] = torch.device('cuda:0')\n",
    "\n",
    "set_seed(seed=cfg[\"training\"].get(\"random_seed\", 42))\n",
    "model_dir = cfg['training']['model_dir']\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "model = build_model(cfg, cls_num, word_emb_tab=embedding_tensor)\n",
    "model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "#load model\n",
    "load_model_path = os.path.join('checkpoint',args.ckpt_name)\n",
    "if os.path.isfile(load_model_path):\n",
    "    state_dict = torch.load(load_model_path, map_location='cuda')\n",
    "    neq_load_customized(model, state_dict['model_state'], verbose=True)\n",
    "    epoch, global_step = state_dict.get('epoch',0), state_dict.get('global_step',0)\n",
    "    print('Load model ckpt from '+load_model_path)\n",
    "else:\n",
    "    print(f'{load_model_path} does not exist')\n",
    "    epoch, global_step = 0, 0\n",
    "\n",
    "model = DDP(model,\n",
    "        device_ids=[cfg['local_rank']],\n",
    "        output_device=cfg['local_rank'],\n",
    "        find_unused_parameters=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome To Colab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
