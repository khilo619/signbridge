{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import json\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"âœ… Imports loaded successfully!\")\n",
    "print(f\"ðŸ“Š TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ðŸ”§ Available CPUs: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Constants and Paths (Updated for 64 frames + ISLR)\n",
    "# UPDATED: 64 frames for complete sign capture\n",
    "TARGET_FRAMES = 64\n",
    "N_FACE = 468\n",
    "N_POSE = 33\n",
    "N_LEFT_HAND = 21\n",
    "N_RIGHT_HAND = 21\n",
    "TOTAL_LANDMARKS = N_FACE + N_POSE + N_LEFT_HAND + N_RIGHT_HAND  # 543\n",
    "INPUT_SHAPE = (TARGET_FRAMES, TOTAL_LANDMARKS * 3)  # (64, 1629)\n",
    "\n",
    "# Dataset Paths\n",
    "ISLR_TRAIN_PATH = \"/kaggle/input/wlasl2000-landmarks/train.tfrecord\"  # 23.36 GB\n",
    "ISLR_VAL_PATH = \"/kaggle/input/wlasl2000-landmarks/val.tfrecord\"      # 7.96 GB\n",
    "WLASL_MAPPING_PATH = \"/kaggle/input/wlasl200-mapping/label_mapping.json\"  # 100 words\n",
    "ISLR_MAPPING_PATH = \"/kaggle/input/wlasl2000-landmarks/sign_to_prediction_index_map.json\"  # 250 words\n",
    "\n",
    "print(\"âœ… Configuration set:\")\n",
    "print(f\"   Target frames: {TARGET_FRAMES}\")\n",
    "print(f\"   Total landmarks: {TOTAL_LANDMARKS}\")\n",
    "print(f\"   Input shape: {INPUT_SHAPE}\")\n",
    "print(\"   Expected dataset size: ~50,000 samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Label Mappings (ISLR 250 + WLASL 100)\n",
    "# Load ISLR 250-word mapping\n",
    "with open(ISLR_MAPPING_PATH, 'r') as f:\n",
    "    islr_mapping = json.load(f)\n",
    "\n",
    "# Load WLASL 100-word mapping  \n",
    "with open(WLASL_MAPPING_PATH, 'r') as f:\n",
    "    wlasl_mapping = json.load(f)\n",
    "\n",
    "# Create combined mapping (prioritize WLASL 100 words)\n",
    "target_words = set(wlasl_mapping['gloss_to_label'].keys())\n",
    "islr_words = set(islr_mapping.keys())\n",
    "common_words = target_words.intersection(islr_words)\n",
    "\n",
    "print(\"âœ… Label mappings loaded:\")\n",
    "print(f\"   ISLR words: {len(islr_mapping)}\")\n",
    "print(f\"   WLASL target words: {len(target_words)}\")\n",
    "print(f\"   Common words: {len(common_words)}\")\n",
    "print(f\"   Common words: {sorted(list(common_words))[:20]}...\")  # Show first 20\n",
    "\n",
    "# Create final mapping for common words only\n",
    "final_gloss_to_label = {}\n",
    "final_label_to_gloss = {}\n",
    "label_idx = 0\n",
    "\n",
    "for word in sorted(common_words):\n",
    "    final_gloss_to_label[word] = label_idx\n",
    "    final_label_to_gloss[str(label_idx)] = word\n",
    "    label_idx += 1\n",
    "\n",
    "num_classes = len(final_gloss_to_label)\n",
    "print(f\"\\nðŸŽ¯ Final model will train on {num_classes} common words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: TFRecord Parsing Functions\n",
    "def parse_tfrecord_example(example_proto):\n",
    "    \"\"\"Parse a single TFRecord example\"\"\"\n",
    "    # Define the feature description based on ISLR format\n",
    "    feature_description = {\n",
    "        'landmarks': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sequence_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    \n",
    "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Decode landmarks\n",
    "    landmarks = tf.io.decode_raw(parsed_features['landmarks'], tf.float32)\n",
    "    landmarks = tf.reshape(landmarks, [-1, TOTAL_LANDMARKS, 3])  # [frames, 543, 3]\n",
    "    \n",
    "    label = parsed_features['label']\n",
    "    sequence_id = parsed_features['sequence_id']\n",
    "    \n",
    "    return landmarks, label, sequence_id\n",
    "\n",
    "def process_landmarks_to_target_frames(landmarks, target_frames=TARGET_FRAMES):\n",
    "    \"\"\"Resample landmarks to target frame count\"\"\"\n",
    "    current_frames = tf.shape(landmarks)[0]\n",
    "    \n",
    "    # If already correct length, return as-is\n",
    "    if current_frames == target_frames:\n",
    "        return tf.reshape(landmarks, [target_frames, -1])  # Flatten to (64, 1629)\n",
    "    \n",
    "    # Resample using linear interpolation\n",
    "    # Take first 64 frames if longer, or repeat if shorter\n",
    "    if current_frames >= target_frames:\n",
    "        # Take evenly spaced frames\n",
    "        indices = tf.linspace(0.0, tf.cast(current_frames-1, tf.float32), target_frames)\n",
    "        indices = tf.cast(indices, tf.int32)\n",
    "        landmarks = tf.gather(landmarks, indices)\n",
    "    else:\n",
    "        # Pad with last frame if too short\n",
    "        padding = target_frames - current_frames\n",
    "        last_frame = landmarks[-1:]\n",
    "        padding_frames = tf.tile(last_frame, [padding, 1, 1])\n",
    "        landmarks = tf.concat([landmarks, padding_frames], axis=0)\n",
    "    \n",
    "    return tf.reshape(landmarks, [target_frames, -1])  # Flatten to (64, 1629)\n",
    "\n",
    "print(\"âœ… TFRecord parsing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load ISLR Dataset (This will take 10-15 minutes)\n",
    "def load_islr_dataset(tfrecord_path, islr_to_final_mapping, max_samples=None):\n",
    "    \"\"\"Load and process ISLR TFRecord dataset\"\"\"\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"ðŸ“¦ Loading from {tfrecord_path}...\")\n",
    "    \n",
    "    for i, record in enumerate(tqdm(dataset)):\n",
    "        if max_samples and i >= max_samples:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            landmarks, label, sequence_id = parse_tfrecord_example(record)\n",
    "            \n",
    "            # Convert TF tensors to numpy\n",
    "            landmarks_np = landmarks.numpy()\n",
    "            label_np = int(label.numpy())\n",
    "            \n",
    "            # Map ISLR label to word\n",
    "            islr_word = None\n",
    "            for word, islr_label in islr_mapping.items():\n",
    "                if islr_label == label_np:\n",
    "                    islr_word = word\n",
    "                    break\n",
    "            \n",
    "            # Check if word is in our target set\n",
    "            if islr_word and islr_word in final_gloss_to_label:\n",
    "                # Process landmarks to 64 frames\n",
    "                processed_landmarks = process_landmarks_to_target_frames(\n",
    "                    tf.constant(landmarks_np)\n",
    "                ).numpy()\n",
    "                \n",
    "                final_label = final_gloss_to_label[islr_word]\n",
    "                \n",
    "                X_data.append(processed_landmarks)\n",
    "                y_data.append(final_label)\n",
    "            else:\n",
    "                skipped += 1\n",
    "                \n",
    "        except Exception:\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"   Loaded: {len(X_data)} samples, Skipped: {skipped}\")\n",
    "    return np.array(X_data, dtype=np.float32), np.array(y_data, dtype=np.int32)\n",
    "\n",
    "# Load training data (this takes time!)\n",
    "print(\"ðŸš€ Loading ISLR training data (this will take 10-15 minutes)...\")\n",
    "X_train_islr, y_train_islr = load_islr_dataset(ISLR_TRAIN_PATH, islr_mapping)\n",
    "\n",
    "print(\"ðŸš€ Loading ISLR validation data...\")\n",
    "X_val_islr, y_val_islr = load_islr_dataset(ISLR_VAL_PATH, islr_mapping)\n",
    "\n",
    "print(\"\\nâœ… ISLR Dataset loaded:\")\n",
    "print(f\"   Train: {len(X_train_islr)} samples, shape: {X_train_islr.shape}\")\n",
    "print(f\"   Val: {len(X_val_islr)} samples, shape: {X_val_islr.shape}\")\n",
    "print(f\"   Total: {len(X_train_islr) + len(X_val_islr)} samples\")\n",
    "\n",
    "# Combine train and val for final training\n",
    "X_combined = np.concatenate([X_train_islr, X_val_islr], axis=0)\n",
    "y_combined = np.concatenate([y_train_islr, y_val_islr], axis=0)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Combined dataset: {len(X_combined)} samples for {num_classes} classes\")\n",
    "unique_labels, counts = np.unique(y_combined, return_counts=True)\n",
    "print(f\"   Samples per class: min={counts.min()}, max={counts.max()}, mean={counts.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train/Val Split (80/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_combined, y_combined, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_combined\n",
    ")\n",
    "\n",
    "print(\"âœ… Final Train/Val split:\")\n",
    "print(f\"   Train: {len(X_train)} samples ({len(X_train)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   Val: {len(X_val)} samples ({len(X_val)/len(X_combined)*100:.1f}%)\")\n",
    "print(f\"   Input shape: {X_train.shape[1:]}\")\n",
    "print(f\"   Classes: {num_classes}\")\n",
    "\n",
    "# Verify data quality\n",
    "print(\"\\nðŸ” Data quality check:\")\n",
    "print(f\"   Train data range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"   NaN values: {np.isnan(X_train).sum()}\")\n",
    "print(f\"   Inf values: {np.isinf(X_train).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Build Enhanced LSTM Model (Optimized for 64 frames)\n",
    "model = Sequential([\n",
    "    # First LSTM layer - extract temporal features\n",
    "    LSTM(128, return_sequences=True, activation='tanh', input_shape=INPUT_SHAPE),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Second LSTM layer - deeper temporal understanding  \n",
    "    LSTM(256, return_sequences=True, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Third LSTM layer - final temporal encoding\n",
    "    LSTM(128, return_sequences=False, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with optimized settings\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Enhanced LSTM Model built:\")\n",
    "print(f\"   Input: {INPUT_SHAPE}\")\n",
    "print(f\"   Output: {num_classes} classes\")\n",
    "print(f\"   Parameters: {model.count_params():,}\")\n",
    "print(f\"   Model size: ~{model.count_params() * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train Model (This will take 45-90 minutes)\n",
    "# Callbacks for optimal training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '/kaggle/working/lstm_islr_combined_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=8,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Starting training (estimated time: 45-90 minutes)...\")\n",
    "print(f\"   Dataset size: {len(X_train):,} training samples\")\n",
    "print(\"   Batch size: 64\")\n",
    "print(\"   Max epochs: 150\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=150,\n",
    "    batch_size=64,  # Larger batch for better stability\n",
    "    callbacks=[checkpoint, reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"   Total epochs: {len(history.history['loss'])}\")\n",
    "print(f\"   Best val accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate Model Performance\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model('/kaggle/working/lstm_islr_combined_best.h5')\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"ðŸ” Evaluating model performance...\")\n",
    "val_loss, val_acc, val_top5 = best_model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate additional metrics\n",
    "top3_acc = top_k_accuracy_score(y_val, y_pred, k=3)\n",
    "top5_acc = top_k_accuracy_score(y_val, y_pred, k=5)\n",
    "\n",
    "print(\"\\nðŸŽ¯ FINAL RESULTS:\")\n",
    "print(f\"   ðŸ“Š Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"   ðŸŽ¯ Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"   ðŸ¥‰ Top-3 Accuracy: {top3_acc*100:.2f}%\")\n",
    "print(f\"   ðŸ¥‡ Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nðŸ” Sample predictions (first 15):\")\n",
    "for i in range(min(15, len(y_val))):\n",
    "    true_word = final_label_to_gloss[str(y_val[i])]\n",
    "    pred_word = final_label_to_gloss[str(y_pred_classes[i])]\n",
    "    confidence = y_pred[i][y_pred_classes[i]] * 100\n",
    "    status = \"âœ…\" if y_val[i] == y_pred_classes[i] else \"âŒ\"\n",
    "    print(f\"   {status} True: {true_word:15s} | Pred: {pred_word:15s} ({confidence:.1f}%)\")\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nðŸ“ˆ Per-class performance:\")\n",
    "unique_labels = np.unique(y_val)\n",
    "class_accuracies = []\n",
    "for label in unique_labels:\n",
    "    mask = y_val == label\n",
    "    if mask.sum() > 0:\n",
    "        acc = (y_pred_classes[mask] == label).mean()\n",
    "        word = final_label_to_gloss[str(label)]\n",
    "        class_accuracies.append(acc)\n",
    "        if acc < 0.5:  # Show poorly performing classes\n",
    "            print(f\"   âš ï¸  {word:15s}: {acc*100:.1f}% ({mask.sum()} samples)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Overall class performance:\")\n",
    "print(f\"   Best class accuracy: {max(class_accuracies)*100:.1f}%\")\n",
    "print(f\"   Worst class accuracy: {min(class_accuracies)*100:.1f}%\")\n",
    "print(f\"   Average class accuracy: {np.mean(class_accuracies)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Model and Create Deployment Package\n",
    "# Save model in multiple formats\n",
    "print(\"ðŸ’¾ Saving model and creating deployment package...\")\n",
    "\n",
    "# Save Keras model\n",
    "best_model.save('/kaggle/working/lstm_islr_combined_final.h5')\n",
    "best_model.save('/kaggle/working/lstm_islr_combined_savedmodel')\n",
    "\n",
    "# Create comprehensive label mapping\n",
    "deployment_config = {\n",
    "    'model_info': {\n",
    "        'name': 'ISLR+WLASL Combined LSTM',\n",
    "        'version': '1.0',\n",
    "        'input_shape': list(INPUT_SHAPE),\n",
    "        'target_frames': TARGET_FRAMES,\n",
    "        'total_landmarks': TOTAL_LANDMARKS,\n",
    "        'num_classes': num_classes,\n",
    "        'training_samples': len(X_train),\n",
    "        'validation_samples': len(X_val)\n",
    "    },\n",
    "    'performance': {\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'val_loss': float(val_loss),\n",
    "        'top3_accuracy': float(top3_acc),\n",
    "        'top5_accuracy': float(top5_acc)\n",
    "    },\n",
    "    'labels': {\n",
    "        'gloss_to_label': final_gloss_to_label,\n",
    "        'label_to_gloss': final_label_to_gloss,\n",
    "        'num_classes': num_classes\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'target_frames': TARGET_FRAMES,\n",
    "        'landmarks_per_frame': TOTAL_LANDMARKS,\n",
    "        'coordinates_per_landmark': 3,\n",
    "        'input_flattened': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save deployment config\n",
    "with open('/kaggle/working/deployment_config.json', 'w') as f:\n",
    "    json.dump(deployment_config, f, indent=2)\n",
    "\n",
    "# Save training history\n",
    "history_data = {\n",
    "    'loss': history.history['loss'],\n",
    "    'accuracy': history.history['accuracy'],\n",
    "    'val_loss': history.history['val_loss'],\n",
    "    'val_accuracy': history.history['val_accuracy']\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/training_history.json', 'w') as f:\n",
    "    json.dump(history_data, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… DEPLOYMENT PACKAGE READY:\")\n",
    "print(\"   ðŸ“¦ lstm_islr_combined_final.h5 (Keras model)\")\n",
    "print(\"   ðŸ“¦ lstm_islr_combined_savedmodel/ (TensorFlow SavedModel)\")\n",
    "print(\"   ðŸ“¦ deployment_config.json (Complete configuration)\")\n",
    "print(\"   ðŸ“¦ training_history.json (Training metrics)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ MODEL SUMMARY:\")\n",
    "print(f\"   ðŸŽª Trained on: {len(X_train):,} samples\")\n",
    "print(f\"   ðŸŽ¯ Accuracy: {val_acc*100:.1f}%\")\n",
    "print(f\"   ðŸ“š Vocabulary: {num_classes} words\")\n",
    "print(f\"   âš¡ Input: {TARGET_FRAMES} frames ({TARGET_FRAMES/30:.1f}s @ 30fps)\")\n",
    "print(f\"   ðŸ’¾ Model size: ~{best_model.count_params() * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(\"\\nðŸ“¥ NEXT STEPS:\")\n",
    "print(\"   1. Download all 4 files from /kaggle/working/\")\n",
    "print(\"   2. Use deployment_config.json for webcam inference setup\")\n",
    "print(f\"   3. Expected real-world accuracy: {val_acc*0.8*100:.1f}-{val_acc*0.9*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
