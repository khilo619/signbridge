{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import interpolate\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"âœ… Imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Constants and Helper Functions\n",
    "TARGET_FRAMES = 30\n",
    "N_FACE = 468\n",
    "N_POSE = 33\n",
    "N_LEFT_HAND = 21\n",
    "N_RIGHT_HAND = 21\n",
    "TOTAL_LANDMARKS = N_FACE + N_POSE + N_LEFT_HAND + N_RIGHT_HAND\n",
    "\n",
    "def load_parquet_landmarks(file_path):\n",
    "    try:\n",
    "        return pd.read_parquet(file_path)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parquet_to_array(df):\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "    n_frames = df['frame'].max() + 1\n",
    "    data = np.zeros((n_frames, TOTAL_LANDMARKS, 3), dtype=np.float32)\n",
    "    for _, row in df.iterrows():\n",
    "        frame_idx = int(row['frame'])\n",
    "        lm_idx = int(row['landmark_index'])\n",
    "        lm_type = row['type']\n",
    "        if lm_type == 'face':\n",
    "            global_idx = lm_idx\n",
    "        elif lm_type == 'left_hand':\n",
    "            global_idx = N_FACE + lm_idx\n",
    "        elif lm_type == 'pose':\n",
    "            global_idx = N_FACE + N_LEFT_HAND + lm_idx\n",
    "        elif lm_type == 'right_hand':\n",
    "            global_idx = N_FACE + N_LEFT_HAND + N_POSE + lm_idx\n",
    "        else:\n",
    "            continue\n",
    "        if global_idx < TOTAL_LANDMARKS and frame_idx < n_frames:\n",
    "            data[frame_idx, global_idx, 0] = row['x']\n",
    "            data[frame_idx, global_idx, 1] = row['y']\n",
    "            data[frame_idx, global_idx, 2] = row['z']\n",
    "    return data\n",
    "\n",
    "def resample_sequence(data, target_len=TARGET_FRAMES):\n",
    "    current_len = data.shape[0]\n",
    "    if current_len == target_len:\n",
    "        return data\n",
    "    old_indices = np.linspace(0, current_len - 1, current_len)\n",
    "    new_indices = np.linspace(0, current_len - 1, target_len)\n",
    "    resampled = np.zeros((target_len, TOTAL_LANDMARKS, 3), dtype=np.float32)\n",
    "    for lm in range(TOTAL_LANDMARKS):\n",
    "        for coord in range(3):\n",
    "            f = interpolate.interp1d(old_indices, data[:, lm, coord], kind='linear')\n",
    "            resampled[:, lm, coord] = f(new_indices)\n",
    "    return resampled\n",
    "\n",
    "print(f\"âœ… Helper functions defined (Target frames: {TARGET_FRAMES}, Landmarks: {TOTAL_LANDMARKS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Label Mapping\n",
    "LABEL_MAPPING_PATH = \"/kaggle/input/wlasl200-mapping/label_mapping.json\"\n",
    "\n",
    "with open(LABEL_MAPPING_PATH, 'r') as f:\n",
    "    label_data = json.load(f)\n",
    "\n",
    "gloss_to_label = label_data['gloss_to_label']\n",
    "label_to_gloss = label_data['label_to_gloss']\n",
    "num_classes = label_data['num_classes']\n",
    "\n",
    "print(f\"âœ… Loaded {num_classes} word labels\")\n",
    "print(f\"   First 10: {list(gloss_to_label.keys())[:10]}\")\n",
    "print(f\"   Last 10: {list(gloss_to_label.keys())[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create Video-to-Word Mapping\n",
    "WLASL_DIR = \"/kaggle/input/wlasl2000-landmarks/wlasl_landmarks\"\n",
    "WLASL_FULL = \"/kaggle/input/wlasl-processed/WLASL_v0.3.json\"\n",
    "\n",
    "with open(WLASL_FULL, 'r') as f:\n",
    "    wlasl_full = json.load(f)\n",
    "\n",
    "video_to_word = {}\n",
    "word_to_videos = {}\n",
    "\n",
    "for entry in tqdm(wlasl_full, desc=\"Creating mapping\"):\n",
    "    word = entry.get('gloss', None)\n",
    "    if word and word in gloss_to_label:\n",
    "        instances = entry.get('instances', [])\n",
    "        for instance in instances:\n",
    "            video_id = instance.get('video_id', None)\n",
    "            if video_id:\n",
    "                video_to_word[video_id] = word\n",
    "                if word not in word_to_videos:\n",
    "                    word_to_videos[word] = []\n",
    "                word_to_videos[word].append(video_id)\n",
    "\n",
    "print(f\"âœ… Mapping created: {len(video_to_word)} videos for {len(word_to_videos)} words\")\n",
    "video_counts = [len(videos) for videos in word_to_videos.values()]\n",
    "print(f\"   Videos per word: min={min(video_counts)}, max={max(video_counts)}, mean={np.mean(video_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Dataset\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for word, videos in tqdm(word_to_videos.items(), desc=\"Loading dataset\"):\n",
    "    label = gloss_to_label[word]\n",
    "    for video_id in videos:\n",
    "        file_path = f\"{WLASL_DIR}/{video_id}.parquet\"\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "        df = load_parquet_landmarks(file_path)\n",
    "        if df is None:\n",
    "            continue\n",
    "        landmarks = parquet_to_array(df)\n",
    "        if landmarks is None:\n",
    "            continue\n",
    "        landmarks_resampled = resample_sequence(landmarks, TARGET_FRAMES)\n",
    "        landmarks_flat = landmarks_resampled.reshape(TARGET_FRAMES, -1)\n",
    "        X_data.append(landmarks_flat)\n",
    "        y_data.append(label)\n",
    "\n",
    "X_data = np.array(X_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.int32)\n",
    "\n",
    "print(f\"âœ… Dataset loaded: {len(X_data)} samples\")\n",
    "print(f\"   X shape: {X_data.shape}\")\n",
    "print(f\"   Unique classes: {len(np.unique(y_data))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train/Val Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "\n",
    "print(\"âœ… Train/Val split:\")\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Val: {len(X_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Build LSTM Model\n",
    "INPUT_SHAPE = (TARGET_FRAMES, TOTAL_LANDMARKS * 3)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, activation='relu', input_shape=INPUT_SHAPE),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… LSTM Model built\")\n",
    "print(f\"   Input: {INPUT_SHAPE}\")\n",
    "print(f\"   Output: {num_classes} classes\")\n",
    "print(f\"   Parameters: {model.count_params():,}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train Model\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '/kaggle/working/lstm_wlasl100_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint, reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluate Model\n",
    "best_model = tf.keras.models.load_model('/kaggle/working/lstm_wlasl100_best.h5')\n",
    "\n",
    "val_loss, val_acc = best_model.evaluate(X_val, y_val, verbose=0)\n",
    "y_pred = best_model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "top5_acc = top_k_accuracy_score(y_val, y_pred, k=5)\n",
    "\n",
    "print(\"âœ… Final Results:\")\n",
    "print(f\"   Val Loss: {val_loss:.4f}\")\n",
    "print(f\"   Val Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"   Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nðŸ” Sample predictions:\")\n",
    "for i in range(min(10, len(y_val))):\n",
    "    true_word = label_to_gloss[str(y_val[i])]\n",
    "    pred_word = label_to_gloss[str(y_pred_classes[i])]\n",
    "    confidence = y_pred[i][y_pred_classes[i]] * 100\n",
    "    status = \"âœ…\" if y_val[i] == y_pred_classes[i] else \"âŒ\"\n",
    "    print(f\"   {status} True: {true_word:15s} | Pred: {pred_word:15s} ({confidence:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Model and Labels\n",
    "model.save('/kaggle/working/lstm_wlasl100_final.h5')\n",
    "model.save('/kaggle/working/lstm_wlasl100_savedmodel')\n",
    "\n",
    "output_mapping = {\n",
    "    'gloss_to_label': gloss_to_label,\n",
    "    'label_to_gloss': label_to_gloss,\n",
    "    'num_classes': num_classes,\n",
    "    'model_info': {\n",
    "        'input_shape': list(INPUT_SHAPE),\n",
    "        'target_frames': TARGET_FRAMES,\n",
    "        'total_landmarks': TOTAL_LANDMARKS,\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'val_loss': float(val_loss)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/lstm_wlasl100_labels.json', 'w') as f:\n",
    "    json.dump(output_mapping, f, indent=2)\n",
    "\n",
    "print(\"âœ… Saved:\")\n",
    "print(\"   - lstm_wlasl100_final.h5\")\n",
    "print(\"   - lstm_wlasl100_savedmodel/\")\n",
    "print(\"   - lstm_wlasl100_labels.json\")\n",
    "print(\"\\nðŸ“¥ Download these files for local webcam inference\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
