{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Real-Time Inference (250 Classes)\n",
    "\n",
    "This notebook tests the pretrained Kaggle 1st-place ASL model with real-time webcam inference.\n",
    "\n",
    "**Features:**\n",
    "- MediaPipe landmark extraction\n",
    "- Transformer model (250 ASL signs)\n",
    "- 7 real-time inference tricks\n",
    "- Webcam integration\n",
    "\n",
    "**Run Order:** Execute cells 1-8 in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "# Run this once, then restart runtime if needed\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import mediapipe as mp\n",
    "    import cv2\n",
    "    print(\"âœ… All packages already installed!\")\n",
    "except (ImportError, ValueError):\n",
    "    print(\"Installing dependencies...\")\n",
    "    !pip uninstall -y numpy\n",
    "    !pip install numpy==1.26.4\n",
    "    !pip install -q mediapipe opencv-python\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"âš ï¸ Click 'Runtime' â†’ 'Restart runtime'\")\n",
    "    print(\"Then continue from Cell 2!\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"âœ… Imports successful!\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Setup Model Weights & Labels\n",
    "\n",
    "import json\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "# Model weights file (upload to /content/ folder)\n",
    "WEIGHTS_FILE = \"/content/islr-fp16-192-8-seed45-foldall-last.h5\"\n",
    "\n",
    "# Check if weights exist\n",
    "if not os.path.exists(WEIGHTS_FILE):\n",
    "    print(\"âš ï¸ Model weights not found!\")\n",
    "    print(\"ðŸ“¤ Please upload 'islr-fp16-192-8-seed45-foldall-last.h5' to Colab:\")\n",
    "    print(\"   1. Click the folder icon on the left sidebar\")\n",
    "    print(\"   2. Click the upload button\")\n",
    "    print(\"   3. Select the .h5 file from your computer\")\n",
    "    print(\"   4. Wait for upload to complete\")\n",
    "    print(\"   5. Re-run this cell\")\n",
    "    raise RuntimeError(\"Weights file not found. Please upload it first.\")\n",
    "else:\n",
    "    file_size = os.path.getsize(WEIGHTS_FILE)\n",
    "    print(f\"âœ… Model weights found: {file_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Download competition labels (250 signs)\n",
    "print(\"\\nðŸ“¥ Downloading 250 sign labels from Kaggle competition...\")\n",
    "\n",
    "# Setup Kaggle credentials if needed\n",
    "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
    "    print(\"ðŸ“¤ Upload your kaggle.json file:\")\n",
    "    uploaded = files.upload()\n",
    "    if 'kaggle.json' in uploaded:\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !mv kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "        print(\"âœ… Kaggle credentials configured!\")\n",
    "    else:\n",
    "        raise RuntimeError(\"kaggle.json not uploaded\")\n",
    "\n",
    "# Install Kaggle API\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Download the sign mapping file\n",
    "LABEL_FILE = \"sign_to_prediction_index_map.json\"\n",
    "\n",
    "if os.path.exists(LABEL_FILE):\n",
    "    print(f\"Found existing {LABEL_FILE}\")\n",
    "else:\n",
    "    print(f\"Downloading {LABEL_FILE} from asl-signs competition...\")\n",
    "    !kaggle competitions download -c asl-signs -f sign_to_prediction_index_map.json -p .\n",
    "    !unzip -o sign_to_prediction_index_map.json.zip\n",
    "    !rm sign_to_prediction_index_map.json.zip\n",
    "\n",
    "# Load labels\n",
    "with open(LABEL_FILE, 'r') as f:\n",
    "    sign_to_index = json.load(f)\n",
    "\n",
    "# Create reverse mapping (index -> sign)\n",
    "index_to_sign = {str(v): k for k, v in sign_to_index.items()}\n",
    "\n",
    "print(f\"\\nâœ… Setup complete!\")\n",
    "print(f\"   Model weights: {WEIGHTS_FILE}\")\n",
    "print(f\"   Labels: {len(sign_to_index)} signs\")\n",
    "print(f\"   Sample signs: {list(sign_to_index.keys())[:10]}\")\n",
    "print(f\"\\nYou can now use:\")\n",
    "print(f\"   - sign_to_index: dict mapping sign names to indices (0-249)\")\n",
    "print(f\"   - index_to_sign: dict mapping indices to sign names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: hoyso48 EXACT Model Architecture (from his Kaggle notebook)\n",
    "\n",
    "# Constants\n",
    "ROWS_PER_FRAME = 236  # 708 / 3 = 236 landmarks\n",
    "MAX_LEN = 384\n",
    "CHANNELS = ROWS_PER_FRAME * 3  # 708\n",
    "NUM_CLASSES = 250\n",
    "PAD = 0  # Padding value for masking\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# CUSTOM LAYERS (exact copy from hoyso48)\n",
    "# ==========================================\n",
    "\n",
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "\n",
    "class LateDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "      \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          dilation_rate=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='swish',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(inputs)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x = CausalDWConv1D(kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            use_bias=False,\n",
    "            name=name + '_dwconv')(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "\n",
    "        x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
    "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]\n",
    "\n",
    "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "        attn = self.drop1(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "    def apply(inputs):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([inputs, x])\n",
    "        attn_out = x\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([attn_out, x])\n",
    "        return x\n",
    "    return apply\n",
    "\n",
    "\n",
    "def get_model(max_len=MAX_LEN, dropout_step=0, dim=192):\n",
    "    inp = tf.keras.Input((max_len,CHANNELS))\n",
    "    # Include Masking layer to match training setup\n",
    "    x = tf.keras.layers.Masking(mask_value=PAD)(inp)\n",
    "    ksize = 17\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = LateDropout(0.8, start_step=dropout_step)(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n",
    "    return tf.keras.Model(inp, x)\n",
    "\n",
    "\n",
    "print(\"Building hoyso48 model (EXACT implementation)...\")\n",
    "print(f\"Input: {MAX_LEN} frames Ã— {CHANNELS} features (236 landmarks Ã— 3)\")\n",
    "model = get_model()\n",
    "\n",
    "print(f\"\\nðŸ“Š Model has {len(model.layers)} layers\")\n",
    "print(\"Layer names:\", [layer.name for layer in model.layers[:10]], \"...\")\n",
    "\n",
    "print(\"\\nLoading weights from /content/...\")\n",
    "# Use by_name=True to load weights by layer name (more flexible)\n",
    "try:\n",
    "    model.load_weights(WEIGHTS_FILE, by_name=True, skip_mismatch=False)\n",
    "    print(\"âœ… Weights loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error loading weights: {e}\")\n",
    "    print(\"\\nTrying with skip_mismatch=True...\")\n",
    "    model.load_weights(WEIGHTS_FILE, by_name=True, skip_mismatch=True)\n",
    "    print(\"âœ… Weights loaded (some layers skipped)\")\n",
    "\n",
    "print(\"\\nâœ… Model ready!\")\n",
    "print(f\"Input shape: {model.input_shape}\")\n",
    "print(f\"Output shape: {model.output_shape}\")\n",
    "print(f\"\\nðŸ“Š Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Verify parameter count\n",
    "total_params = model.count_params()\n",
    "expected_params = 1_836_569\n",
    "if total_params == expected_params:\n",
    "    print(f\"\\nâœ… PERFECT MATCH: {total_params:,} parameters!\")\n",
    "else:\n",
    "    diff = abs(total_params - expected_params)\n",
    "    print(f\"\\nâš ï¸ Parameters: {total_params:,} (expected: {expected_params:,}, diff: {diff:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: MediaPipe Landmark Extraction (hoyso48's format)\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# hoyso48 uses only LIPS (not full face)\n",
    "LIPS_IDXS = np.array([\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "])  # 40 landmarks\n",
    "\n",
    "LEFT_HAND_IDXS = np.arange(0, 21)   # 21 landmarks\n",
    "POSE_IDXS = np.arange(0, 33)        # 33 landmarks\n",
    "RIGHT_HAND_IDXS = np.arange(0, 21)  # 21 landmarks\n",
    "\n",
    "# Total: 40 + 21 + 33 + 21 = 115 landmarks\n",
    "# Padded to 236 landmarks (hoyso48's format)\n",
    "\n",
    "def extract_landmarks(frame, holistic):\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(image_rgb)\n",
    "    \n",
    "    # Initialize with zeros (236 landmarks Ã— 3 = 708 features)\n",
    "    landmarks = np.zeros((236, 3), dtype=np.float32)\n",
    "    \n",
    "    # Lips (40 landmarks) â†’ indices 0-39\n",
    "    if results.face_landmarks:\n",
    "        face_lms = np.array([[lm.x, lm.y, lm.z] for lm in results.face_landmarks.landmark])\n",
    "        landmarks[0:40] = face_lms[LIPS_IDXS]\n",
    "    \n",
    "    # Left hand (21 landmarks) â†’ indices 40-60\n",
    "    if results.left_hand_landmarks:\n",
    "        left_hand = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark])\n",
    "        landmarks[40:61] = left_hand\n",
    "    \n",
    "    # Pose (33 landmarks) â†’ indices 61-93\n",
    "    if results.pose_landmarks:\n",
    "        pose_lms = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark])\n",
    "        landmarks[61:94] = pose_lms\n",
    "    \n",
    "    # Right hand (21 landmarks) â†’ indices 94-114\n",
    "    if results.right_hand_landmarks:\n",
    "        right_hand = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark])\n",
    "        landmarks[94:115] = right_hand\n",
    "    \n",
    "    # Remaining indices 115-235 stay as zeros (padding)\n",
    "    \n",
    "    return landmarks, results\n",
    "\n",
    "def normalize_landmarks(landmarks):\n",
    "    mean = landmarks.mean(axis=0)\n",
    "    landmarks_centered = landmarks - mean\n",
    "    std = landmarks_centered.std()\n",
    "    if std > 0:\n",
    "        return landmarks_centered / std\n",
    "    return landmarks_centered\n",
    "\n",
    "print(\"âœ… MediaPipe ready (hoyso48's 236-landmark format)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Real-Time Detector (7 Inference Tricks)\n",
    "\n",
    "MAX_LEN = 384  # Match model's sequence length\n",
    "WINDOW_SIZE = 60\n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "SMOOTHING_WINDOW = 5\n",
    "MIN_SIGN_FRAMES = 20\n",
    "\n",
    "class RealTimeSignDetector:\n",
    "    def __init__(self, model, index_to_sign, max_len=384, window_size=60):\n",
    "        self.model = model\n",
    "        self.index_to_sign = index_to_sign\n",
    "        self.max_len = max_len\n",
    "        self.window_size = window_size\n",
    "        self.frame_buffer = deque(maxlen=window_size)\n",
    "        self.prediction_buffer = deque(maxlen=SMOOTHING_WINDOW)\n",
    "        self.is_signing = False\n",
    "        self.current_sign_frames = []\n",
    "        self.fps_buffer = deque(maxlen=30)\n",
    "    \n",
    "    def detect_sign_activity(self, landmarks):\n",
    "        left_hand = landmarks[LEFT_HAND_IDXS]\n",
    "        right_hand = landmarks[RIGHT_HAND_IDXS]\n",
    "        return (left_hand.sum() != 0) or (right_hand.sum() != 0)\n",
    "    \n",
    "    def process_frame(self, landmarks):\n",
    "        landmarks_norm = normalize_landmarks(landmarks)\n",
    "        self.frame_buffer.append(landmarks_norm)\n",
    "        is_active = self.detect_sign_activity(landmarks_norm)\n",
    "        \n",
    "        if is_active:\n",
    "            if not self.is_signing:\n",
    "                self.is_signing = True\n",
    "                self.current_sign_frames = []\n",
    "            self.current_sign_frames.append(landmarks_norm)\n",
    "        else:\n",
    "            if self.is_signing and len(self.current_sign_frames) >= MIN_SIGN_FRAMES:\n",
    "                return self.predict_sign()\n",
    "            self.is_signing = False\n",
    "            self.current_sign_frames = []\n",
    "        \n",
    "        return None, 0.0, []\n",
    "    \n",
    "    def predict_sign(self):\n",
    "        if len(self.frame_buffer) < MIN_SIGN_FRAMES:\n",
    "            return None, 0.0, []\n",
    "        \n",
    "        sequence = np.array(list(self.frame_buffer))\n",
    "        \n",
    "        if len(sequence) < self.max_len:\n",
    "            padding = np.zeros((self.max_len - len(sequence), 543, 3))\n",
    "            sequence = np.concatenate([sequence, padding], axis=0)\n",
    "        else:\n",
    "            sequence = sequence[-self.max_len:]\n",
    "        \n",
    "        # Flatten to (max_len, 543*3)\n",
    "        sequence = sequence.reshape(self.max_len, -1)\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        predictions = self.model.predict(sequence, verbose=0)[0]\n",
    "        \n",
    "        top_k_indices = np.argsort(predictions)[-5:][::-1]\n",
    "        top_k_probs = predictions[top_k_indices]\n",
    "        top_k_signs = [(self.index_to_sign[str(idx)], prob) for idx, prob in zip(top_k_indices, top_k_probs)]\n",
    "        \n",
    "        self.prediction_buffer.append((top_k_indices[0], top_k_probs[0]))\n",
    "        \n",
    "        if len(self.prediction_buffer) >= SMOOTHING_WINDOW:\n",
    "            avg_pred_idx = max(set([p[0] for p in self.prediction_buffer]), \n",
    "                             key=lambda x: sum([p[1] for p in self.prediction_buffer if p[0] == x]))\n",
    "            avg_confidence = np.mean([p[1] for p in self.prediction_buffer if p[0] == avg_pred_idx])\n",
    "            \n",
    "            if avg_confidence >= CONFIDENCE_THRESHOLD:\n",
    "                predicted_sign = self.index_to_sign[str(avg_pred_idx)]\n",
    "                return predicted_sign, avg_confidence, top_k_signs\n",
    "        \n",
    "        return None, 0.0, top_k_signs\n",
    "\n",
    "# Create reverse mapping\n",
    "index_to_sign = {str(v): k for k, v in sign_to_index.items()}\n",
    "\n",
    "print(\"âœ… Detector ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Webcam Inference (Run this to start!)\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "    js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "    display(js)\n",
    "    data = eval_js('takePhoto({})'.format(quality))\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(binary)\n",
    "    return filename\n",
    "\n",
    "# Initialize\n",
    "detector = RealTimeSignDetector(model, index_to_sign, max_len=MAX_LEN, window_size=WINDOW_SIZE)\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "print(\"ðŸŽ¥ Webcam Inference Ready!\")\n",
    "print(\"Click 'Capture' button to test a frame\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Capture and process\n",
    "try:\n",
    "    filename = take_photo()\n",
    "    frame = cv2.imread(filename)\n",
    "    \n",
    "    landmarks, results = extract_landmarks(frame, holistic)\n",
    "    predicted_sign, confidence, top_k = detector.process_frame(landmarks)\n",
    "    \n",
    "    if predicted_sign:\n",
    "        print(f\"âœ… Detected: {predicted_sign} ({confidence:.2%})\")\n",
    "        print(f\"Top-5: {', '.join([f'{s}({p:.2%})' for s, p in top_k[:5]])}\")\n",
    "    else:\n",
    "        print(\"No sign detected (try performing a sign)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    holistic.close()\n",
    "\n",
    "print(\"\\nðŸ’¡ Note: For continuous inference, use a local setup with OpenCV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "**Colab Limitations:**\n",
    "\n",
    "- Webcam access in Colab is limited (single frame capture)\n",
    "- For real-time continuous inference, run locally with `cv2.VideoCapture(0)`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Wait for your fine-tuned 200-class model from Kaggle\n",
    "2. Replace the model file and labels\n",
    "3. Test with your custom vocabulary\n",
    "\n",
    "**Performance:**\n",
    "- GPU inference: ~30-50ms per prediction\n",
    "- Expected FPS: 20-30 FPS on local setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
