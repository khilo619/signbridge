{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import subprocess\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = \"/kaggle/working/msasl_subset\"\n",
    "VIDEO_DIR = os.path.join(OUTPUT_DIR, \"videos\")\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "# The 55 Target Classes for SignBridge Demo\n",
    "TARGET_GLOSSES = [\n",
    "    # Old Classes (25)\n",
    "    \"need\", \"who\", \"no\", \"like\", \"work\", \"before\", \"go\", \"time\", \"later\", \"fine\", \n",
    "    \"eat\", \"want\", \"woman\", \"drink\", \"help\", \"yes\", \"hot\", \"mother\", \"now\", \"what\", \n",
    "    \"family\", \"man\", \"pizza\", \"school\", \"how\",\n",
    "    # New Classes (30)\n",
    "    \"i\", \"tomorrow\", \"happy\", \"father\", \"friend\", \"your\", \"water\", \"why\", \"home\", \"stay\", \n",
    "    \"cold\", \"they\", \"sick\", \"good\", \"know\", \"you\", \"day\", \"where\", \"tired\", \"please\", \n",
    "    \"when\", \"hello\", \"sorry\", \"my\", \"ok\", \"hungry\", \"see\", \"we\", \"he\", \"have\"\n",
    "]\n",
    "\n",
    "print(f\"Targeting {len(TARGET_GLOSSES)} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ccd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup MS-ASL Metadata Paths\n",
    "# We use the pre-uploaded Kaggle dataset instead of downloading from GitHub\n",
    "MSASL_DIR = \"/kaggle/input/ms-asl/MS-ASL\"\n",
    "\n",
    "TRAIN_JSON = os.path.join(MSASL_DIR, \"MSASL_train.json\")\n",
    "VAL_JSON = os.path.join(MSASL_DIR, \"MSASL_val.json\")\n",
    "TEST_JSON = os.path.join(MSASL_DIR, \"MSASL_test.json\")\n",
    "\n",
    "# Verify existence\n",
    "for p in [TRAIN_JSON, VAL_JSON, TEST_JSON]:\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"⚠️ Warning: Could not find {p}\")\n",
    "        print(\"Please ensure you have added the 'ms-asl' dataset to your notebook.\")\n",
    "    else:\n",
    "        print(f\"✅ Found {os.path.basename(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ecd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load and Filter Data\n",
    "def load_and_filter(json_path, split_name):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    filtered = []\n",
    "    for entry in data:\n",
    "        # Normalize gloss text (lowercase, strip)\n",
    "        gloss = entry['clean_text'].lower().strip()\n",
    "        \n",
    "        if gloss in TARGET_GLOSSES:\n",
    "            # Add split info\n",
    "            entry['split'] = split_name\n",
    "            # Create a safe filename\n",
    "            safe_gloss = gloss.replace(\" \", \"_\")\n",
    "            # MS-ASL format: org_video_id + start_time + end_time\n",
    "            # We'll make a unique filename\n",
    "            filename = f\"{safe_gloss}_{entry['org_text']}_{entry['start_time']}_{entry['end_time']}.mp4\"\n",
    "            # Clean filename of weird chars\n",
    "            filename = \"\".join([c for c in filename if c.isalnum() or c in ('_', '.', '-')])\n",
    "            entry['filename'] = filename\n",
    "            filtered.append(entry)\n",
    "            \n",
    "    return filtered\n",
    "\n",
    "# Use the paths defined in the previous cell\n",
    "train_data = load_and_filter(TRAIN_JSON, 'train')\n",
    "val_data = load_and_filter(VAL_JSON, 'val')\n",
    "test_data = load_and_filter(TEST_JSON, 'test')\n",
    "\n",
    "all_samples = train_data + val_data + test_data\n",
    "print(f\"Found {len(all_samples)} total samples for our 55 classes.\")\n",
    "\n",
    "# Save the filtered manifest\n",
    "manifest_path = os.path.join(OUTPUT_DIR, \"msasl_subset.json\")\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(all_samples, f, indent=2)\n",
    "    \n",
    "# Show distribution\n",
    "df = pd.DataFrame(all_samples)\n",
    "print(\"Top 10 classes by count:\")\n",
    "print(df['clean_text'].value_counts().head(10))\n",
    "print(\"Bottom 10 classes by count:\")\n",
    "print(df['clean_text'].value_counts().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download Function\n",
    "def download_clip(url, start_time, end_time, output_path):\n",
    "    \"\"\"\n",
    "    Downloads a specific section of a YouTube video using yt-dlp.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_path):\n",
    "        return True # Already exists\n",
    "        \n",
    "    # yt-dlp command to download specific section\n",
    "    # --download-sections \"*start-end\"\n",
    "    # -f best[ext=mp4] to get mp4\n",
    "    \n",
    "    cmd = [\n",
    "        \"yt-dlp\",\n",
    "        \"--quiet\",\n",
    "        \"--no-warnings\",\n",
    "        \"--force-keyframes-at-cuts\", # Crucial for accurate trimming\n",
    "        \"--download-sections\", f\"*{start_time}-{end_time}\",\n",
    "        \"-f\", \"best[ext=mp4]/best\",\n",
    "        \"-o\", output_path,\n",
    "        url\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, timeout=60) # 60s timeout per clip\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Run Download Loop (Parallel)\n",
    "import concurrent.futures\n",
    "\n",
    "# We shuffle to avoid hitting the same channel sequentially (helps avoid some blocks)\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "# Kaggle typically provides 4 vCPUs. \n",
    "# We set max_workers to 4 to maximize throughput.\n",
    "MAX_WORKERS = 4 \n",
    "\n",
    "print(f\"Starting download of {len(all_samples)} clips using {MAX_WORKERS} workers...\")\n",
    "print(\"This may take a while. Grab a coffee ☕\")\n",
    "\n",
    "def process_sample(sample):\n",
    "    url = sample['url']\n",
    "    start = sample['start_time']\n",
    "    end = sample['end_time']\n",
    "    path = os.path.join(VIDEO_DIR, sample['filename'])\n",
    "    \n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(path):\n",
    "        return True\n",
    "        \n",
    "    # Attempt download\n",
    "    return download_clip(url, start, end, path)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {executor.submit(process_sample, sample): sample for sample in all_samples}\n",
    "    \n",
    "    # Use tqdm to track progress as tasks complete\n",
    "    pbar = tqdm(concurrent.futures.as_completed(futures), total=len(all_samples))\n",
    "    \n",
    "    for future in pbar:\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                success_count += 1\n",
    "            else:\n",
    "                fail_count += 1\n",
    "        except Exception:\n",
    "            fail_count += 1\n",
    "            \n",
    "        pbar.set_description(f\"Success: {success_count} | Fail: {fail_count}\")\n",
    "\n",
    "print(f\"Download complete.\")\n",
    "print(f\"Successfully downloaded: {success_count}\")\n",
    "print(f\"Failed (deleted/private/blocked): {fail_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Zip Output for Export\n",
    "# This makes it easy to download or create a dataset\n",
    "import shutil\n",
    "\n",
    "print(\"Zipping dataset...\")\n",
    "shutil.make_archive(\"/kaggle/working/msasl_55_subset\", 'zip', OUTPUT_DIR)\n",
    "print(\"Done! You can now find 'msasl_55_subset.zip' in the output files.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
