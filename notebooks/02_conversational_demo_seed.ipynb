{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "6ed7621b",
   "metadata": {
    "papermill": {
     "duration": 0.005568,
     "end_time": "2025-12-09T22:50:54.597700",
     "exception": false,
     "start_time": "2025-12-09T22:50:54.592132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SignBridge Conversational Demo Seed (v4 - 55 Classes)\n",
=======
   "id": "92704c9e",
   "metadata": {},
   "source": [
    "# SignBridge Conversational Demo Seed (v3 - 55 Classes)\n",
>>>>>>> khaled
    "\n",
    "This notebook prepares a **conversational demo vocabulary** and a **demo model seed** for real-time ASL recognition.\n",
    "\n",
    "We will:\n",
<<<<<<< HEAD
    "- Load WLASL, ASL Citizen, and **MS-ASL** manifests (from Kaggle).\n",
=======
    "- Load WLASL and ASL Citizen manifests (from Kaggle).\n",
>>>>>>> khaled
    "- Select a compact set of **55 glosses**: pronouns (i, you, we, he, they), core verbs, and everyday nouns.\n",
    "- Build a dedicated `label_mapping_demo.json` for this vocabulary.\n",
    "- Prepare a training manifest and fine-tune a **demo model** starting from the 87.6% checkpoint.\n",
    "\n",
<<<<<<< HEAD
    "---\n",
    "\n",
    "## üì¶ Required Kaggle Datasets (Add these to your notebook!)\n",
    "\n",
    "| Dataset Name | Expected Path | Contents |\n",
    "|--------------|---------------|----------|\n",
    "| `wlasl-processed` | `/kaggle/input/wlasl-processed` | WLASL_v0.3.json + videos/ |\n",
    "| `asl-citizen` | `/kaggle/input/asl-citizen` | ASL_Citizen/splits/*.csv + videos/ |\n",
    "| `ms-asl` | `/kaggle/input/ms-asl` | msasl_subset.json + videos/ |\n",
    "| `best-model-87` | `/kaggle/input/best-model-87` | best_model_citizen100_87pct.pth |\n",
    "| `label-map-100` | `/kaggle/input/label-map-100` | label_mapping.json |\n",
    "\n",
    "---\n",
    "\n",
=======
>>>>>>> khaled
    "**Version History:**\n",
    "- v1: 58 classes (initial)\n",
    "- v2: 53 classes (removed 5 weak: food, goodbye, today, feel, me)\n",
    "- v3: 55 classes (added he, they for third-person pronouns)\n",
<<<<<<< HEAD
    "- v4: **MS-ASL integration** (added ~1000 clips for underrepresented classes like \"i\", \"he\", \"they\")\n",
=======
>>>>>>> khaled
    "\n",
    "> **Important:** This notebook is designed to run on **Kaggle**. Paths below assume Kaggle input mounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "051e3b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:54.607358Z",
     "iopub.status.busy": "2025-12-09T22:50:54.607107Z",
     "iopub.status.idle": "2025-12-09T22:50:56.029538Z",
     "shell.execute_reply": "2025-12-09T22:50:56.028561Z"
    },
    "papermill": {
     "duration": 1.428721,
     "end_time": "2025-12-09T22:50:56.030917",
     "exception": false,
     "start_time": "2025-12-09T22:50:54.602196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WLASL_JSON = /kaggle/input/wlasl-processed/WLASL_v0.3.json\n",
      "CITIZEN_SPLITS_DIR = /kaggle/input/asl-citizen/ASL_Citizen/splits\n",
      "MANIFESTS_DIR = /kaggle/working/SignBridge_demo/manifests\n",
      "\n",
      "--- Verifying paths ---\n",
      "WLASL_JSON exists: True\n",
      "CITIZEN_SPLITS_DIR exists: True\n"
     ]
    }
   ],
=======
   "id": "19fce67d",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 1: Imports & Paths (Kaggle-style) ----------\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Kaggle input paths (adjust if your dataset names differ)\n",
    "WLASL_INPUT = \"/kaggle/input/wlasl-processed\"\n",
    "CITIZEN_INPUT = \"/kaggle/input/asl-citizen\"\n",
<<<<<<< HEAD
    "MSASL_INPUT = \"/kaggle/input/ms-asl\"  # <--- MS-ASL Dataset (your uploaded subset)\n",
=======
>>>>>>> khaled
    "\n",
    "# WLASL main manifest (JSON)\n",
    "WLASL_JSON = os.path.join(WLASL_INPUT, \"WLASL_v0.3.json\")\n",
    "\n",
    "# ASL Citizen splits directory\n",
    "CITIZEN_SPLITS_DIR = os.path.join(CITIZEN_INPUT, \"ASL_Citizen\", \"splits\")\n",
    "\n",
<<<<<<< HEAD
    "# MS-ASL paths (direct structure - videos folder + manifest JSON)\n",
    "MSASL_JSON = os.path.join(MSASL_INPUT, \"msasl_subset.json\")\n",
    "MSASL_VIDEOS_DIR = os.path.join(MSASL_INPUT, \"videos\")\n",
    "\n",
=======
>>>>>>> khaled
    "# Output directory for manifests and label mappings inside Kaggle working dir\n",
    "BASE_OUTPUT_DIR = \"/kaggle/working/SignBridge_demo\"\n",
    "MANIFESTS_DIR = os.path.join(BASE_OUTPUT_DIR, \"manifests\")\n",
    "os.makedirs(MANIFESTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"WLASL_JSON =\", WLASL_JSON)\n",
    "print(\"CITIZEN_SPLITS_DIR =\", CITIZEN_SPLITS_DIR)\n",
<<<<<<< HEAD
    "print(\"MSASL_JSON =\", MSASL_JSON)\n",
=======
>>>>>>> khaled
    "print(\"MANIFESTS_DIR =\", MANIFESTS_DIR)\n",
    "\n",
    "# Verify files exist\n",
    "print(\"\\n--- Verifying paths ---\")\n",
    "print(f\"WLASL_JSON exists: {os.path.exists(WLASL_JSON)}\")\n",
    "print(f\"CITIZEN_SPLITS_DIR exists: {os.path.exists(CITIZEN_SPLITS_DIR)}\")\n",
<<<<<<< HEAD
    "print(f\"MSASL_JSON exists: {os.path.exists(MSASL_JSON)}\")\n",
    "print(f\"MSASL_VIDEOS_DIR exists: {os.path.exists(MSASL_VIDEOS_DIR)}\")\n",
=======
>>>>>>> khaled
    "\n",
    "# Demo-word selection philosophy (summary):\n",
    "# - Start from all glosses in WLASL and (optionally) ASL Citizen.\n",
    "# - Focus on a compact set (e.g., 30‚Äì50) of:\n",
    "#   * Pronouns: I, YOU, WE, THEY, etc.\n",
    "#   * Core verbs: WANT, NEED, LIKE, GO, COME, HELP, KNOW, WORK, etc.\n",
    "#   * Everyday nouns: FAMILY, FRIEND, SCHOOL, HOME, TIME, YEAR, etc.\n",
    "# - Prefer glosses that:\n",
    "#   * Are reasonably frequent in WLASL/Citizen (enough videos to train).\n",
    "#   * Are already present in the existing 100-label mapping when possible,\n",
    "#     to maximize reuse of the 87.6% checkpoint.\n",
    "# - Result: a small, conversational vocabulary for a robust real-time demo\n",
    "#   model, separate from the full 100-label general model."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "0cb8df77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:56.041857Z",
     "iopub.status.busy": "2025-12-09T22:50:56.041164Z",
     "iopub.status.idle": "2025-12-09T22:50:56.213579Z",
     "shell.execute_reply": "2025-12-09T22:50:56.212622Z"
    },
    "papermill": {
     "duration": 0.179093,
     "end_time": "2025-12-09T22:50:56.214964",
     "exception": false,
     "start_time": "2025-12-09T22:50:56.035871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded WLASL manifest: 2000 total glosses\n",
      "‚úÖ Total unique glosses: 2000\n",
      "‚úÖ Total video instances: 21083\n",
      "\n",
      "üìä Top 30 glosses by instance count:\n",
      "   1. book                 ‚Üí 40 instances\n",
      "   2. drink                ‚Üí 35 instances\n",
      "   3. computer             ‚Üí 30 instances\n",
      "   4. before               ‚Üí 26 instances\n",
      "   5. chair                ‚Üí 26 instances\n",
      "   6. go                   ‚Üí 26 instances\n",
      "   7. clothes              ‚Üí 25 instances\n",
      "   8. who                  ‚Üí 25 instances\n",
      "   9. candy                ‚Üí 24 instances\n",
      "  10. cousin               ‚Üí 23 instances\n",
      "  11. deaf                 ‚Üí 23 instances\n",
      "  12. fine                 ‚Üí 22 instances\n",
      "  13. help                 ‚Üí 22 instances\n",
      "  14. no                   ‚Üí 22 instances\n",
      "  15. thin                 ‚Üí 22 instances\n",
      "  16. walk                 ‚Üí 22 instances\n",
      "  17. year                 ‚Üí 22 instances\n",
      "  18. yes                  ‚Üí 22 instances\n",
      "  19. all                  ‚Üí 21 instances\n",
      "  20. black                ‚Üí 21 instances\n",
      "  21. cool                 ‚Üí 21 instances\n",
      "  22. finish               ‚Üí 21 instances\n",
      "  23. hot                  ‚Üí 21 instances\n",
      "  24. like                 ‚Üí 21 instances\n",
      "  25. many                 ‚Üí 21 instances\n",
      "  26. mother               ‚Üí 21 instances\n",
      "  27. now                  ‚Üí 21 instances\n",
      "  28. orange               ‚Üí 21 instances\n",
      "  29. table                ‚Üí 21 instances\n",
      "  30. thanksgiving         ‚Üí 21 instances\n",
      "\n",
      "üìà Instance count stats:\n",
      "   Min: 6, Max: 40, Median: 10\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "5cdf6f81",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 2: Load WLASL Manifest & Count Instances per Gloss ----------\n",
    "\n",
    "with open(WLASL_JSON, \"r\") as f:\n",
    "    wlasl_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded WLASL manifest: {len(wlasl_data)} total glosses\")\n",
    "\n",
    "# Count instances per gloss\n",
    "gloss_instance_counts = {}\n",
    "for entry in wlasl_data:\n",
    "    gloss = entry[\"gloss\"].lower().strip()\n",
    "    num_instances = len(entry.get(\"instances\", []))\n",
    "    gloss_instance_counts[gloss] = num_instances\n",
    "\n",
    "# Sort by frequency (descending)\n",
    "sorted_glosses = sorted(gloss_instance_counts.items(), key=lambda x: -x[1])\n",
    "\n",
    "print(f\"‚úÖ Total unique glosses: {len(sorted_glosses)}\")\n",
    "print(f\"‚úÖ Total video instances: {sum(gloss_instance_counts.values())}\")\n",
    "\n",
    "# Show top 30 most frequent glosses\n",
    "print(\"\\nüìä Top 30 glosses by instance count:\")\n",
    "for i, (gloss, count) in enumerate(sorted_glosses[:30], 1):\n",
    "    print(f\"  {i:2d}. {gloss:20s} ‚Üí {count} instances\")\n",
    "\n",
    "# Show some stats\n",
    "counts = list(gloss_instance_counts.values())\n",
    "print(f\"\\nüìà Instance count stats:\")\n",
    "print(f\"   Min: {min(counts)}, Max: {max(counts)}, Median: {sorted(counts)[len(counts)//2]}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "1bdfad26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:56.225816Z",
     "iopub.status.busy": "2025-12-09T22:50:56.225578Z",
     "iopub.status.idle": "2025-12-09T22:50:56.240219Z",
     "shell.execute_reply": "2025-12-09T22:50:56.238624Z"
    },
    "papermill": {
     "duration": 0.021385,
     "end_time": "2025-12-09T22:50:56.241271",
     "exception": false,
     "start_time": "2025-12-09T22:50:56.219886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Current 100-label vocabulary loaded: 100 glosses\n",
      "‚úÖ Target conversational glosses: 144 words\n",
      "\n",
      "üìä Target glosses available in WLASL: 140/144\n",
      "‚ùå Target glosses NOT in WLASL: 4\n",
      "\n",
      "‚úÖ Already in our 100-label model: 35 glosses\n",
      "üÜï NEW glosses to add (exist in WLASL, not in our 100): 105 glosses\n",
      "\n",
      "üÜï NEW conversational glosses available in WLASL:\n",
      "   water           ‚Üí 18 instances\n",
      "   wait            ‚Üí 18 instances\n",
      "   visit           ‚Üí 18 instances\n",
      "   yesterday       ‚Üí 17 instances\n",
      "   run             ‚Üí 17 instances\n",
      "   learn           ‚Üí 17 instances\n",
      "   take            ‚Üí 17 instances\n",
      "   brother         ‚Üí 17 instances\n",
      "   why             ‚Üí 17 instances\n",
      "   read            ‚Üí 17 instances\n",
      "   sick            ‚Üí 17 instances\n",
      "   week            ‚Üí 17 instances\n",
      "   write           ‚Üí 17 instances\n",
      "   have            ‚Üí 17 instances\n",
      "   friend          ‚Üí 16 instances\n",
      "   old             ‚Üí 16 instances\n",
      "   daughter        ‚Üí 16 instances\n",
      "   cold            ‚Üí 16 instances\n",
      "   tired           ‚Üí 16 instances\n",
      "   stay            ‚Üí 16 instances\n",
      "   leave           ‚Üí 16 instances\n",
      "   know            ‚Üí 16 instances\n",
      "   you             ‚Üí 16 instances\n",
      "   buy             ‚Üí 16 instances\n",
      "   again           ‚Üí 16 instances\n",
      "   remember        ‚Üí 16 instances\n",
      "   call            ‚Üí 16 instances\n",
      "   home            ‚Üí 16 instances\n",
      "   bad             ‚Üí 16 instances\n",
      "   happy           ‚Üí 16 instances\n",
      "   name            ‚Üí 16 instances\n",
      "   good            ‚Üí 16 instances\n",
      "   person          ‚Üí 16 instances\n",
      "   make            ‚Üí 15 instances\n",
      "   please          ‚Üí 15 instances\n",
      "   your            ‚Üí 15 instances\n",
      "   office          ‚Üí 15 instances\n",
      "   small           ‚Üí 15 instances\n",
      "   house           ‚Üí 15 instances\n",
      "   where           ‚Üí 15 instances\n",
      "   more            ‚Üí 15 instances\n",
      "   day             ‚Üí 15 instances\n",
      "   new             ‚Üí 15 instances\n",
      "   hear            ‚Üí 15 instances\n",
      "   live            ‚Üí 15 instances\n",
      "   feel            ‚Üí 15 instances\n",
      "   teach           ‚Üí 15 instances\n",
      "   arrive          ‚Üí 15 instances\n",
      "   money           ‚Üí 14 instances\n",
      "   use             ‚Üí 14 instances\n",
      "   hard            ‚Üí 14 instances\n",
      "   sister          ‚Üí 14 instances\n",
      "   ok              ‚Üí 14 instances\n",
      "   easy            ‚Üí 14 instances\n",
      "   sorry           ‚Üí 14 instances\n",
      "   move            ‚Üí 14 instances\n",
      "   talk            ‚Üí 14 instances\n",
      "   pay             ‚Üí 14 instances\n",
      "   today           ‚Üí 14 instances\n",
      "   month           ‚Üí 14 instances\n",
      "   when            ‚Üí 14 instances\n",
      "   think           ‚Üí 14 instances\n",
      "   baby            ‚Üí 14 instances\n",
      "   father          ‚Üí 14 instances\n",
      "   which           ‚Üí 14 instances\n",
      "   understand      ‚Üí 14 instances\n",
      "   sleep           ‚Üí 14 instances\n",
      "   sad             ‚Üí 14 instances\n",
      "   get             ‚Üí 13 instances\n",
      "   child           ‚Üí 13 instances\n",
      "   people          ‚Üí 13 instances\n",
      "   tomorrow        ‚Üí 13 instances\n",
      "   hospital        ‚Üí 13 instances\n",
      "   afternoon       ‚Üí 13 instances\n",
      "   hello           ‚Üí 13 instances\n",
      "   morning         ‚Üí 13 instances\n",
      "   night           ‚Üí 13 instances\n",
      "   sit             ‚Üí 13 instances\n",
      "   love            ‚Üí 12 instances\n",
      "   my              ‚Üí 12 instances\n",
      "   after           ‚Üí 12 instances\n",
      "   sell            ‚Üí 12 instances\n",
      "   food            ‚Üí 12 instances\n",
      "   big             ‚Üí 12 instances\n",
      "   hungry          ‚Üí 12 instances\n",
      "   ask             ‚Üí 11 instances\n",
      "   hate            ‚Üí 11 instances\n",
      "   stop            ‚Üí 11 instances\n",
      "   put             ‚Üí 11 instances\n",
      "   stand           ‚Üí 11 instances\n",
      "   see             ‚Üí 11 instances\n",
      "   busy            ‚Üí 10 instances\n",
      "   goodbye         ‚Üí 10 instances\n",
      "   start           ‚Üí 10 instances\n",
      "   say             ‚Üí 10 instances\n",
      "   store           ‚Üí 10 instances\n",
      "   try             ‚Üí 10 instances\n",
      "   we              ‚Üí 10 instances\n",
      "   she             ‚Üí  9 instances\n",
      "   they            ‚Üí  9 instances\n",
      "   me              ‚Üí  9 instances\n",
      "   free            ‚Üí  8 instances\n",
      "   come            ‚Üí  8 instances\n",
      "   our             ‚Üí  7 instances\n",
      "   i               ‚Üí  7 instances\n",
      "\n",
      "‚ùå Target glosses not found in WLASL (cannot add):\n",
      "   he, it, thank, wake\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "dd66b877",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 3: Define Target Conversational Glosses & Check Availability ----------\n",
    "\n",
    "# Our current 100-label vocabulary (from the 87.6% checkpoint)\n",
    "# These are the glosses the model already knows well.\n",
    "CURRENT_100_LABELS = {\n",
    "    \"accident\", \"africa\", \"all\", \"apple\", \"basketball\", \"bed\", \"before\", \"bird\",\n",
    "    \"birthday\", \"black\", \"blue\", \"book\", \"bowling\", \"brown\", \"but\", \"can\",\n",
    "    \"candy\", \"chair\", \"change\", \"cheat\", \"city\", \"clothes\", \"color\", \"computer\",\n",
    "    \"cook\", \"cool\", \"corn\", \"cousin\", \"cow\", \"dance\", \"dark\", \"deaf\", \"decide\",\n",
    "    \"doctor\", \"dog\", \"drink\", \"eat\", \"enjoy\", \"family\", \"fine\", \"finish\", \"fish\",\n",
    "    \"forget\", \"full\", \"give\", \"go\", \"graduate\", \"hat\", \"hearing\", \"help\", \"hot\",\n",
    "    \"how\", \"jacket\", \"kiss\", \"language\", \"last\", \"later\", \"letter\", \"like\", \"man\",\n",
    "    \"many\", \"medicine\", \"meet\", \"mother\", \"need\", \"no\", \"now\", \"orange\", \"paint\",\n",
    "    \"paper\", \"pink\", \"pizza\", \"play\", \"pull\", \"purple\", \"right\", \"same\", \"school\",\n",
    "    \"secretary\", \"shirt\", \"short\", \"son\", \"study\", \"table\", \"tall\", \"tell\",\n",
    "    \"thanksgiving\", \"thin\", \"thursday\", \"time\", \"walk\", \"want\", \"what\", \"white\",\n",
    "    \"who\", \"woman\", \"work\", \"wrong\", \"year\", \"yes\"\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Current 100-label vocabulary loaded: {len(CURRENT_100_LABELS)} glosses\")\n",
    "\n",
    "# Target conversational glosses we WANT for the demo\n",
    "# These are words that make natural short sentences possible.\n",
    "TARGET_CONVERSATIONAL = {\n",
    "    # Pronouns (critical for sentences!)\n",
    "    \"i\", \"you\", \"we\", \"they\", \"he\", \"she\", \"it\", \"my\", \"your\", \"our\", \"me\",\n",
    "    \n",
    "    # Core verbs (actions)\n",
    "    \"want\", \"need\", \"like\", \"go\", \"come\", \"help\", \"know\", \"have\", \"see\", \"hear\",\n",
    "    \"think\", \"feel\", \"love\", \"hate\", \"make\", \"take\", \"give\", \"get\", \"put\", \"use\",\n",
    "    \"work\", \"play\", \"eat\", \"drink\", \"sleep\", \"wake\", \"run\", \"walk\", \"sit\", \"stand\",\n",
    "    \"read\", \"write\", \"learn\", \"teach\", \"study\", \"understand\", \"remember\", \"forget\",\n",
    "    \"try\", \"start\", \"stop\", \"finish\", \"wait\", \"ask\", \"tell\", \"say\", \"talk\", \"call\",\n",
    "    \"meet\", \"visit\", \"live\", \"stay\", \"leave\", \"arrive\", \"move\", \"buy\", \"sell\", \"pay\",\n",
    "    \n",
    "    # Question words\n",
    "    \"what\", \"who\", \"where\", \"when\", \"why\", \"how\", \"which\",\n",
    "    \n",
    "    # Common nouns (people, places, things)\n",
    "    \"family\", \"friend\", \"mother\", \"father\", \"brother\", \"sister\", \"son\", \"daughter\",\n",
    "    \"baby\", \"child\", \"man\", \"woman\", \"person\", \"people\", \"name\",\n",
    "    \"home\", \"house\", \"school\", \"work\", \"office\", \"store\", \"hospital\", \"city\",\n",
    "    \"food\", \"water\", \"money\", \"time\", \"day\", \"night\", \"morning\", \"afternoon\", \"week\", \"month\", \"year\",\n",
    "    \"today\", \"tomorrow\", \"yesterday\", \"now\", \"later\", \"before\", \"after\",\n",
    "    \n",
    "    # Adjectives / states\n",
    "    \"good\", \"bad\", \"happy\", \"sad\", \"hungry\", \"tired\", \"sick\", \"fine\", \"busy\", \"free\",\n",
    "    \"new\", \"old\", \"big\", \"small\", \"hot\", \"cold\", \"easy\", \"hard\",\n",
    "    \n",
    "    # Basic responses / connectors\n",
    "    \"yes\", \"no\", \"please\", \"thank\", \"sorry\", \"hello\", \"goodbye\", \"ok\", \"more\", \"again\",\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Target conversational glosses: {len(TARGET_CONVERSATIONAL)} words\")\n",
    "\n",
    "# Check which target glosses exist in WLASL\n",
    "available_in_wlasl = {}\n",
    "missing_from_wlasl = []\n",
    "\n",
    "for gloss in TARGET_CONVERSATIONAL:\n",
    "    if gloss in gloss_instance_counts:\n",
    "        available_in_wlasl[gloss] = gloss_instance_counts[gloss]\n",
    "    else:\n",
    "        missing_from_wlasl.append(gloss)\n",
    "\n",
    "print(f\"\\nüìä Target glosses available in WLASL: {len(available_in_wlasl)}/{len(TARGET_CONVERSATIONAL)}\")\n",
    "print(f\"‚ùå Target glosses NOT in WLASL: {len(missing_from_wlasl)}\")\n",
    "\n",
    "# Categorize available glosses\n",
    "already_in_100 = {g: c for g, c in available_in_wlasl.items() if g in CURRENT_100_LABELS}\n",
    "new_glosses = {g: c for g, c in available_in_wlasl.items() if g not in CURRENT_100_LABELS}\n",
    "\n",
    "print(f\"\\n‚úÖ Already in our 100-label model: {len(already_in_100)} glosses\")\n",
    "print(f\"üÜï NEW glosses to add (exist in WLASL, not in our 100): {len(new_glosses)} glosses\")\n",
    "\n",
    "# Show the new glosses with their instance counts\n",
    "print(\"\\nüÜï NEW conversational glosses available in WLASL:\")\n",
    "for gloss, count in sorted(new_glosses.items(), key=lambda x: -x[1]):\n",
    "    print(f\"   {gloss:15s} ‚Üí {count:2d} instances\")\n",
    "\n",
    "# Show what's missing entirely from WLASL\n",
    "if missing_from_wlasl:\n",
    "    print(f\"\\n‚ùå Target glosses not found in WLASL (cannot add):\")\n",
    "    print(f\"   {', '.join(sorted(missing_from_wlasl))}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "id": "065f9eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:56.251591Z",
     "iopub.status.busy": "2025-12-09T22:50:56.251406Z",
     "iopub.status.idle": "2025-12-09T22:50:56.260918Z",
     "shell.execute_reply": "2025-12-09T22:50:56.259222Z"
    },
    "papermill": {
     "duration": 0.016021,
     "end_time": "2025-12-09T22:50:56.261952",
     "exception": false,
     "start_time": "2025-12-09T22:50:56.245931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï New glosses with >= 7 instances: 105\n",
      "‚úÖ Useful glosses from existing 100-label model: 66\n",
      "\n",
      "üéØ FINAL DEMO VOCABULARY: 171 glosses\n",
      "   ‚Ä¢ From existing 100-label model: 66\n",
      "   ‚Ä¢ NEW glosses (not in 100): 105\n",
      "\n",
      "üÜï NEW glosses to add (105 total):\n",
      "     1. water           ‚Üí 18 instances\n",
      "     2. wait            ‚Üí 18 instances\n",
      "     3. visit           ‚Üí 18 instances\n",
      "     4. yesterday       ‚Üí 17 instances\n",
      "     5. run             ‚Üí 17 instances\n",
      "     6. learn           ‚Üí 17 instances\n",
      "     7. take            ‚Üí 17 instances\n",
      "     8. brother         ‚Üí 17 instances\n",
      "     9. why             ‚Üí 17 instances\n",
      "    10. read            ‚Üí 17 instances\n",
      "    11. sick            ‚Üí 17 instances\n",
      "    12. week            ‚Üí 17 instances\n",
      "    13. write           ‚Üí 17 instances\n",
      "    14. have            ‚Üí 17 instances\n",
      "    15. friend          ‚Üí 16 instances\n",
      "    16. old             ‚Üí 16 instances\n",
      "    17. daughter        ‚Üí 16 instances\n",
      "    18. cold            ‚Üí 16 instances\n",
      "    19. tired           ‚Üí 16 instances\n",
      "    20. stay            ‚Üí 16 instances\n",
      "    21. leave           ‚Üí 16 instances\n",
      "    22. know            ‚Üí 16 instances\n",
      "    23. you             ‚Üí 16 instances\n",
      "    24. buy             ‚Üí 16 instances\n",
      "    25. again           ‚Üí 16 instances\n",
      "    26. remember        ‚Üí 16 instances\n",
      "    27. call            ‚Üí 16 instances\n",
      "    28. home            ‚Üí 16 instances\n",
      "    29. bad             ‚Üí 16 instances\n",
      "    30. happy           ‚Üí 16 instances\n",
      "    31. name            ‚Üí 16 instances\n",
      "    32. good            ‚Üí 16 instances\n",
      "    33. person          ‚Üí 16 instances\n",
      "    34. make            ‚Üí 15 instances\n",
      "    35. please          ‚Üí 15 instances\n",
      "    36. your            ‚Üí 15 instances\n",
      "    37. office          ‚Üí 15 instances\n",
      "    38. small           ‚Üí 15 instances\n",
      "    39. house           ‚Üí 15 instances\n",
      "    40. where           ‚Üí 15 instances\n",
      "    41. more            ‚Üí 15 instances\n",
      "    42. day             ‚Üí 15 instances\n",
      "    43. new             ‚Üí 15 instances\n",
      "    44. hear            ‚Üí 15 instances\n",
      "    45. live            ‚Üí 15 instances\n",
      "    46. feel            ‚Üí 15 instances\n",
      "    47. teach           ‚Üí 15 instances\n",
      "    48. arrive          ‚Üí 15 instances\n",
      "    49. money           ‚Üí 14 instances\n",
      "    50. use             ‚Üí 14 instances\n",
      "    51. hard            ‚Üí 14 instances\n",
      "    52. sister          ‚Üí 14 instances\n",
      "    53. ok              ‚Üí 14 instances\n",
      "    54. easy            ‚Üí 14 instances\n",
      "    55. sorry           ‚Üí 14 instances\n",
      "    56. move            ‚Üí 14 instances\n",
      "    57. talk            ‚Üí 14 instances\n",
      "    58. pay             ‚Üí 14 instances\n",
      "    59. today           ‚Üí 14 instances\n",
      "    60. month           ‚Üí 14 instances\n",
      "    61. when            ‚Üí 14 instances\n",
      "    62. think           ‚Üí 14 instances\n",
      "    63. baby            ‚Üí 14 instances\n",
      "    64. father          ‚Üí 14 instances\n",
      "    65. which           ‚Üí 14 instances\n",
      "    66. understand      ‚Üí 14 instances\n",
      "    67. sleep           ‚Üí 14 instances\n",
      "    68. sad             ‚Üí 14 instances\n",
      "    69. get             ‚Üí 13 instances\n",
      "    70. child           ‚Üí 13 instances\n",
      "    71. people          ‚Üí 13 instances\n",
      "    72. tomorrow        ‚Üí 13 instances\n",
      "    73. hospital        ‚Üí 13 instances\n",
      "    74. afternoon       ‚Üí 13 instances\n",
      "    75. hello           ‚Üí 13 instances\n",
      "    76. morning         ‚Üí 13 instances\n",
      "    77. night           ‚Üí 13 instances\n",
      "    78. sit             ‚Üí 13 instances\n",
      "    79. love            ‚Üí 12 instances\n",
      "    80. my              ‚Üí 12 instances\n",
      "    81. after           ‚Üí 12 instances\n",
      "    82. sell            ‚Üí 12 instances\n",
      "    83. food            ‚Üí 12 instances\n",
      "    84. big             ‚Üí 12 instances\n",
      "    85. hungry          ‚Üí 12 instances\n",
      "    86. ask             ‚Üí 11 instances\n",
      "    87. hate            ‚Üí 11 instances\n",
      "    88. stop            ‚Üí 11 instances\n",
      "    89. put             ‚Üí 11 instances\n",
      "    90. stand           ‚Üí 11 instances\n",
      "    91. see             ‚Üí 11 instances\n",
      "    92. busy            ‚Üí 10 instances\n",
      "    93. goodbye         ‚Üí 10 instances\n",
      "    94. start           ‚Üí 10 instances\n",
      "    95. say             ‚Üí 10 instances\n",
      "    96. store           ‚Üí 10 instances\n",
      "    97. try             ‚Üí 10 instances\n",
      "    98. we              ‚Üí 10 instances\n",
      "    99. she             ‚Üí  9 instances\n",
      "   100. they            ‚Üí  9 instances\n",
      "   101. me              ‚Üí  9 instances\n",
      "   102. free            ‚Üí  8 instances\n",
      "   103. come            ‚Üí  8 instances\n",
      "   104. our             ‚Üí  7 instances\n",
      "   105. i               ‚Üí  7 instances\n",
      "\n",
      "‚úÖ Keeping from existing model (66 total):\n",
      "   before, bird, book, can, change, city, cook, cool, cow, dance, dark, deaf, decide, doctor, dog, drink, eat, enjoy, family, fine, finish, fish, forget, full, give, go, hearing, help, hot, how, last, later, letter, like, man, medicine, meet, mother, need, no, now, paint, paper, pizza, play, pull, right, same, school, secretary, short, son, study, tall, tell, thin, time, walk, want, what, who, woman, work, wrong, year, yes\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "b620b41c",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 4: Build Final Demo Vocabulary ----------\n",
    "\n",
    "# Minimum instances required to include a gloss (for trainability)\n",
    "MIN_INSTANCES = 7\n",
    "\n",
    "# Filter new glosses by minimum instance count\n",
    "new_glosses_filtered = {g: c for g, c in new_glosses.items() if c >= MIN_INSTANCES}\n",
    "print(f\"üÜï New glosses with >= {MIN_INSTANCES} instances: {len(new_glosses_filtered)}\")\n",
    "\n",
    "# From the existing 100-label set, select the ones that are useful for conversation\n",
    "# (We already checked these are in CURRENT_100_LABELS)\n",
    "USEFUL_FROM_100 = {\n",
    "    # Verbs already in our model\n",
    "    \"can\", \"change\", \"cook\", \"dance\", \"decide\", \"drink\", \"eat\", \"enjoy\",\n",
    "    \"finish\", \"forget\", \"give\", \"go\", \"help\", \"like\", \"meet\", \"need\",\n",
    "    \"paint\", \"play\", \"pull\", \"study\", \"tell\", \"walk\", \"want\", \"work\",\n",
    "    \n",
    "    # Nouns already in our model\n",
    "    \"family\", \"mother\", \"son\", \"man\", \"woman\", \"doctor\", \"secretary\",\n",
    "    \"school\", \"city\", \"book\", \"letter\", \"paper\", \"medicine\", \"pizza\",\n",
    "    \"dog\", \"bird\", \"fish\", \"cow\",\n",
    "    \n",
    "    # Question words / time / connectors\n",
    "    \"what\", \"who\", \"how\", \"now\", \"later\", \"before\", \"last\", \"time\", \"year\",\n",
    "    \n",
    "    # Adjectives / states\n",
    "    \"fine\", \"hot\", \"cool\", \"dark\", \"full\", \"short\", \"tall\", \"thin\",\n",
    "    \"right\", \"wrong\", \"same\", \"deaf\", \"hearing\",\n",
    "    \n",
    "    # Responses\n",
    "    \"yes\", \"no\",\n",
    "}\n",
    "\n",
    "# Verify all USEFUL_FROM_100 are actually in CURRENT_100_LABELS\n",
    "assert USEFUL_FROM_100.issubset(CURRENT_100_LABELS), \"Some glosses not in 100-label set!\"\n",
    "print(f\"‚úÖ Useful glosses from existing 100-label model: {len(USEFUL_FROM_100)}\")\n",
    "\n",
    "# Combine: existing useful + all new filtered\n",
    "demo_vocab = USEFUL_FROM_100.union(set(new_glosses_filtered.keys()))\n",
    "print(f\"\\nüéØ FINAL DEMO VOCABULARY: {len(demo_vocab)} glosses\")\n",
    "\n",
    "# Categorize for display\n",
    "demo_from_100 = demo_vocab.intersection(CURRENT_100_LABELS)\n",
    "demo_new = demo_vocab - CURRENT_100_LABELS\n",
    "\n",
    "print(f\"   ‚Ä¢ From existing 100-label model: {len(demo_from_100)}\")\n",
    "print(f\"   ‚Ä¢ NEW glosses (not in 100): {len(demo_new)}\")\n",
    "\n",
    "# Show the NEW glosses that will be added (sorted by instance count)\n",
    "print(f\"\\nüÜï NEW glosses to add ({len(demo_new)} total):\")\n",
    "new_sorted = sorted([(g, new_glosses[g]) for g in demo_new], key=lambda x: -x[1])\n",
    "for i, (gloss, count) in enumerate(new_sorted, 1):\n",
    "    print(f\"   {i:3d}. {gloss:15s} ‚Üí {count:2d} instances\")\n",
    "\n",
    "# Show the glosses we're keeping from the 100-label model\n",
    "print(f\"\\n‚úÖ Keeping from existing model ({len(demo_from_100)} total):\")\n",
    "print(f\"   {', '.join(sorted(demo_from_100))}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "f83ef359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:56.273533Z",
     "iopub.status.busy": "2025-12-09T22:50:56.273300Z",
     "iopub.status.idle": "2025-12-09T22:50:56.282894Z",
     "shell.execute_reply": "2025-12-09T22:50:56.281545Z"
    },
    "papermill": {
     "duration": 0.017354,
     "end_time": "2025-12-09T22:50:56.283952",
     "exception": false,
     "start_time": "2025-12-09T22:50:56.266598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ COMPACT DEMO VOCABULARY v3: 55 glosses\n",
      "   Added: he (30 samples from Citizen), they (67 samples)\n",
      "   Skipped: she (9), it (0) - insufficient samples\n",
      "\n",
      "   ‚úÖ Available in WLASL: 54\n",
      "   ‚ö†Ô∏è Not in WLASL (may be in Citizen): 1\n",
      "      he\n",
      "\n",
      "üìä WLASL Breakdown:\n",
      "   ‚Ä¢ Already in 100-label model: 25\n",
      "   ‚Ä¢ NEW glosses: 29\n",
      "\n",
      "üìã WLASL instance counts:\n",
      "   drink        ‚Üí  35 instances  [‚úì in 100]\n",
      "   before       ‚Üí  26 instances  [‚úì in 100]\n",
      "   go           ‚Üí  26 instances  [‚úì in 100]\n",
      "   who          ‚Üí  25 instances  [‚úì in 100]\n",
      "   no           ‚Üí  22 instances  [‚úì in 100]\n",
      "   fine         ‚Üí  22 instances  [‚úì in 100]\n",
      "   help         ‚Üí  22 instances  [‚úì in 100]\n",
      "   yes          ‚Üí  22 instances  [‚úì in 100]\n",
      "   mother       ‚Üí  21 instances  [‚úì in 100]\n",
      "   like         ‚Üí  21 instances  [‚úì in 100]\n",
      "   hot          ‚Üí  21 instances  [‚úì in 100]\n",
      "   what         ‚Üí  21 instances  [‚úì in 100]\n",
      "   now          ‚Üí  21 instances  [‚úì in 100]\n",
      "   woman        ‚Üí  21 instances  [‚úì in 100]\n",
      "   family       ‚Üí  20 instances  [‚úì in 100]\n",
      "   man          ‚Üí  20 instances  [‚úì in 100]\n",
      "   later        ‚Üí  20 instances  [‚úì in 100]\n",
      "   want         ‚Üí  19 instances  [‚úì in 100]\n",
      "   eat          ‚Üí  19 instances  [‚úì in 100]\n",
      "   time         ‚Üí  19 instances  [‚úì in 100]\n",
      "   work         ‚Üí  19 instances  [‚úì in 100]\n",
      "   school       ‚Üí  19 instances  [‚úì in 100]\n",
      "   pizza        ‚Üí  19 instances  [‚úì in 100]\n",
      "   how          ‚Üí  18 instances  [‚úì in 100]\n",
      "   water        ‚Üí  18 instances  [üÜï NEW]\n",
      "   need         ‚Üí  18 instances  [‚úì in 100]\n",
      "   sick         ‚Üí  17 instances  [üÜï NEW]\n",
      "   have         ‚Üí  17 instances  [üÜï NEW]\n",
      "   why          ‚Üí  17 instances  [üÜï NEW]\n",
      "   stay         ‚Üí  16 instances  [üÜï NEW]\n",
      "   home         ‚Üí  16 instances  [üÜï NEW]\n",
      "   you          ‚Üí  16 instances  [üÜï NEW]\n",
      "   friend       ‚Üí  16 instances  [üÜï NEW]\n",
      "   happy        ‚Üí  16 instances  [üÜï NEW]\n",
      "   cold         ‚Üí  16 instances  [üÜï NEW]\n",
      "   tired        ‚Üí  16 instances  [üÜï NEW]\n",
      "   know         ‚Üí  16 instances  [üÜï NEW]\n",
      "   good         ‚Üí  16 instances  [üÜï NEW]\n",
      "   where        ‚Üí  15 instances  [üÜï NEW]\n",
      "   day          ‚Üí  15 instances  [üÜï NEW]\n",
      "   please       ‚Üí  15 instances  [üÜï NEW]\n",
      "   your         ‚Üí  15 instances  [üÜï NEW]\n",
      "   sorry        ‚Üí  14 instances  [üÜï NEW]\n",
      "   father       ‚Üí  14 instances  [üÜï NEW]\n",
      "   when         ‚Üí  14 instances  [üÜï NEW]\n",
      "   ok           ‚Üí  14 instances  [üÜï NEW]\n",
      "   hello        ‚Üí  13 instances  [üÜï NEW]\n",
      "   tomorrow     ‚Üí  13 instances  [üÜï NEW]\n",
      "   my           ‚Üí  12 instances  [üÜï NEW]\n",
      "   hungry       ‚Üí  12 instances  [üÜï NEW]\n",
      "   see          ‚Üí  11 instances  [üÜï NEW]\n",
      "   we           ‚Üí  10 instances  [üÜï NEW]\n",
      "   they         ‚Üí   9 instances  [üÜï NEW]\n",
      "   i            ‚Üí   7 instances  [üÜï NEW]\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "94a79e7b",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 5: Compact Dialogue-Focused Demo Vocabulary (v3 - 55 classes) ----------\n",
    "\n",
    "# Let's define actual dialogue scenarios we want to demo, then pick ONLY the glosses needed.\n",
    "\n",
    "# === DEMO DIALOGUE SCENARIOS ===\n",
    "# \n",
    "# Dialogue 1: Greeting & How are you\n",
    "#   A: \"HELLO, HOW YOU?\"\n",
    "#   B: \"I FINE, YOU?\"\n",
    "#   A: \"I GOOD\"\n",
    "#\n",
    "# Dialogue 2: Asking about plans / needs\n",
    "#   A: \"YOU WORK TODAY?\"\n",
    "#   B: \"NO, I STAY HOME\"\n",
    "#   A: \"YOU NEED HELP?\"\n",
    "#   B: \"YES PLEASE\"\n",
    "#\n",
    "# Dialogue 3: Talking about family / feelings\n",
    "#   A: \"HOW FAMILY?\"\n",
    "#   B: \"MOTHER FINE, FATHER SICK\"\n",
    "#   A: \"SORRY, I HOPE BETTER\"\n",
    "#   B: \"YES, SOON\"\n",
    "#\n",
    "# Dialogue 4: Food / daily activities\n",
    "#   A: \"YOU HUNGRY?\"\n",
    "#   B: \"YES, I WANT EAT\"\n",
    "#   A: \"WHAT YOU LIKE?\"\n",
    "#   B: \"I LIKE PIZZA\"\n",
    "#\n",
    "# Dialogue 5: Talking about others\n",
    "#   A: \"WHERE HE GO?\"\n",
    "#   B: \"HE WORK\"\n",
    "#   A: \"THEY COME LATER?\"\n",
    "#   B: \"YES, THEY COME\"\n",
    "\n",
    "# === COMPACT DEMO VOCABULARY v3 (55 classes) ===\n",
    "# v1: 58 classes\n",
    "# v2: 53 classes (removed 5 weak: food, goodbye, today, feel, me)\n",
    "# v3: 55 classes (added he, they for third-person pronouns)\n",
    "\n",
    "DEMO_VOCAB_COMPACT = {\n",
    "    # === PRONOUNS (essential for any sentence) ===\n",
    "    \"i\", \"you\", \"my\", \"your\", \"we\", \"he\", \"they\",  # Added: he (30), they (67)\n",
    "    \n",
    "    # === CORE VERBS ===\n",
    "    \"want\", \"need\", \"like\", \"go\", \"help\", \"know\", \"have\",\n",
    "    \"work\", \"eat\", \"drink\", \"stay\", \"see\",\n",
    "    \n",
    "    # === QUESTION WORDS ===\n",
    "    \"what\", \"who\", \"how\", \"where\", \"when\", \"why\",\n",
    "    \n",
    "    # === PEOPLE / FAMILY ===\n",
    "    \"family\", \"mother\", \"father\", \"friend\", \"man\", \"woman\",\n",
    "    \n",
    "    # === PLACES / THINGS ===\n",
    "    \"home\", \"school\", \"water\", \"pizza\", \"time\", \"day\",\n",
    "    \n",
    "    # === ADJECTIVES / STATES ===\n",
    "    \"good\", \"fine\", \"happy\", \"tired\", \"hungry\", \"sick\", \"hot\", \"cold\",\n",
    "    \n",
    "    # === TIME WORDS ===\n",
    "    \"tomorrow\", \"now\", \"later\", \"before\",\n",
    "    \n",
    "    # === RESPONSES / CONNECTORS ===\n",
    "    \"yes\", \"no\", \"please\", \"sorry\", \"hello\", \"ok\",\n",
    "}\n",
    "\n",
    "print(f\"üéØ COMPACT DEMO VOCABULARY v3: {len(DEMO_VOCAB_COMPACT)} glosses\")\n",
<<<<<<< HEAD
    "print(f\"   Added: he (30 samples from Citizen), they (67 samples)\")\n",
    "print(f\"   Skipped: she (9), it (0) - insufficient samples\")\n",
    "\n",
    "# Check availability in WLASL only (Citizen will be checked in Cell 6)\n",
    "available_wlasl = {g for g in DEMO_VOCAB_COMPACT if g in gloss_instance_counts}\n",
    "not_in_wlasl = DEMO_VOCAB_COMPACT - available_wlasl\n",
    "\n",
    "print(f\"\\n   ‚úÖ Available in WLASL: {len(available_wlasl)}\")\n",
    "print(f\"   ‚ö†Ô∏è Not in WLASL (may be in Citizen): {len(not_in_wlasl)}\")\n",
    "if not_in_wlasl:\n",
    "    print(f\"      {', '.join(sorted(not_in_wlasl))}\")\n",
    "\n",
    "# Check which are already in our 100-label model vs new\n",
    "from_100 = available_wlasl.intersection(CURRENT_100_LABELS)\n",
    "new_to_add = available_wlasl - CURRENT_100_LABELS\n",
    "\n",
    "print(f\"\\nüìä WLASL Breakdown:\")\n",
    "print(f\"   ‚Ä¢ Already in 100-label model: {len(from_100)}\")\n",
    "print(f\"   ‚Ä¢ NEW glosses: {len(new_to_add)}\")\n",
    "\n",
    "# Show WLASL instance counts\n",
    "print(f\"\\nüìã WLASL instance counts:\")\n",
    "for gloss, count in sorted([(g, gloss_instance_counts.get(g, 0)) for g in DEMO_VOCAB_COMPACT], key=lambda x: -x[1]):\n",
    "    if count > 0:\n",
    "        source = \"‚úì in 100\" if gloss in CURRENT_100_LABELS else \"üÜï NEW\"\n",
    "        print(f\"   {gloss:12s} ‚Üí {count:3d} instances  [{source}]\")"
=======
    "print(f\"   Added: he (30 samples), they (67 samples)\")\n",
    "print(f\"   Skipped: she (9), it (0) - insufficient samples\")\n",
    "\n",
    "# Check availability in WLASL\n",
    "available = {g for g in DEMO_VOCAB_COMPACT if g in gloss_instance_counts or g in citizen_gloss_counts}\n",
    "missing = DEMO_VOCAB_COMPACT - available\n",
    "\n",
    "print(f\"\\n   ‚úÖ Available in WLASL/Citizen: {len(available)}\")\n",
    "print(f\"   ‚ùå Missing from both: {len(missing)}\")\n",
    "if missing:\n",
    "    print(f\"      Missing: {', '.join(sorted(missing))}\")\n",
    "\n",
    "# Check which are already in our 100-label model vs new\n",
    "from_100 = available.intersection(CURRENT_100_LABELS)\n",
    "new_to_add = available - CURRENT_100_LABELS\n",
    "\n",
    "print(f\"\\nüìä Breakdown:\")\n",
    "print(f\"   ‚Ä¢ Already in 100-label model (reuse weights): {len(from_100)}\")\n",
    "print(f\"   ‚Ä¢ NEW glosses to learn: {len(new_to_add)}\")\n",
    "\n",
    "# Show instance counts for all demo glosses (WLASL + Citizen combined)\n",
    "print(f\"\\nüìã Demo vocabulary with instance counts:\")\n",
    "demo_with_counts = []\n",
    "for g in sorted(available):\n",
    "    wlasl = gloss_instance_counts.get(g, 0)\n",
    "    citizen = citizen_gloss_counts.get(g, 0)\n",
    "    total = wlasl + citizen\n",
    "    demo_with_counts.append((g, total))\n",
    "\n",
    "for gloss, count in sorted(demo_with_counts, key=lambda x: -x[1]):\n",
    "    source = \"‚úì in 100\" if gloss in CURRENT_100_LABELS else \"üÜï NEW\"\n",
    "    print(f\"   {gloss:12s} ‚Üí {count:3d} instances  [{source}]\")"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "9dcbf5b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:56.295080Z",
     "iopub.status.busy": "2025-12-09T22:50:56.294874Z",
     "iopub.status.idle": "2025-12-09T22:50:56.524448Z",
     "shell.execute_reply": "2025-12-09T22:50:56.523617Z"
    },
    "papermill": {
     "duration": 0.236973,
     "end_time": "2025-12-09T22:50:56.525813",
     "exception": false,
     "start_time": "2025-12-09T22:50:56.288840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded train.csv: 40154 rows\n",
      "‚úÖ Loaded val.csv: 10304 rows\n",
      "‚úÖ Loaded test.csv: 32941 rows\n",
      "\n",
      "‚úÖ Total unique glosses in Citizen: 2300\n",
      "‚úÖ Total Citizen videos: 83399\n",
      "\n",
      "üìä Demo glosses in ASL Citizen:\n",
      "   ‚úÖ Found in Citizen: 54/55\n",
      "   ‚ùå Not in Citizen: 1\n",
      "      Missing: i\n",
      "\n",
      "üìä Final availability check:\n",
      "   ‚úÖ Available in WLASL or Citizen: 55\n",
      "   ‚ùå Missing from both: 0\n",
      "\n",
      "üìã Demo vocabulary: WLASL + Citizen combined\n",
      "Gloss         WLASL  Citizen   TOTAL  Source\n",
      "--------------------------------------------------\n",
      "drink            35       61      96  [‚úì in 100]\n",
      "fine             22       62      84  [‚úì in 100]\n",
      "what             21       63      84  [‚úì in 100]\n",
      "woman            21       62      83  [‚úì in 100]\n",
      "eat              19       63      82  [‚úì in 100]\n",
      "pizza            19       62      81  [‚úì in 100]\n",
      "want             19       62      81  [‚úì in 100]\n",
      "how              18       62      80  [‚úì in 100]\n",
      "they              9       58      67  [üÜï NEW]\n",
      "go               26       31      57  [‚úì in 100]\n",
      "who              25       32      57  [‚úì in 100]\n",
      "before           26       30      56  [‚úì in 100]\n",
      "help             22       31      53  [‚úì in 100]\n",
      "mother           21       32      53  [‚úì in 100]\n",
      "hot              21       31      52  [‚úì in 100]\n",
      "later            20       32      52  [‚úì in 100]\n",
      "no               22       30      52  [‚úì in 100]\n",
      "yes              22       30      52  [‚úì in 100]\n",
      "like             21       30      51  [‚úì in 100]\n",
      "man              20       31      51  [‚úì in 100]\n",
      "now              21       30      51  [‚úì in 100]\n",
      "work             19       32      51  [‚úì in 100]\n",
      "family           20       30      50  [‚úì in 100]\n",
      "time             19       31      50  [‚úì in 100]\n",
      "school           19       30      49  [‚úì in 100]\n",
      "why              17       32      49  [üÜï NEW]\n",
      "need             18       30      48  [‚úì in 100]\n",
      "sick             17       31      48  [üÜï NEW]\n",
      "water            18       30      48  [üÜï NEW]\n",
      "cold             16       31      47  [üÜï NEW]\n",
      "good             16       31      47  [üÜï NEW]\n",
      "happy            16       31      47  [üÜï NEW]\n",
      "have             17       30      47  [üÜï NEW]\n",
      "home             16       31      47  [üÜï NEW]\n",
      "know             16       31      47  [üÜï NEW]\n",
      "you              16       31      47  [üÜï NEW]\n",
      "please           15       31      46  [üÜï NEW]\n",
      "stay             16       30      46  [üÜï NEW]\n",
      "tired            16       30      46  [üÜï NEW]\n",
      "where            15       31      46  [üÜï NEW]\n",
      "your             15       31      46  [üÜï NEW]\n",
      "friend           16       29      45  [üÜï NEW]\n",
      "hello            13       32      45  [üÜï NEW]\n",
      "ok               14       31      45  [üÜï NEW]\n",
      "when             14       31      45  [üÜï NEW]\n",
      "day              15       29      44  [üÜï NEW]\n",
      "sorry            14       30      44  [üÜï NEW]\n",
      "father           14       29      43  [üÜï NEW]\n",
      "hungry           12       31      43  [üÜï NEW]\n",
      "tomorrow         13       30      43  [üÜï NEW]\n",
      "my               12       30      42  [üÜï NEW]\n",
      "see              11       31      42  [üÜï NEW]\n",
      "we               10       31      41  [üÜï NEW]\n",
      "he                0       30      30  [üÜï NEW]\n",
      "i                 7        0       7  [üÜï NEW]\n",
      "\n",
      "üìà TOTALS: WLASL=952, Citizen=1934, Combined=2886\n"
     ]
    }
   ],
   "source": [
    "# ---------- Cell 6: Cross-Reference with ASL Citizen AND MS-ASL ----------\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. Load Citizen Counts ---\n",
    "citizen_gloss_counts = defaultdict(int)\n",
    "for split_file in [\"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    split_path = os.path.join(CITIZEN_SPLITS_DIR, split_file)\n",
    "    if not os.path.exists(split_path):\n",
    "        continue\n",
    "    df = pd.read_csv(split_path)\n",
    "    gloss_col = \"Gloss\" if \"Gloss\" in df.columns else df.columns[0]\n",
    "    for gloss_raw in df[gloss_col]:\n",
    "        gloss = ''.join([c for c in str(gloss_raw) if not c.isdigit()]).strip().lower()\n",
    "        citizen_gloss_counts[gloss] += 1\n",
    "\n",
    "print(f\"‚úÖ Loaded Citizen: {sum(citizen_gloss_counts.values())} videos\")\n",
    "\n",
    "# --- 2. Load MS-ASL Counts ---\n",
    "msasl_gloss_counts = defaultdict(int)\n",
    "if os.path.exists(MSASL_JSON):\n",
    "    with open(MSASL_JSON, 'r') as f:\n",
    "        msasl_data = json.load(f)\n",
    "    for entry in msasl_data:\n",
    "        gloss = entry['clean_text'].lower().strip()\n",
    "        msasl_gloss_counts[gloss] += 1\n",
    "    print(f\"‚úÖ Loaded MS-ASL: {len(msasl_data)} videos\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è MS-ASL JSON not found!\")\n",
    "\n",
    "# --- 3. Combined Analysis ---\n",
    "print(f\"\\nüìã Demo vocabulary: WLASL + Citizen + MS-ASL combined\")\n",
    "print(f\"{'Gloss':<12} {'WLASL':>6} {'Citizen':>8} {'MS-ASL':>8} {'TOTAL':>7}  Source\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "combined_data = []\n",
    "available = set()\n",
    "\n",
    "for gloss in sorted(DEMO_VOCAB_COMPACT):\n",
    "    wlasl_count = gloss_instance_counts.get(gloss, 0)\n",
    "    citizen_count = citizen_gloss_counts.get(gloss, 0)\n",
    "    msasl_count = msasl_gloss_counts.get(gloss, 0)\n",
    "    \n",
    "    total = wlasl_count + citizen_count + msasl_count\n",
    "    \n",
    "    if total > 0:\n",
    "        available.add(gloss)\n",
    "        source = \"‚úì in 100\" if gloss in CURRENT_100_LABELS else \"üÜï NEW\"\n",
    "        combined_data.append((gloss, wlasl_count, citizen_count, msasl_count, total, source))\n",
    "\n",
    "# Sort by total count descending\n",
    "for gloss, wlasl, citizen, msasl, total, source in sorted(combined_data, key=lambda x: -x[4]):\n",
    "    print(f\"{gloss:<12} {wlasl:>6} {citizen:>8} {msasl:>8} {total:>7}  [{source}]\")\n",
=======
   "id": "b6577a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 6: Cross-Reference with ASL Citizen for More Samples ----------\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load Citizen splits and count instances per gloss\n",
    "citizen_gloss_counts = defaultdict(int)\n",
    "citizen_split_counts = defaultdict(lambda: defaultdict(int))  # split -> gloss -> count\n",
    "\n",
    "for split_file in [\"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    split_path = os.path.join(CITIZEN_SPLITS_DIR, split_file)\n",
    "    if not os.path.exists(split_path):\n",
    "        print(f\"‚ö†Ô∏è {split_file} not found at {split_path}\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(split_path)\n",
    "    split_name = split_file.replace(\".csv\", \"\")\n",
    "    \n",
    "    # Find the gloss column (usually 'Gloss' or first column)\n",
    "    gloss_col = \"Gloss\" if \"Gloss\" in df.columns else df.columns[0]\n",
    "    \n",
    "    for gloss_raw in df[gloss_col]:\n",
    "        # Normalize: \"APPLE\" -> \"apple\", \"SOCCER2\" -> \"soccer\"\n",
    "        gloss = ''.join([c for c in str(gloss_raw) if not c.isdigit()]).strip().lower()\n",
    "        citizen_gloss_counts[gloss] += 1\n",
    "        citizen_split_counts[split_name][gloss] += 1\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {split_file}: {len(df)} rows\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total unique glosses in Citizen: {len(citizen_gloss_counts)}\")\n",
    "print(f\"‚úÖ Total Citizen videos: {sum(citizen_gloss_counts.values())}\")\n",
    "\n",
    "# Cross-reference with our compact demo vocab\n",
    "print(f\"\\nüìä Demo glosses in ASL Citizen:\")\n",
    "demo_in_citizen = {}\n",
    "demo_not_in_citizen = []\n",
    "\n",
    "for gloss in sorted(available):  # 'available' from Cell 5 = our 58 demo glosses\n",
    "    if gloss in citizen_gloss_counts:\n",
    "        demo_in_citizen[gloss] = citizen_gloss_counts[gloss]\n",
    "    else:\n",
    "        demo_not_in_citizen.append(gloss)\n",
    "\n",
    "print(f\"   ‚úÖ Found in Citizen: {len(demo_in_citizen)}/{len(available)}\")\n",
    "print(f\"   ‚ùå Not in Citizen: {len(demo_not_in_citizen)}\")\n",
    "\n",
    "if demo_not_in_citizen:\n",
    "    print(f\"      Missing: {', '.join(demo_not_in_citizen)}\")\n",
    "\n",
    "# Show combined totals (WLASL + Citizen)\n",
    "print(f\"\\nüìã Demo vocabulary: WLASL + Citizen combined\")\n",
    "print(f\"{'Gloss':<12} {'WLASL':>6} {'Citizen':>8} {'TOTAL':>7}  Source\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "combined_data = []\n",
    "for gloss in sorted(available):\n",
    "    wlasl_count = gloss_instance_counts.get(gloss, 0)\n",
    "    citizen_count = citizen_gloss_counts.get(gloss, 0)\n",
    "    total = wlasl_count + citizen_count\n",
    "    source = \"‚úì in 100\" if gloss in CURRENT_100_LABELS else \"üÜï NEW\"\n",
    "    combined_data.append((gloss, wlasl_count, citizen_count, total, source))\n",
    "\n",
    "# Sort by total count descending\n",
    "for gloss, wlasl, citizen, total, source in sorted(combined_data, key=lambda x: -x[3]):\n",
    "    print(f\"{gloss:<12} {wlasl:>6} {citizen:>8} {total:>7}  [{source}]\")\n",
>>>>>>> khaled
    "\n",
    "# Summary stats\n",
    "total_wlasl = sum(x[1] for x in combined_data)\n",
    "total_citizen = sum(x[2] for x in combined_data)\n",
<<<<<<< HEAD
    "total_msasl = sum(x[3] for x in combined_data)\n",
    "grand_total = total_wlasl + total_citizen + total_msasl\n",
    "\n",
    "print(f\"\\nüìà TOTALS:\")\n",
    "print(f\"   WLASL:   {total_wlasl}\")\n",
    "print(f\"   Citizen: {total_citizen}\")\n",
    "print(f\"   MS-ASL:  {total_msasl}\")\n",
    "print(f\"   COMBINED:{grand_total}\")\n",
    "\n",
    "missing = DEMO_VOCAB_COMPACT - available\n",
    "if missing:\n",
    "    print(f\"\\n‚ùå STILL MISSING: {', '.join(sorted(missing))}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ ALL 55 CLASSES HAVE DATA!\")"
=======
    "print(f\"\\nüìà TOTALS: WLASL={total_wlasl}, Citizen={total_citizen}, Combined={total_wlasl + total_citizen}\")"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "1cd3f36b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:56.537457Z",
     "iopub.status.busy": "2025-12-09T22:50:56.537091Z",
     "iopub.status.idle": "2025-12-09T22:50:56.545916Z",
     "shell.execute_reply": "2025-12-09T22:50:56.544908Z"
    },
    "papermill": {
     "duration": 0.015766,
     "end_time": "2025-12-09T22:50:56.547060",
     "exception": false,
     "start_time": "2025-12-09T22:50:56.531294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Low-count glosses (< 15 combined instances):\n",
      "   i: 7 instances (will need heavy augmentation)\n",
      "\n",
      "üéØ FINAL DEMO VOCABULARY: 55 glosses\n",
      "‚úÖ Saved label mapping to: /kaggle/working/SignBridge_demo/manifests/label_mapping_demo.json\n",
      "\n",
      "üìã Label Mapping (gloss ‚Üí label):\n",
      "    0 ‚Üí before        [‚úì in 100]\n",
      "    1 ‚Üí cold          [üÜï NEW]\n",
      "    2 ‚Üí day           [üÜï NEW]\n",
      "    3 ‚Üí drink         [‚úì in 100]\n",
      "    4 ‚Üí eat           [‚úì in 100]\n",
      "    5 ‚Üí family        [‚úì in 100]\n",
      "    6 ‚Üí father        [üÜï NEW]\n",
      "    7 ‚Üí fine          [‚úì in 100]\n",
      "    8 ‚Üí friend        [üÜï NEW]\n",
      "    9 ‚Üí go            [‚úì in 100]\n",
      "   10 ‚Üí good          [üÜï NEW]\n",
      "   11 ‚Üí happy         [üÜï NEW]\n",
      "   12 ‚Üí have          [üÜï NEW]\n",
      "   13 ‚Üí he            [üÜï NEW]\n",
      "   14 ‚Üí hello         [üÜï NEW]\n",
      "   15 ‚Üí help          [‚úì in 100]\n",
      "   16 ‚Üí home          [üÜï NEW]\n",
      "   17 ‚Üí hot           [‚úì in 100]\n",
      "   18 ‚Üí how           [‚úì in 100]\n",
      "   19 ‚Üí hungry        [üÜï NEW]\n",
      "   20 ‚Üí i             [üÜï NEW]\n",
      "   21 ‚Üí know          [üÜï NEW]\n",
      "   22 ‚Üí later         [‚úì in 100]\n",
      "   23 ‚Üí like          [‚úì in 100]\n",
      "   24 ‚Üí man           [‚úì in 100]\n",
      "   25 ‚Üí mother        [‚úì in 100]\n",
      "   26 ‚Üí my            [üÜï NEW]\n",
      "   27 ‚Üí need          [‚úì in 100]\n",
      "   28 ‚Üí no            [‚úì in 100]\n",
      "   29 ‚Üí now           [‚úì in 100]\n",
      "   30 ‚Üí ok            [üÜï NEW]\n",
      "   31 ‚Üí pizza         [‚úì in 100]\n",
      "   32 ‚Üí please        [üÜï NEW]\n",
      "   33 ‚Üí school        [‚úì in 100]\n",
      "   34 ‚Üí see           [üÜï NEW]\n",
      "   35 ‚Üí sick          [üÜï NEW]\n",
      "   36 ‚Üí sorry         [üÜï NEW]\n",
      "   37 ‚Üí stay          [üÜï NEW]\n",
      "   38 ‚Üí they          [üÜï NEW]\n",
      "   39 ‚Üí time          [‚úì in 100]\n",
      "   40 ‚Üí tired         [üÜï NEW]\n",
      "   41 ‚Üí tomorrow      [üÜï NEW]\n",
      "   42 ‚Üí want          [‚úì in 100]\n",
      "   43 ‚Üí water         [üÜï NEW]\n",
      "   44 ‚Üí we            [üÜï NEW]\n",
      "   45 ‚Üí what          [‚úì in 100]\n",
      "   46 ‚Üí when          [üÜï NEW]\n",
      "   47 ‚Üí where         [üÜï NEW]\n",
      "   48 ‚Üí who           [‚úì in 100]\n",
      "   49 ‚Üí why           [üÜï NEW]\n",
      "   50 ‚Üí woman         [‚úì in 100]\n",
      "   51 ‚Üí work          [‚úì in 100]\n",
      "   52 ‚Üí yes           [‚úì in 100]\n",
      "   53 ‚Üí you           [üÜï NEW]\n",
      "   54 ‚Üí your          [üÜï NEW]\n"
     ]
    }
   ],
=======
   "id": "8b29f598",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 7: Finalize Demo Vocab & Create Label Mapping ----------\n",
    "\n",
    "# Low-count glosses (< 15 combined instances) - may need extra augmentation\n",
<<<<<<< HEAD
    "# combined_data format: (gloss, wlasl_count, citizen_count, msasl_count, total, source)\n",
    "LOW_COUNT_THRESHOLD = 15\n",
    "low_count_glosses = [(g, w, c, m, total) for g, w, c, m, total, s in combined_data if total < LOW_COUNT_THRESHOLD]\n",
    "\n",
    "if low_count_glosses:\n",
    "    print(f\"‚ö†Ô∏è Low-count glosses (< {LOW_COUNT_THRESHOLD} combined instances):\")\n",
    "    for gloss, wlasl, citizen, msasl, total in low_count_glosses:\n",
    "        print(f\"   {gloss}: {total} instances (WLASL:{wlasl} + Citizen:{citizen} + MS-ASL:{msasl})\")\n",
    "else:\n",
    "    print(f\"‚úÖ All glosses have >= {LOW_COUNT_THRESHOLD} combined instances\")\n",
    "\n",
    "# Final demo vocabulary (keeping all 55 - we'll rely on augmentation for low-count ones)\n",
=======
    "LOW_COUNT_THRESHOLD = 15\n",
    "low_count_glosses = [(g, w, c, w+c) for g, w, c, t, s in combined_data if w + c < LOW_COUNT_THRESHOLD]\n",
    "\n",
    "if low_count_glosses:\n",
    "    print(f\"‚ö†Ô∏è Low-count glosses (< {LOW_COUNT_THRESHOLD} combined instances):\")\n",
    "    for gloss, wlasl, citizen, total in low_count_glosses:\n",
    "        print(f\"   {gloss}: {total} instances (will need heavy augmentation)\")\n",
    "else:\n",
    "    print(f\"‚úÖ All glosses have >= {LOW_COUNT_THRESHOLD} combined instances\")\n",
    "\n",
    "# Final demo vocabulary (keeping all 58 - we'll rely on augmentation for low-count ones)\n",
>>>>>>> khaled
    "FINAL_DEMO_VOCAB = sorted(available)  # alphabetically sorted for consistent label assignment\n",
    "\n",
    "print(f\"\\nüéØ FINAL DEMO VOCABULARY: {len(FINAL_DEMO_VOCAB)} glosses\")\n",
    "\n",
    "# Create label mapping\n",
    "gloss_to_label_demo = {gloss: idx for idx, gloss in enumerate(FINAL_DEMO_VOCAB)}\n",
    "label_to_gloss_demo = {idx: gloss for gloss, idx in gloss_to_label_demo.items()}\n",
    "\n",
    "label_mapping_demo = {\n",
    "    \"gloss_to_label\": gloss_to_label_demo,\n",
    "    \"label_to_gloss\": label_to_gloss_demo,\n",
    "    \"num_classes\": len(FINAL_DEMO_VOCAB),\n",
    "    \"description\": \"Compact conversational demo vocabulary for SignBridge real-time demo\",\n",
<<<<<<< HEAD
    "    \"source\": \"WLASL + ASL Citizen + MS-ASL, dialogue-focused selection\"\n",
=======
    "    \"source\": \"WLASL + ASL Citizen intersection, dialogue-focused selection\"\n",
>>>>>>> khaled
    "}\n",
    "\n",
    "# Save to manifests directory\n",
    "label_mapping_path = os.path.join(MANIFESTS_DIR, \"label_mapping_demo.json\")\n",
    "with open(label_mapping_path, \"w\") as f:\n",
    "    json.dump(label_mapping_demo, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved label mapping to: {label_mapping_path}\")\n",
    "\n",
    "# Show the mapping\n",
    "print(f\"\\nüìã Label Mapping (gloss ‚Üí label):\")\n",
    "for gloss, label in gloss_to_label_demo.items():\n",
    "    source = \"‚úì in 100\" if gloss in CURRENT_100_LABELS else \"üÜï NEW\"\n",
    "    print(f\"   {label:2d} ‚Üí {gloss:12s}  [{source}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "17663e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:56.558698Z",
     "iopub.status.busy": "2025-12-09T22:50:56.558495Z",
     "iopub.status.idle": "2025-12-09T22:50:59.834885Z",
     "shell.execute_reply": "2025-12-09T22:50:59.833946Z"
    },
    "papermill": {
     "duration": 3.283619,
     "end_time": "2025-12-09T22:50:59.836120",
     "exception": false,
     "start_time": "2025-12-09T22:50:56.552501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning WLASL videos directory: /kaggle/input/wlasl-processed/videos\n",
      "‚úÖ Found 11980 existing WLASL video files\n",
      "‚úÖ WLASL records for demo vocab: 486 (skipped 466 missing videos)\n",
      "‚úÖ Citizen records for demo vocab: 1934\n",
      "\n",
      "üìä Combined manifest:\n",
      "   Total records: 2420\n",
      "   By source: {'citizen': 1934, 'wlasl': 486}\n",
      "   By split: {'train': 1272, 'test': 821, 'val': 327}\n",
      "\n",
      "‚úÖ Saved manifest to: /kaggle/working/SignBridge_demo/manifests/demo_training_manifest.csv\n",
      "\n",
      "üìã Sample records:\n",
      "video_id                                     video_path gloss  label split source\n",
      "   69302 /kaggle/input/wlasl-processed/videos/69302.mp4 drink      3   val  wlasl\n",
      "   65539 /kaggle/input/wlasl-processed/videos/65539.mp4 drink      3 train  wlasl\n",
      "   17710 /kaggle/input/wlasl-processed/videos/17710.mp4 drink      3 train  wlasl\n",
      "   17733 /kaggle/input/wlasl-processed/videos/17733.mp4 drink      3 train  wlasl\n",
      "   65540 /kaggle/input/wlasl-processed/videos/65540.mp4 drink      3 train  wlasl\n",
      "   17734 /kaggle/input/wlasl-processed/videos/17734.mp4 drink      3   val  wlasl\n",
      "   17711 /kaggle/input/wlasl-processed/videos/17711.mp4 drink      3 train  wlasl\n",
      "   17712 /kaggle/input/wlasl-processed/videos/17712.mp4 drink      3 train  wlasl\n",
      "   17713 /kaggle/input/wlasl-processed/videos/17713.mp4 drink      3  test  wlasl\n",
      "   17709 /kaggle/input/wlasl-processed/videos/17709.mp4 drink      3 train  wlasl\n"
     ]
    }
   ],
   "source": [
    "# ---------- Cell 8: Build Training Manifest from WLASL + Citizen + MS-ASL ----------\n",
=======
   "id": "37208221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 8: Build Training Manifest from WLASL + Citizen (with existence check) ----------\n",
>>>>>>> khaled
    "\n",
    "# We'll create a combined manifest CSV with columns:\n",
    "#   video_id, video_path, gloss, label, split, source\n",
    "\n",
<<<<<<< HEAD
    "# ========== 1. WLASL Videos ==========\n",
    "WLASL_VIDEOS_DIR = os.path.join(WLASL_INPUT, \"videos\")\n",
    "existing_wlasl_ids = set()\n",
    "if os.path.exists(WLASL_VIDEOS_DIR):\n",
    "    for f in os.listdir(WLASL_VIDEOS_DIR):\n",
    "        if f.endswith(\".mp4\"):\n",
    "            existing_wlasl_ids.add(f.replace(\".mp4\", \"\"))\n",
    "\n",
    "wlasl_records = []\n",
    "for entry in wlasl_data:\n",
    "    gloss = entry[\"gloss\"].lower().strip()\n",
    "    if gloss not in gloss_to_label_demo: continue\n",
    "    label = gloss_to_label_demo[gloss]\n",
    "    for instance in entry.get(\"instances\", []):\n",
    "        video_id = instance.get(\"video_id\", \"\")\n",
    "        if video_id in existing_wlasl_ids:\n",
    "            wlasl_records.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"video_path\": os.path.join(WLASL_VIDEOS_DIR, f\"{video_id}.mp4\"),\n",
    "                \"gloss\": gloss, \"label\": label, \"split\": instance.get(\"split\", \"train\"), \"source\": \"wlasl\"\n",
    "            })\n",
    "\n",
    "print(f\"‚úÖ WLASL records: {len(wlasl_records)}\")\n",
    "\n",
    "# ========== 2. Citizen Videos ==========\n",
    "CITIZEN_VIDEOS_DIR = os.path.join(CITIZEN_INPUT, \"ASL_Citizen\", \"videos\")\n",
    "citizen_records = []\n",
    "for split_file in [\"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    split_path = os.path.join(CITIZEN_SPLITS_DIR, split_file)\n",
    "    if not os.path.exists(split_path): continue\n",
    "    df = pd.read_csv(split_path)\n",
    "    gloss_col = \"Gloss\" if \"Gloss\" in df.columns else df.columns[0]\n",
    "    video_col = next((c for c in [\"video_id\", \"Video\", \"filename\", \"file\", \"video\"] if c in df.columns), df.columns[1])\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        gloss = ''.join([c for c in str(row[gloss_col]) if not c.isdigit()]).strip().lower()\n",
    "        if gloss not in gloss_to_label_demo: continue\n",
    "        \n",
    "        video_id = str(row[video_col])\n",
    "        video_filename = video_id if video_id.endswith(\".mp4\") else f\"{video_id}.mp4\"\n",
=======
    "# ========== Build set of existing WLASL video IDs ==========\n",
    "WLASL_VIDEOS_DIR = os.path.join(WLASL_INPUT, \"videos\")\n",
    "\n",
    "print(f\"üîç Scanning WLASL videos directory: {WLASL_VIDEOS_DIR}\")\n",
    "existing_wlasl_ids = set()\n",
    "for f in os.listdir(WLASL_VIDEOS_DIR):\n",
    "    if f.endswith(\".mp4\"):\n",
    "        existing_wlasl_ids.add(f.replace(\".mp4\", \"\"))\n",
    "\n",
    "print(f\"‚úÖ Found {len(existing_wlasl_ids)} existing WLASL video files\")\n",
    "\n",
    "# ========== WLASL Videos (with existence check) ==========\n",
    "wlasl_records = []\n",
    "wlasl_skipped = 0\n",
    "\n",
    "for entry in wlasl_data:\n",
    "    gloss = entry[\"gloss\"].lower().strip()\n",
    "    if gloss not in gloss_to_label_demo:\n",
    "        continue  # skip glosses not in our demo vocab\n",
    "    \n",
    "    label = gloss_to_label_demo[gloss]\n",
    "    \n",
    "    for instance in entry.get(\"instances\", []):\n",
    "        video_id = instance.get(\"video_id\", \"\")\n",
    "        split = instance.get(\"split\", \"train\")\n",
    "        \n",
    "        # Check if video actually exists\n",
    "        if video_id not in existing_wlasl_ids:\n",
    "            wlasl_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        video_path = os.path.join(WLASL_VIDEOS_DIR, f\"{video_id}.mp4\")\n",
    "        \n",
    "        wlasl_records.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_path\": video_path,\n",
    "            \"gloss\": gloss,\n",
    "            \"label\": label,\n",
    "            \"split\": split,\n",
    "            \"source\": \"wlasl\"\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ WLASL records for demo vocab: {len(wlasl_records)} (skipped {wlasl_skipped} missing videos)\")\n",
    "\n",
    "# ========== Citizen Videos ==========\n",
    "CITIZEN_VIDEOS_DIR = os.path.join(CITIZEN_INPUT, \"ASL_Citizen\", \"videos\")\n",
    "\n",
    "citizen_records = []\n",
    "for split_file in [\"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    split_path = os.path.join(CITIZEN_SPLITS_DIR, split_file)\n",
    "    if not os.path.exists(split_path):\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(split_path)\n",
    "    split_name = split_file.replace(\".csv\", \"\")\n",
    "    \n",
    "    # Find columns\n",
    "    gloss_col = \"Gloss\" if \"Gloss\" in df.columns else df.columns[0]\n",
    "    video_col = None\n",
    "    for candidate in [\"video_id\", \"Video\", \"filename\", \"file\", \"video\"]:\n",
    "        if candidate in df.columns:\n",
    "            video_col = candidate\n",
    "            break\n",
    "    if video_col is None:\n",
    "        video_col = df.columns[1] if len(df.columns) > 1 else df.columns[0]\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        gloss_raw = str(row[gloss_col])\n",
    "        gloss = ''.join([c for c in gloss_raw if not c.isdigit()]).strip().lower()\n",
    "        \n",
    "        if gloss not in gloss_to_label_demo:\n",
    "            continue\n",
    "        \n",
    "        label = gloss_to_label_demo[gloss]\n",
    "        video_id = str(row[video_col]) if video_col else \"\"\n",
    "        \n",
    "        if not video_id.endswith(\".mp4\"):\n",
    "            video_filename = f\"{video_id}.mp4\"\n",
    "        else:\n",
    "            video_filename = video_id\n",
    "        \n",
>>>>>>> khaled
    "        video_path = os.path.join(CITIZEN_VIDEOS_DIR, video_filename)\n",
    "        \n",
    "        citizen_records.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"video_path\": video_path,\n",
<<<<<<< HEAD
    "            \"gloss\": gloss, \"label\": gloss_to_label_demo[gloss], \n",
    "            \"split\": split_file.replace(\".csv\", \"\"), \"source\": \"citizen\"\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Citizen records: {len(citizen_records)}\")\n",
    "\n",
    "# ========== 3. MS-ASL Videos ==========\n",
    "# Video filename format: {gloss}_{org_text}_{start_time}_{end_time}.mp4\n",
    "# Example: before_BEFORE-non-dominant-hand-palm-forward-version-prior-to_0.0_2.502.mp4\n",
    "\n",
    "msasl_records = []\n",
    "msasl_found = 0\n",
    "msasl_missing = 0\n",
    "\n",
    "if os.path.exists(MSASL_JSON):\n",
    "    with open(MSASL_JSON, 'r') as f:\n",
    "        msasl_manifest = json.load(f)\n",
    "    \n",
    "    for entry in msasl_manifest:\n",
    "        gloss = entry['clean_text'].lower().strip()\n",
    "        if gloss not in gloss_to_label_demo: continue\n",
    "        \n",
    "        # Get filename from manifest (already pre-computed during download)\n",
    "        filename = entry.get('filename')\n",
    "        if not filename:\n",
    "            # Fallback: construct filename from metadata\n",
    "            safe_gloss = gloss.replace(\" \", \"_\")\n",
    "            org_text = entry.get('org_text', '')\n",
    "            start_time = entry.get('start_time', 0)\n",
    "            end_time = entry.get('end_time', 0)\n",
    "            filename = f\"{safe_gloss}_{org_text}_{start_time}_{end_time}.mp4\"\n",
    "            # Clean filename of weird chars\n",
    "            filename = \"\".join([c for c in filename if c.isalnum() or c in ('_', '.', '-')])\n",
    "        \n",
    "        video_path = os.path.join(MSASL_VIDEOS_DIR, filename)\n",
    "        \n",
    "        # Check if file actually exists (some downloads might have failed)\n",
    "        if os.path.exists(video_path):\n",
    "            msasl_records.append({\n",
    "                \"video_id\": filename.replace('.mp4', ''),\n",
    "                \"video_path\": video_path,\n",
    "                \"gloss\": gloss,\n",
    "                \"label\": gloss_to_label_demo[gloss],\n",
    "                \"split\": entry.get('split', 'train'),  # Use original MS-ASL split\n",
    "                \"source\": \"msasl\"\n",
    "            })\n",
    "            msasl_found += 1\n",
    "        else:\n",
    "            msasl_missing += 1\n",
    "\n",
    "    print(f\"‚úÖ MS-ASL records: {len(msasl_records)} (found: {msasl_found}, missing: {msasl_missing})\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è MS-ASL JSON not found at: {MSASL_JSON}\")\n",
    "\n",
    "# ========== Combine ==========\n",
    "all_records = wlasl_records + citizen_records + msasl_records\n",
=======
    "            \"gloss\": gloss,\n",
    "            \"label\": label,\n",
    "            \"split\": split_name,\n",
    "            \"source\": \"citizen\"\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Citizen records for demo vocab: {len(citizen_records)}\")\n",
    "\n",
    "# ========== Combine ==========\n",
    "all_records = wlasl_records + citizen_records\n",
>>>>>>> khaled
    "df_manifest = pd.DataFrame(all_records)\n",
    "\n",
    "print(f\"\\nüìä Combined manifest:\")\n",
    "print(f\"   Total records: {len(df_manifest)}\")\n",
    "print(f\"   By source: {df_manifest['source'].value_counts().to_dict()}\")\n",
    "print(f\"   By split: {df_manifest['split'].value_counts().to_dict()}\")\n",
    "\n",
<<<<<<< HEAD
    "# Show per-class distribution by source\n",
    "print(f\"\\nüìã Per-class breakdown (WLASL / Citizen / MS-ASL):\")\n",
    "for gloss in sorted(gloss_to_label_demo.keys()):\n",
    "    w = len([r for r in wlasl_records if r['gloss'] == gloss])\n",
    "    c = len([r for r in citizen_records if r['gloss'] == gloss])\n",
    "    m = len([r for r in msasl_records if r['gloss'] == gloss])\n",
    "    total = w + c + m\n",
    "    print(f\"   {gloss:12s}: {w:3d} / {c:3d} / {m:3d} = {total:4d}\")\n",
    "\n",
    "# Save manifest\n",
    "manifest_path = os.path.join(MANIFESTS_DIR, \"demo_training_manifest.csv\")\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved manifest to: {manifest_path}\")"
=======
    "# Save manifest\n",
    "manifest_path = os.path.join(MANIFESTS_DIR, \"demo_training_manifest.csv\")\n",
    "df_manifest.to_csv(manifest_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved manifest to: {manifest_path}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nüìã Sample records:\")\n",
    "print(df_manifest.head(10).to_string(index=False))"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "2b28f424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:59.847859Z",
     "iopub.status.busy": "2025-12-09T22:50:59.847643Z",
     "iopub.status.idle": "2025-12-09T22:50:59.882670Z",
     "shell.execute_reply": "2025-12-09T22:50:59.881798Z"
    },
    "papermill": {
     "duration": 0.042127,
     "end_time": "2025-12-09T22:50:59.883857",
     "exception": false,
     "start_time": "2025-12-09T22:50:59.841730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loaded manifest:\n",
      "split\n",
      "train    1272\n",
      "test      821\n",
      "val       327\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä New split sizes:\n",
      "   TRAIN: 2093 (was train+test)\n",
      "   VAL:   327 (original val)\n",
      "\n",
      "‚úÖ Saved TRAIN manifest to: /kaggle/working/SignBridge_demo/manifests/demo_train_manifest.csv\n",
      "‚úÖ Saved VAL manifest to:   /kaggle/working/SignBridge_demo/manifests/demo_val_manifest.csv\n",
      "\n",
      "üìã TRAIN sample:\n",
      "video_id                                     video_path gloss  label split source\n",
      "   65539 /kaggle/input/wlasl-processed/videos/65539.mp4 drink      3 train  wlasl\n",
      "   17710 /kaggle/input/wlasl-processed/videos/17710.mp4 drink      3 train  wlasl\n",
      "   17733 /kaggle/input/wlasl-processed/videos/17733.mp4 drink      3 train  wlasl\n",
      "   65540 /kaggle/input/wlasl-processed/videos/65540.mp4 drink      3 train  wlasl\n",
      "   17711 /kaggle/input/wlasl-processed/videos/17711.mp4 drink      3 train  wlasl\n",
      "   17712 /kaggle/input/wlasl-processed/videos/17712.mp4 drink      3 train  wlasl\n",
      "   17713 /kaggle/input/wlasl-processed/videos/17713.mp4 drink      3 train  wlasl\n",
      "   17709 /kaggle/input/wlasl-processed/videos/17709.mp4 drink      3 train  wlasl\n",
      "   17720 /kaggle/input/wlasl-processed/videos/17720.mp4 drink      3 train  wlasl\n",
      "   17721 /kaggle/input/wlasl-processed/videos/17721.mp4 drink      3 train  wlasl\n",
      "\n",
      "üìã VAL sample:\n",
      "video_id                                     video_path  gloss  label split source\n",
      "   69302 /kaggle/input/wlasl-processed/videos/69302.mp4  drink      3   val  wlasl\n",
      "   17734 /kaggle/input/wlasl-processed/videos/17734.mp4  drink      3   val  wlasl\n",
      "   17724 /kaggle/input/wlasl-processed/videos/17724.mp4  drink      3   val  wlasl\n",
      "   05750 /kaggle/input/wlasl-processed/videos/05750.mp4 before      0   val  wlasl\n",
      "   69345 /kaggle/input/wlasl-processed/videos/69345.mp4     go      9   val  wlasl\n",
      "   24946 /kaggle/input/wlasl-processed/videos/24946.mp4     go      9   val  wlasl\n",
      "   24952 /kaggle/input/wlasl-processed/videos/24952.mp4     go      9   val  wlasl\n",
      "   63229 /kaggle/input/wlasl-processed/videos/63229.mp4    who     48   val  wlasl\n",
      "   63230 /kaggle/input/wlasl-processed/videos/63230.mp4    who     48   val  wlasl\n",
      "   63237 /kaggle/input/wlasl-processed/videos/63237.mp4    who     48   val  wlasl\n"
     ]
    }
   ],
   "source": [
    "# ---------- Cell 9: PROFESSIONAL SPLIT STRATEGY ----------\n",
    "\n",
    "# ===========================================================================\n",
    "# üéØ PROFESSIONAL SPLIT STRATEGY (MAXIMIZING DATA)\n",
    "# ===========================================================================\n",
    "# We preserve signer separation while maximizing training data:\n",
    "#\n",
    "# TRAIN: WLASL (train+test) + Citizen (train+test) + MS-ASL (train)\n",
    "# VAL:   WLASL (val) + Citizen (val) + MS-ASL (val)\n",
    "# TEST:  MS-ASL (test) - HELD OUT until final evaluation\n",
    "#\n",
    "# Why this works:\n",
    "# 1. Each dataset's val split is signer-disjoint from train\n",
    "# 2. MS-ASL test = completely unseen signers for honest final eval\n",
    "# 3. WLASL/Citizen test added to train = more training data!\n",
    "#    (No leakage since MS-ASL test is our true held-out set)\n",
    "# ===========================================================================\n",
=======
   "id": "6c445136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 9: Re-split manifest for fine-tuning (train + test as train) ----------\n",
    "\n",
    "# For this demo-focused fine-tuning, we care more about using as much data as possible\n",
    "# than about having a separate held-out *test* set.\n",
    "#\n",
    "# Strategy:\n",
    "#   - Use all rows with split in {\"train\", \"test\"} as TRAIN.\n",
    "#   - Keep existing \"val\" rows as VALIDATION for early stopping / sanity checks.\n",
>>>>>>> khaled
    "\n",
    "manifest_path = os.path.join(MANIFESTS_DIR, \"demo_training_manifest.csv\")\n",
    "df = pd.read_csv(manifest_path)\n",
    "\n",
    "print(\"üì• Loaded manifest:\")\n",
<<<<<<< HEAD
    "print(f\"   Total records: {len(df)}\")\n",
    "print(f\"   By source: {df['source'].value_counts().to_dict()}\")\n",
    "print(f\"   By split: {df['split'].value_counts().to_dict()}\")\n",
    "\n",
    "# ===========================================================================\n",
    "# Step 1: Split Strategy (maximize training data)\n",
    "# ===========================================================================\n",
    "\n",
    "# TRAIN: All train splits + WLASL/Citizen test (NOT MS-ASL test)\n",
    "train_original = df[df['split'] == 'train'].copy()\n",
    "wlasl_citizen_test = df[(df['split'] == 'test') & (df['source'].isin(['wlasl', 'citizen']))].copy()\n",
    "train_df = pd.concat([train_original, wlasl_citizen_test], ignore_index=True)\n",
    "train_df['split'] = 'train'\n",
    "\n",
    "# VAL: All val splits from all sources\n",
    "val_df = df[df['split'] == 'val'].copy()\n",
    "val_df['split'] = 'val'\n",
    "\n",
    "# TEST: MS-ASL test split ONLY (held-out for final evaluation)\n",
    "test_df = df[(df['source'] == 'msasl') & (df['split'] == 'test')].copy()\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "# Verify no data loss\n",
    "total_used = len(train_df) + len(val_df) + len(test_df)\n",
    "print(f\"\\n‚úÖ Data utilization: {total_used}/{len(df)} records ({100*total_used/len(df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä PROFESSIONAL SPLIT:\")\n",
    "print(f\"   TRAIN: {len(train_df)} samples\")\n",
    "print(f\"      ‚îî‚îÄ Original train: {len(train_original)}\")\n",
    "print(f\"      ‚îî‚îÄ Added from WLASL/Citizen test: {len(wlasl_citizen_test)}\")\n",
    "print(f\"      ‚îî‚îÄ Sources: {train_df['source'].value_counts().to_dict()}\")\n",
    "print(f\"   VAL:   {len(val_df)} samples\")\n",
    "print(f\"      ‚îî‚îÄ Sources: {val_df['source'].value_counts().to_dict()}\")\n",
    "print(f\"   TEST:  {len(test_df)} samples (MS-ASL test only - HELD OUT)\")\n",
    "print(f\"      ‚îî‚îÄ Sources: {test_df['source'].value_counts().to_dict()}\")\n",
    "\n",
    "# Check class coverage\n",
    "train_classes = set(train_df[\"gloss\"].unique())\n",
    "val_classes = set(val_df[\"gloss\"].unique())\n",
    "test_classes = set(test_df[\"gloss\"].unique()) if len(test_df) > 0 else set()\n",
    "\n",
    "print(f\"\\nüìã Class Coverage:\")\n",
    "print(f\"   Train: {len(train_classes)}/{len(gloss_to_label_demo)} classes\")\n",
    "print(f\"   Val:   {len(val_classes)}/{len(gloss_to_label_demo)} classes\")\n",
    "print(f\"   Test:  {len(test_classes)}/{len(gloss_to_label_demo)} classes\")\n",
    "\n",
    "# Check for missing classes in val\n",
    "missing_in_val = train_classes - val_classes\n",
    "if missing_in_val:\n",
    "    print(f\"\\n‚ö†Ô∏è Classes missing from VAL: {sorted(missing_in_val)}\")\n",
    "    print(\"   These will be trained but not validated during training.\")\n",
    "\n",
    "# ===========================================================================\n",
    "# Step 2: Save manifests\n",
    "# ===========================================================================\n",
    "train_manifest_path = os.path.join(MANIFESTS_DIR, \"demo_train_manifest.csv\")\n",
    "val_manifest_path = os.path.join(MANIFESTS_DIR, \"demo_val_manifest.csv\")\n",
    "test_manifest_path = os.path.join(MANIFESTS_DIR, \"demo_test_manifest.csv\")\n",
    "\n",
    "train_df.to_csv(train_manifest_path, index=False)\n",
    "val_df.to_csv(val_manifest_path, index=False)\n",
    "test_df.to_csv(test_manifest_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved TRAIN manifest: {train_manifest_path}\")\n",
    "print(f\"‚úÖ Saved VAL manifest:   {val_manifest_path}\")\n",
    "print(f\"‚úÖ Saved TEST manifest:  {test_manifest_path}\")\n",
    "\n",
    "# Per-class breakdown\n",
    "print(\"\\nüìã Per-class sample counts (Train/Val/Test):\")\n",
    "print(f\"   {'Gloss':<12} {'Train':>6} {'Val':>5} {'Test':>5} {'Total':>6}\")\n",
    "print(f\"   {'-'*40}\")\n",
    "for gloss in sorted(train_classes):\n",
    "    t = len(train_df[train_df[\"gloss\"] == gloss])\n",
    "    v = len(val_df[val_df[\"gloss\"] == gloss]) if gloss in val_classes else 0\n",
    "    ts = len(test_df[test_df[\"gloss\"] == gloss]) if gloss in test_classes else 0\n",
    "    total = t + v + ts\n",
    "    marker = \"‚ö†Ô∏è\" if v == 0 else \"\"\n",
    "    print(f\"   {gloss:<12} {t:>6} {v:>5} {ts:>5} {total:>6}  {marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SPLIT STRATEGY SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(\"   ‚úÖ TRAIN: Original train + WLASL/Citizen test (maximize data!)\")\n",
    "print(\"   ‚úÖ VAL:   Original val from all 3 datasets\")\n",
    "print(\"   ‚úÖ TEST:  MS-ASL test ONLY (held-out, never seen)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: DO NOT use TEST set until final evaluation!\")\n",
    "print(\"   The TEST accuracy should match real-world inference accuracy.\")"
=======
    "print(df[\"split\"].value_counts())\n",
    "\n",
    "train_mask = df[\"split\"].isin([\"train\", \"test\"])\n",
    "val_mask = df[\"split\"] == \"val\"\n",
    "\n",
    "train_df = df[train_mask].copy()\n",
    "val_df = df[val_mask].copy()\n",
    "\n",
    "# Normalize split names\n",
    "train_df[\"split\"] = \"train\"\n",
    "val_df[\"split\"] = \"val\"\n",
    "\n",
    "print(\"\\nüìä New split sizes:\")\n",
    "print(f\"   TRAIN: {len(train_df)} (was train+test)\")\n",
    "print(f\"   VAL:   {len(val_df)} (original val)\")\n",
    "\n",
    "# Save separate manifests\n",
    "train_manifest_path = os.path.join(MANIFESTS_DIR, \"demo_train_manifest.csv\")\n",
    "val_manifest_path = os.path.join(MANIFESTS_DIR, \"demo_val_manifest.csv\")\n",
    "\n",
    "train_df.to_csv(train_manifest_path, index=False)\n",
    "val_df.to_csv(val_manifest_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved TRAIN manifest to: {train_manifest_path}\")\n",
    "print(f\"‚úÖ Saved VAL manifest to:   {val_manifest_path}\")\n",
    "\n",
    "print(\"\\nüìã TRAIN sample:\")\n",
    "print(train_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nüìã VAL sample:\")\n",
    "print(val_df.head(10).to_string(index=False))"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "01629e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:50:59.895714Z",
     "iopub.status.busy": "2025-12-09T22:50:59.895207Z",
     "iopub.status.idle": "2025-12-09T22:51:04.569189Z",
     "shell.execute_reply": "2025-12-09T22:51:04.568253Z"
    },
    "papermill": {
     "duration": 4.681195,
     "end_time": "2025-12-09T22:51:04.570401",
     "exception": false,
     "start_time": "2025-12-09T22:50:59.889206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading 100-class checkpoint from: /kaggle/input/best-model-87/best_model_citizen100_87pct.pth\n",
      "   Keys: ['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'train_loss', 'train_acc', 'val_loss', 'val_acc', 'history', 'num_classes']\n",
      "   State dict has 344 keys\n",
      "‚úÖ Loaded 100-class label mapping: 100 glosses\n",
      "‚úÖ Demo 55-class label mapping: 55 glosses\n",
      "\n",
      "üîó Overlapping glosses (can transfer weights): 25\n",
      "   Will copy classifier weights for 25 classes\n"
     ]
    }
   ],
   "source": [
    "# ---------- Cell 10: Checkpoint & Training Configuration ----------\n",
=======
   "id": "b7fe6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 10: Load 87.6% Checkpoint & Prepare Weight Transfer ----------\n",
>>>>>>> khaled
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
<<<<<<< HEAD
    "# ============================================================================\n",
    "# üéØ CHECKPOINT SELECTION - 75.15% Model (Achieved 87.69% on 100 classes!)\n",
    "# ============================================================================\n",
    "# We use the 75.15% checkpoint that achieved 87.69% validation accuracy.\n",
    "# This model already knows WLASL signs and will be fine-tuned for our 55 demo classes.\n",
    "#\n",
    "# Why this checkpoint?\n",
    "# 1. Already trained on WLASL signs = better starting point\n",
    "# 2. Achieved 87.69% on 100 classes = proven feature quality\n",
    "# 3. 55 classes is a SUBSET, so we should get even better results!\n",
    "# ============================================================================\n",
    "\n",
    "# Path to the 75.15% checkpoint (that achieved 87.69%)\n",
    "CHECKPOINT_PATH = \"/kaggle/input/wlasl-finetuned-full-model/wlasl100_best_model_75.15pct_FULL.pth\"\n",
    "\n",
    "print(\"üì¶ Checkpoint Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Checkpoint: 75.15% model (achieved 87.69% on 100 classes)\")\n",
    "print(f\"   Path: {CHECKPOINT_PATH}\")\n",
    "print(f\"   Exists: {os.path.exists(CHECKPOINT_PATH)}\")\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"75.15% checkpoint not found!\\n\"\n",
    "        f\"Please add the Kaggle dataset: 'wlasl-finetuned-full-model'\\n\"\n",
    "        f\"Expected path: {CHECKPOINT_PATH}\"\n",
    "    )\n",
    "\n",
    "# Our demo vocabulary (from Cell 7)\n",
    "NUM_CLASSES_DEMO = len(gloss_to_label_demo)\n",
    "DEMO_LABELS = list(gloss_to_label_demo.keys())\n",
    "\n",
    "print(f\"\\nüéØ Training Target:\")\n",
    "print(f\"   Demo vocabulary: {NUM_CLASSES_DEMO} classes\")\n",
    "print(f\"   Baseline: 75.15% checkpoint ‚Üí 87.69% validation\")\n",
    "print(f\"   Goal: Beat 87.69% (fewer classes = should be easier!)\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to define model in Cell 11 and train in Cell 17\")"
=======
    "# Load the 87.6% checkpoint (100 classes) from Kaggle input\n",
    "CHECKPOINT_100_PATH = \"/kaggle/input/best-model-87/best_model_citizen100_87pct.pth\"\n",
    "\n",
    "# Load the original 100-class label mapping (for weight transfer)\n",
    "LABEL_MAPPING_100_PATH = \"/kaggle/input/label-map-100/label_mapping.json\"\n",
    "\n",
    "print(f\"üì¶ Loading 100-class checkpoint from: {CHECKPOINT_100_PATH}\")\n",
    "checkpoint_100 = torch.load(CHECKPOINT_100_PATH, map_location=\"cpu\")\n",
    "\n",
    "# Check what's in the checkpoint\n",
    "if isinstance(checkpoint_100, dict):\n",
    "    print(f\"   Keys: {list(checkpoint_100.keys())}\")\n",
    "    if \"model_state_dict\" in checkpoint_100:\n",
    "        state_dict_100 = checkpoint_100[\"model_state_dict\"]\n",
    "    else:\n",
    "        state_dict_100 = checkpoint_100\n",
    "else:\n",
    "    state_dict_100 = checkpoint_100\n",
    "\n",
    "print(f\"   State dict has {len(state_dict_100)} keys\")\n",
    "\n",
    "with open(LABEL_MAPPING_100_PATH, \"r\") as f:\n",
    "    label_mapping_100 = json.load(f)\n",
    "\n",
    "gloss_to_label_100 = label_mapping_100[\"gloss_to_label\"]\n",
    "print(f\"‚úÖ Loaded 100-class label mapping: {len(gloss_to_label_100)} glosses\")\n",
    "\n",
    "# Our demo N-class mapping (from Cell 7)\n",
    "NUM_CLASSES_DEMO = len(gloss_to_label_demo)\n",
    "print(f\"‚úÖ Demo {NUM_CLASSES_DEMO}-class label mapping: {NUM_CLASSES_DEMO} glosses\")\n",
    "\n",
    "# Find overlapping glosses (present in both 100-class and demo)\n",
    "overlapping_glosses = set(gloss_to_label_100.keys()).intersection(set(gloss_to_label_demo.keys()))\n",
    "print(f\"\\nüîó Overlapping glosses (can transfer weights): {len(overlapping_glosses)}\")\n",
    "\n",
    "# Build mapping: demo_label -> 100_label (for weight copying)\n",
    "demo_to_100_label = {}\n",
    "for gloss in overlapping_glosses:\n",
    "    demo_label = gloss_to_label_demo[gloss]\n",
    "    old_label = gloss_to_label_100[gloss]\n",
    "    demo_to_100_label[demo_label] = old_label\n",
    "\n",
    "print(f\"   Will copy classifier weights for {len(demo_to_100_label)} classes\")"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a6b6b5c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:51:04.583383Z",
     "iopub.status.busy": "2025-12-09T22:51:04.583022Z",
     "iopub.status.idle": "2025-12-09T22:51:04.931885Z",
     "shell.execute_reply": "2025-12-09T22:51:04.930957Z"
    },
    "papermill": {
     "duration": 0.357042,
     "end_time": "2025-12-09T22:51:04.933137",
     "exception": false,
     "start_time": "2025-12-09T22:51:04.576095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Building 55-class demo model...\n",
      "‚úÖ Loaded 100-class weights into temporary model\n",
      "‚úÖ Copied backbone weights (342 keys)\n",
      "‚úÖ Transferred classifier weights for 25 overlapping classes\n",
      "üÜï Remaining 30 classes initialized randomly\n",
      "\n",
      "üéØ Demo model ready: InceptionI3d with 55 classes\n",
      "   Total parameters: 12,343,639\n"
     ]
    }
   ],
   "source": [
    "# ---------- Cell 11: I3D Model Definition ----------\n",
=======
   "id": "39ca2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 11: Build Demo I3D Model with Weight Transfer ----------\n",
>>>>>>> khaled
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ============================================================================\n",
<<<<<<< HEAD
    "# I3D Model Definition (matching original WLASL paper implementation)\n",
    "# ============================================================================\n",
    "# This cell ONLY defines the model architecture classes.\n",
    "# Model instantiation and checkpoint loading happens in Cell 17.\n",
=======
    "# I3D Model Definition (matching CV/models/i3d.py from SignBridge repo)\n",
>>>>>>> khaled
    "# ============================================================================\n",
    "\n",
    "class MaxPool3dSamePadding(nn.MaxPool3d):\n",
    "    def compute_pad(self, dim, s):\n",
    "        if s % self.stride[dim] == 0:\n",
    "            return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
    "        else:\n",
    "            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channel, t, h, w = x.size()\n",
    "        pad_t = self.compute_pad(0, t)\n",
    "        pad_h = self.compute_pad(1, h)\n",
    "        pad_w = self.compute_pad(2, w)\n",
    "        pad_t_f, pad_t_b = pad_t // 2, pad_t - pad_t // 2\n",
    "        pad_h_f, pad_h_b = pad_h // 2, pad_h - pad_h // 2\n",
    "        pad_w_f, pad_w_b = pad_w // 2, pad_w - pad_w // 2\n",
    "        x = F.pad(x, (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b))\n",
    "        return super().forward(x)\n",
    "\n",
    "\n",
    "class Unit3D(nn.Module):\n",
    "    def __init__(self, in_channels, output_channels, kernel_shape=(1,1,1),\n",
    "                 stride=(1,1,1), padding=0, activation_fn=F.relu,\n",
    "                 use_batch_norm=True, use_bias=False, name=\"unit_3d\"):\n",
    "        super().__init__()\n",
    "        self._output_channels = output_channels\n",
    "        self._kernel_shape = kernel_shape\n",
    "        self._stride = stride\n",
    "        self._use_batch_norm = use_batch_norm\n",
    "        self._activation_fn = activation_fn\n",
    "        self._use_bias = use_bias\n",
    "        self.name = name\n",
    "        self.conv3d = nn.Conv3d(in_channels, output_channels, kernel_shape, stride, padding=0, bias=use_bias)\n",
    "        if use_batch_norm:\n",
    "            self.bn = nn.BatchNorm3d(output_channels, eps=0.001, momentum=0.01)\n",
    "\n",
    "    def compute_pad(self, dim, s):\n",
    "        if s % self._stride[dim] == 0:\n",
    "            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n",
    "        else:\n",
    "            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channel, t, h, w = x.size()\n",
    "        pad_t = self.compute_pad(0, t)\n",
    "        pad_h = self.compute_pad(1, h)\n",
    "        pad_w = self.compute_pad(2, w)\n",
    "        pad_t_f, pad_t_b = pad_t // 2, pad_t - pad_t // 2\n",
    "        pad_h_f, pad_h_b = pad_h // 2, pad_h - pad_h // 2\n",
    "        pad_w_f, pad_w_b = pad_w // 2, pad_w - pad_w // 2\n",
    "        x = F.pad(x, (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b))\n",
    "        x = self.conv3d(x)\n",
    "        if self._use_batch_norm:\n",
    "            x = self.bn(x)\n",
    "        if self._activation_fn is not None:\n",
    "            x = self._activation_fn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, name):\n",
    "        super().__init__()\n",
    "        self.b0 = Unit3D(in_channels, out_channels[0], kernel_shape=[1,1,1], name=name+\"/Branch_0/Conv3d_0a_1x1\")\n",
    "        self.b1a = Unit3D(in_channels, out_channels[1], kernel_shape=[1,1,1], name=name+\"/Branch_1/Conv3d_0a_1x1\")\n",
    "        self.b1b = Unit3D(out_channels[1], out_channels[2], kernel_shape=[3,3,3], name=name+\"/Branch_1/Conv3d_0b_3x3\")\n",
    "        self.b2a = Unit3D(in_channels, out_channels[3], kernel_shape=[1,1,1], name=name+\"/Branch_2/Conv3d_0a_1x1\")\n",
    "        self.b2b = Unit3D(out_channels[3], out_channels[4], kernel_shape=[3,3,3], name=name+\"/Branch_2/Conv3d_0b_3x3\")\n",
    "        self.b3a = MaxPool3dSamePadding(kernel_size=[3,3,3], stride=(1,1,1))\n",
    "        self.b3b = Unit3D(in_channels, out_channels[5], kernel_shape=[1,1,1], name=name+\"/Branch_3/Conv3d_0b_1x1\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        b0 = self.b0(x)\n",
    "        b1 = self.b1b(self.b1a(x))\n",
    "        b2 = self.b2b(self.b2a(x))\n",
    "        b3 = self.b3b(self.b3a(x))\n",
    "        return torch.cat([b0, b1, b2, b3], dim=1)\n",
    "\n",
    "\n",
    "class InceptionI3d(nn.Module):\n",
<<<<<<< HEAD
    "    def __init__(self, num_classes=400, spatial_squeeze=True, in_channels=3, dropout_keep_prob=1.0):\n",
=======
    "    def __init__(self, num_classes=400, spatial_squeeze=True, in_channels=3, dropout_keep_prob=0.5):\n",
>>>>>>> khaled
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.spatial_squeeze = spatial_squeeze\n",
    "        self.Conv3d_1a_7x7 = Unit3D(in_channels, 64, kernel_shape=[7,7,7], stride=(2,2,2), name=\"Conv3d_1a_7x7\")\n",
    "        self.MaxPool3d_2a_3x3 = MaxPool3dSamePadding(kernel_size=[1,3,3], stride=(1,2,2))\n",
    "        self.Conv3d_2b_1x1 = Unit3D(64, 64, kernel_shape=[1,1,1], name=\"Conv3d_2b_1x1\")\n",
    "        self.Conv3d_2c_3x3 = Unit3D(64, 192, kernel_shape=[3,3,3], name=\"Conv3d_2c_3x3\")\n",
    "        self.MaxPool3d_3a_3x3 = MaxPool3dSamePadding(kernel_size=[1,3,3], stride=(1,2,2))\n",
    "        self.Mixed_3b = InceptionModule(192, [64,96,128,16,32,32], \"Mixed_3b\")\n",
    "        self.Mixed_3c = InceptionModule(256, [128,128,192,32,96,64], \"Mixed_3c\")\n",
    "        self.MaxPool3d_4a_3x3 = MaxPool3dSamePadding(kernel_size=[3,3,3], stride=(2,2,2))\n",
    "        self.Mixed_4b = InceptionModule(480, [192,96,208,16,48,64], \"Mixed_4b\")\n",
    "        self.Mixed_4c = InceptionModule(512, [160,112,224,24,64,64], \"Mixed_4c\")\n",
    "        self.Mixed_4d = InceptionModule(512, [128,128,256,24,64,64], \"Mixed_4d\")\n",
    "        self.Mixed_4e = InceptionModule(512, [112,144,288,32,64,64], \"Mixed_4e\")\n",
    "        self.Mixed_4f = InceptionModule(528, [256,160,320,32,128,128], \"Mixed_4f\")\n",
    "        self.MaxPool3d_5a_2x2 = MaxPool3dSamePadding(kernel_size=[2,2,2], stride=(2,2,2))\n",
    "        self.Mixed_5b = InceptionModule(832, [256,160,320,32,128,128], \"Mixed_5b\")\n",
    "        self.Mixed_5c = InceptionModule(832, [384,192,384,48,128,128], \"Mixed_5c\")\n",
    "        self.avg_pool = nn.AvgPool3d(kernel_size=[2,7,7], stride=(1,1,1))\n",
<<<<<<< HEAD
    "        # Note: dropout_keep_prob=1.0 means NO dropout (matching 87.69% config)\n",
    "        self.dropout = nn.Dropout(1.0 - dropout_keep_prob) if dropout_keep_prob < 1.0 else nn.Identity()\n",
=======
    "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
>>>>>>> khaled
    "        self.logits = Unit3D(1024, num_classes, kernel_shape=[1,1,1], activation_fn=None,\n",
    "                             use_batch_norm=False, use_bias=True, name=\"logits\")\n",
    "\n",
    "    def replace_logits(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.logits = Unit3D(1024, num_classes, kernel_shape=[1,1,1], activation_fn=None,\n",
    "                             use_batch_norm=False, use_bias=True, name=\"logits\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Conv3d_1a_7x7(x)\n",
    "        x = self.MaxPool3d_2a_3x3(x)\n",
    "        x = self.Conv3d_2b_1x1(x)\n",
    "        x = self.Conv3d_2c_3x3(x)\n",
    "        x = self.MaxPool3d_3a_3x3(x)\n",
    "        x = self.Mixed_3b(x)\n",
    "        x = self.Mixed_3c(x)\n",
    "        x = self.MaxPool3d_4a_3x3(x)\n",
    "        x = self.Mixed_4b(x)\n",
    "        x = self.Mixed_4c(x)\n",
    "        x = self.Mixed_4d(x)\n",
    "        x = self.Mixed_4e(x)\n",
    "        x = self.Mixed_4f(x)\n",
    "        x = self.MaxPool3d_5a_2x2(x)\n",
    "        x = self.Mixed_5b(x)\n",
    "        x = self.Mixed_5c(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.logits(x)\n",
    "        if self.spatial_squeeze:\n",
    "            x = x.squeeze(3).squeeze(3)\n",
    "        x = x.mean(2)\n",
    "        return x\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "print(\"‚úÖ I3D model classes defined:\")\n",
    "print(\"   - MaxPool3dSamePadding\")\n",
    "print(\"   - Unit3D\") \n",
    "print(\"   - InceptionModule\")\n",
    "print(\"   - InceptionI3d\")\n",
    "print(\"\\nüìù Model will be instantiated and loaded in Cell 17 (Training cell)\")"
=======
    "# ============================================================================\n",
    "# Create demo model and transfer weights\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"üéØ Building {NUM_CLASSES_DEMO}-class demo model...\")\n",
    "\n",
    "# Step 1: Create model with 100 classes first to load checkpoint\n",
    "model_100 = InceptionI3d(num_classes=100, in_channels=3, dropout_keep_prob=0.5)\n",
    "model_100.load_state_dict(state_dict_100)\n",
    "print(\"‚úÖ Loaded 100-class weights into temporary model\")\n",
    "\n",
    "# Step 2: Create the demo model with correct number of classes\n",
    "model_demo = InceptionI3d(num_classes=NUM_CLASSES_DEMO, in_channels=3, dropout_keep_prob=0.5)\n",
    "\n",
    "# Step 3: Copy backbone weights (everything except logits)\n",
    "backbone_state = {k: v for k, v in model_100.state_dict().items() if not k.startswith(\"logits\")}\n",
    "model_demo.load_state_dict(backbone_state, strict=False)\n",
    "print(f\"‚úÖ Copied backbone weights ({len(backbone_state)} keys)\")\n",
    "\n",
    "# Step 4: Transfer classifier weights for overlapping classes\n",
    "old_weight = model_100.logits.conv3d.weight.data  # [100, 1024, 1, 1, 1]\n",
    "old_bias = model_100.logits.conv3d.bias.data      # [100]\n",
    "\n",
    "new_weight = model_demo.logits.conv3d.weight.data  # [NUM_CLASSES_DEMO, 1024, 1, 1, 1]\n",
    "new_bias = model_demo.logits.conv3d.bias.data      # [NUM_CLASSES_DEMO]\n",
    "\n",
    "transferred = 0\n",
    "for demo_label, old_label in demo_to_100_label.items():\n",
    "    new_weight[demo_label] = old_weight[old_label]\n",
    "    new_bias[demo_label] = old_bias[old_label]\n",
    "    transferred += 1\n",
    "\n",
    "print(f\"‚úÖ Transferred classifier weights for {transferred} overlapping classes\")\n",
    "print(f\"üÜï Remaining {NUM_CLASSES_DEMO - transferred} classes initialized randomly\")\n",
    "\n",
    "# Clean up\n",
    "del model_100\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"\\nüéØ Demo model ready: InceptionI3d with {NUM_CLASSES_DEMO} classes\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model_demo.parameters()):,}\")"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "587febb3",
   "metadata": {
    "papermill": {
     "duration": 0.005398,
     "end_time": "2025-12-09T22:51:04.944188",
     "exception": false,
     "start_time": "2025-12-09T22:51:04.938790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ‚ö†Ô∏è SWITCH TO CPU FOR PREPROCESSING\n",
    "\n",
    "The next cell (Cell 13) preprocesses all demo videos to `.npz` format. This is CPU-bound (video decoding + resizing), so:\n",
    "\n",
    "1. **Use CPU accelerator** (saves GPU quota)\n",
    "2. **Uses 4 workers** (parallel processing)\n",
    "3. Takes ~10-15 minutes depending on total video count\n",
    "\n",
    "After preprocessing completes, **switch to GPU** for training (see instructions after Cell 15)."
=======
   "id": "1ff081db",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è SWITCH TO CPU FOR PREPROCESSING\n",
    "\n",
    "The next cell preprocesses all 2941 demo videos to `.npz` format. This is CPU-bound (video decoding + resizing), so:\n",
    "\n",
    "1. **Use CPU accelerator** (saves GPU quota)\n",
    "2. **Uses 4 workers** (parallel processing)\n",
    "3. Takes ~10-15 minutes for 2941 videos\n",
    "\n",
    "After preprocessing completes, **switch to GPU** for training."
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "c415c10a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:51:04.955914Z",
     "iopub.status.busy": "2025-12-09T22:51:04.955680Z",
     "iopub.status.idle": "2025-12-09T22:56:30.513704Z",
     "shell.execute_reply": "2025-12-09T22:56:30.512841Z"
    },
    "papermill": {
     "duration": 325.565632,
     "end_time": "2025-12-09T22:56:30.515062",
     "exception": false,
     "start_time": "2025-12-09T22:51:04.949430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Preprocessed output: /kaggle/working/SignBridge_demo/preprocessed\n",
      "üìä Total videos to preprocess: 2420\n",
      "   Already preprocessed: train=0, val=0\n",
      "\n",
      "üîÑ Preprocessing with 4 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2420/2420 [05:25<00:00,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Preprocessing complete!\n",
      "   Processed: 2420\n",
      "   Skipped (already existed): 0\n",
      "   Failed: 0\n",
      "\n",
      "üíæ Saved preprocessed manifest to: /kaggle/working/SignBridge_demo/manifests/demo_preprocessed_manifest.csv\n",
      "\n",
      "üíæ Disk space used: 6.50 GB\n",
      "   Average per video: 2.75 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
=======
   "id": "76e59005",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 13: Preprocess Demo Videos to NPZ (CPU, 4 workers) ----------\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing config (matching your 87.6% training)\n",
    "NUM_FRAMES = 32\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Output directory for preprocessed files\n",
    "PREPROCESSED_DIR = os.path.join(BASE_OUTPUT_DIR, \"preprocessed\")\n",
    "os.makedirs(os.path.join(PREPROCESSED_DIR, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(PREPROCESSED_DIR, \"val\"), exist_ok=True)\n",
<<<<<<< HEAD
    "os.makedirs(os.path.join(PREPROCESSED_DIR, \"test\"), exist_ok=True)\n",
=======
>>>>>>> khaled
    "\n",
    "print(f\"üìÅ Preprocessed output: {PREPROCESSED_DIR}\")\n",
    "\n",
    "def preprocess_video(video_path: str, num_frames: int = 32, image_size: int = 224):\n",
    "    \"\"\"\n",
    "    Read video, sample frames uniformly, resize to (image_size, image_size).\n",
    "    Returns: numpy array of shape (num_frames, H, W, 3), dtype uint8\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    \n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    if len(frames) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Sample frames uniformly to get exactly num_frames\n",
    "    total = len(frames)\n",
    "    if total >= num_frames:\n",
    "        indices = np.linspace(0, total - 1, num_frames, dtype=int)\n",
    "    else:\n",
    "        # Repeat frames if video is too short\n",
    "        indices = np.linspace(0, total - 1, num_frames, dtype=int)\n",
    "    \n",
    "    sampled = [frames[i] for i in indices]\n",
    "    \n",
    "    # Resize and convert BGR -> RGB\n",
    "    resized = []\n",
    "    for f in sampled:\n",
    "        f_resized = cv2.resize(f, (image_size, image_size), interpolation=cv2.INTER_LINEAR)\n",
    "        f_rgb = cv2.cvtColor(f_resized, cv2.COLOR_BGR2RGB)\n",
    "        resized.append(f_rgb)\n",
    "    \n",
    "    return np.stack(resized, axis=0).astype(np.uint8)  # (T, H, W, 3)\n",
    "\n",
    "\n",
    "def process_one_video(row):\n",
    "    \"\"\"Process a single video and save as NPZ. Returns metadata dict or None.\"\"\"\n",
    "    video_id = row[\"video_id\"]\n",
    "    video_path = row[\"video_path\"]\n",
    "    gloss = row[\"gloss\"]\n",
    "    label = row[\"label\"]\n",
    "    split = row[\"split\"]\n",
    "    \n",
    "    save_path = os.path.join(PREPROCESSED_DIR, split, f\"{video_id}.npz\")\n",
    "    \n",
    "    # Skip if already exists\n",
    "    if os.path.exists(save_path):\n",
    "        return {\n",
    "            \"video_id\": video_id,\n",
    "            \"gloss\": gloss,\n",
    "            \"label\": label,\n",
    "            \"split\": split,\n",
    "            \"save_path\": save_path,\n",
    "            \"status\": \"skipped\"\n",
    "        }\n",
    "    \n",
    "    # Check if source video exists\n",
    "    if not os.path.exists(video_path):\n",
    "        return None\n",
    "    \n",
    "    # Preprocess\n",
    "    frames = preprocess_video(video_path, NUM_FRAMES, IMAGE_SIZE)\n",
    "    if frames is None:\n",
    "        return None\n",
    "    \n",
    "    # Save as compressed NPZ (uint8)\n",
    "    np.savez_compressed(save_path, frames=frames)\n",
    "    \n",
    "    return {\n",
    "        \"video_id\": video_id,\n",
    "        \"gloss\": gloss,\n",
    "        \"label\": label,\n",
    "        \"split\": split,\n",
    "        \"save_path\": save_path,\n",
    "        \"status\": \"processed\"\n",
    "    }\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "# Load train, val, and test manifests\n",
    "train_df = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_train_manifest.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_val_manifest.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_test_manifest.csv\"))\n",
    "\n",
    "all_rows = pd.concat([train_df, val_df, test_df], ignore_index=True).to_dict(\"records\")\n",
=======
    "# Load train and val manifests\n",
    "train_df = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_train_manifest.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_val_manifest.csv\"))\n",
    "\n",
    "all_rows = pd.concat([train_df, val_df], ignore_index=True).to_dict(\"records\")\n",
>>>>>>> khaled
    "print(f\"üìä Total videos to preprocess: {len(all_rows)}\")\n",
    "\n",
    "# Check how many already exist\n",
    "existing_train = len(list(Path(os.path.join(PREPROCESSED_DIR, \"train\")).glob(\"*.npz\")))\n",
    "existing_val = len(list(Path(os.path.join(PREPROCESSED_DIR, \"val\")).glob(\"*.npz\")))\n",
<<<<<<< HEAD
    "existing_test = len(list(Path(os.path.join(PREPROCESSED_DIR, \"test\")).glob(\"*.npz\")))\n",
    "print(f\"   Already preprocessed: train={existing_train}, val={existing_val}, test={existing_test}\")\n",
=======
    "print(f\"   Already preprocessed: train={existing_train}, val={existing_val}\")\n",
>>>>>>> khaled
    "\n",
    "# Process with 4 workers\n",
    "NUM_WORKERS = 4\n",
    "results = []\n",
    "failed = 0\n",
    "\n",
    "print(f\"\\nüîÑ Preprocessing with {NUM_WORKERS} workers...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = {executor.submit(process_one_video, row): row for row in all_rows}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Preprocessing\"):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "# Summary\n",
    "processed = sum(1 for r in results if r[\"status\"] == \"processed\")\n",
    "skipped = sum(1 for r in results if r[\"status\"] == \"skipped\")\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "print(f\"   Processed: {processed}\")\n",
    "print(f\"   Skipped (already existed): {skipped}\")\n",
    "print(f\"   Failed: {failed}\")\n",
    "\n",
    "# Save preprocessed manifest\n",
    "df_preprocessed = pd.DataFrame(results)\n",
    "preprocessed_manifest_path = os.path.join(MANIFESTS_DIR, \"demo_preprocessed_manifest.csv\")\n",
    "df_preprocessed.to_csv(preprocessed_manifest_path, index=False)\n",
    "print(f\"\\nüíæ Saved preprocessed manifest to: {preprocessed_manifest_path}\")\n",
    "\n",
    "# Check disk usage\n",
    "total_size = 0\n",
<<<<<<< HEAD
    "for split in [\"train\", \"val\", \"test\"]:\n",
=======
    "for split in [\"train\", \"val\"]:\n",
>>>>>>> khaled
    "    split_dir = os.path.join(PREPROCESSED_DIR, split)\n",
    "    for npz_file in Path(split_dir).glob(\"*.npz\"):\n",
    "        total_size += npz_file.stat().st_size\n",
    "\n",
    "print(f\"\\nüíæ Disk space used: {total_size / (1024**3):.2f} GB\")\n",
    "print(f\"   Average per video: {total_size / max(len(results), 1) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "36e9cd53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:56:30.636093Z",
     "iopub.status.busy": "2025-12-09T22:56:30.635827Z",
     "iopub.status.idle": "2025-12-09T22:56:30.656076Z",
     "shell.execute_reply": "2025-12-09T22:56:30.654751Z"
    },
    "papermill": {
     "duration": 0.080351,
     "end_time": "2025-12-09T22:56:30.657336",
     "exception": false,
     "start_time": "2025-12-09T22:56:30.576985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total preprocessed videos: 2420\n",
      "   Train: 2093\n",
      "   Val: 327\n",
      "\n",
      "üìä Class distribution (55 classes):\n",
      "   Min samples: 4 (i)\n",
      "   Max samples: 76 (drink)\n",
      "   Mean samples: 44.0\n",
      "   Median samples: 39.0\n",
      "\n",
      "‚ö†Ô∏è Sparse classes (< 20 samples): 1\n",
      "   i: 4\n",
      "\n",
      "üìã Bottom 10 classes:\n",
      "   i           : 4\n",
      "   he          : 30\n",
      "   friend      : 36\n",
      "   father      : 36\n",
      "   hello       : 36\n",
      "   my          : 36\n",
      "   need        : 36\n",
      "   have        : 36\n",
      "   see         : 36\n",
      "   hungry      : 37\n",
      "\n",
      "üìã Top 10 classes:\n",
      "   go          : 46\n",
      "   they        : 63\n",
      "   eat         : 70\n",
      "   want        : 71\n",
      "   how         : 71\n",
      "   fine        : 71\n",
      "   woman       : 73\n",
      "   pizza       : 74\n",
      "   what        : 75\n",
      "   drink       : 76\n"
     ]
    }
   ],
=======
   "id": "b7243459",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 14: Check Class Distribution After Preprocessing ----------\n",
    "\n",
    "# Load the successfully preprocessed manifest\n",
    "df_preprocessed = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_preprocessed_manifest.csv\"))\n",
    "\n",
    "print(f\"‚úÖ Total preprocessed videos: {len(df_preprocessed)}\")\n",
    "print(f\"   Train: {len(df_preprocessed[df_preprocessed['split'] == 'train'])}\")\n",
    "print(f\"   Val: {len(df_preprocessed[df_preprocessed['split'] == 'val'])}\")\n",
<<<<<<< HEAD
    "print(f\"   Test: {len(df_preprocessed[df_preprocessed['split'] == 'test'])}\")\n",
=======
>>>>>>> khaled
    "\n",
    "# Check class distribution\n",
    "class_counts = df_preprocessed.groupby(\"gloss\").size().sort_values()\n",
    "\n",
    "print(f\"\\nüìä Class distribution ({len(class_counts)} classes):\")\n",
    "print(f\"   Min samples: {class_counts.min()} ({class_counts.idxmin()})\")\n",
    "print(f\"   Max samples: {class_counts.max()} ({class_counts.idxmax()})\")\n",
    "print(f\"   Mean samples: {class_counts.mean():.1f}\")\n",
    "print(f\"   Median samples: {class_counts.median():.1f}\")\n",
    "\n",
    "# Show classes with < 20 samples (may need extra augmentation)\n",
    "sparse_classes = class_counts[class_counts < 20]\n",
    "if len(sparse_classes) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Sparse classes (< 20 samples): {len(sparse_classes)}\")\n",
    "    for gloss, count in sparse_classes.items():\n",
    "        print(f\"   {gloss}: {count}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All classes have >= 20 samples\")\n",
    "\n",
    "# Show bottom 10 and top 10\n",
    "print(f\"\\nüìã Bottom 10 classes:\")\n",
    "for gloss, count in class_counts.head(10).items():\n",
    "    print(f\"   {gloss:12s}: {count}\")\n",
    "\n",
    "print(f\"\\nüìã Top 10 classes:\")\n",
    "for gloss, count in class_counts.tail(10).items():\n",
    "    print(f\"   {gloss:12s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "aa303525",
   "metadata": {
    "papermill": {
     "duration": 0.061582,
     "end_time": "2025-12-09T22:56:30.777977",
     "exception": false,
     "start_time": "2025-12-09T22:56:30.716395",
     "status": "completed"
    },
    "tags": []
   },
=======
   "id": "73ab07fa",
   "metadata": {},
>>>>>>> khaled
   "source": [
    "## ‚ö†Ô∏è SWITCH TO GPU FOR TRAINING\n",
    "\n",
    "Preprocessing is complete. Now:\n",
    "\n",
<<<<<<< HEAD
    "1. **Go to Settings ‚Üí Accelerator ‚Üí GPU T4 x2** (or P100)\n",
    "2. **Restart the notebook** (Run ‚Üí Restart & Clear Outputs)\n",
    "3. **Re-run cells 1-12** (they're fast, just loading data and building model)\n",
    "4. **Skip cells 13-15** (preprocessing is done, data is saved)\n",
    "5. **Continue from Cell 16** (Dataset & DataLoaders)\n",
    "\n",
    "The preprocessed `.npz` files are saved in `/kaggle/working/SignBridge_demo/preprocessed/` and will persist.\n",
    "\n",
    "**Current version: v4 (55 classes)** - includes MS-ASL data for better pronoun coverage."
=======
    "1. **Go to Settings ‚Üí Accelerator ‚Üí GPU**\n",
    "2. **Restart the notebook** (Run ‚Üí Restart & Clear Outputs)\n",
    "3. **Re-run cells 1-11** (they're fast, just loading data and building model)\n",
    "4. **Skip cells 12-14** (preprocessing is done, data is saved)\n",
    "5. **Continue from Cell 16** (training loop)\n",
    "\n",
    "The preprocessed `.npz` files are saved in `/kaggle/working/SignBridge_demo/preprocessed/` and will persist.\n",
    "\n",
    "**Current version: v3 (55 classes)** - includes he, they pronouns."
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "6d9c4c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:56:30.896465Z",
     "iopub.status.busy": "2025-12-09T22:56:30.896193Z",
     "iopub.status.idle": "2025-12-09T22:56:30.969229Z",
     "shell.execute_reply": "2025-12-09T22:56:30.968334Z"
    },
    "papermill": {
     "duration": 0.134317,
     "end_time": "2025-12-09T22:56:30.970376",
     "exception": false,
     "start_time": "2025-12-09T22:56:30.836059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train samples: 2093, Val samples: 327\n",
      "‚úÖ Train batches: 261, Val batches: 41\n",
      "\n",
      "üìä Class weights (top 5 highest):\n",
      "   i: 10.00x\n",
      "   he: 1.46x\n",
      "   my: 1.27x\n",
      "   see: 1.23x\n",
      "   hello: 1.23x\n",
      "\n",
      "‚úÖ Dataset & DataLoaders ready for 55 classes!\n"
     ]
    }
   ],
   "source": [
    "# ---------- Cell 16: Dataset & DataLoaders v6 (ROBUST AUGMENTATION) ----------\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# ============================================================================\n",
    "# GPU CONFIGURATION\n",
    "# ============================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üñ•Ô∏è GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "    \n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected, using CPU\")\n",
    "\n",
    "# ============================================================================\n",
    "# v6 CONFIGURATION - Anti-Overfitting\n",
    "# ============================================================================\n",
    "BATCH_SIZE = 8    # ‚¨áÔ∏è Reduced from 16 for better generalization\n",
    "USE_AMP = True\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(f\"\\nüìä v6 Training configuration (anti-overfitting):\")\n",
    "print(f\"   Batch size: {BATCH_SIZE} (reduced from 16)\")\n",
    "print(f\"   Mixed precision (AMP): {USE_AMP}\")\n",
    "print(f\"   Workers: {NUM_WORKERS}\")\n",
    "\n",
    "\n",
    "class RobustVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with ROBUST augmentations for better generalization.\n",
    "    \n",
    "    v6 Augmentations:\n",
    "    1. Horizontal flip (existing)\n",
    "    2. Temporal crop (existing)\n",
    "    3. Color jitter - ENHANCED with HSV\n",
    "    4. Spatial crop (existing)\n",
    "    5. Gaussian noise - STRONGER\n",
    "    6. NEW: Temporal speed variation (¬±25%)\n",
    "    7. NEW: Rotation (¬±12¬∞)\n",
    "    8. NEW: Cutout/Random erasing\n",
    "    9. NEW: Frame dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, manifest_df, augment=False, class_sample_counts=None, mixup_alpha=0.0):\n",
    "        self.records = manifest_df.to_dict(\"records\")\n",
    "        self.augment = augment\n",
    "        self.class_sample_counts = class_sample_counts or {}\n",
    "        self.median_count = np.median(list(class_sample_counts.values())) if class_sample_counts else 50\n",
    "        self.mixup_alpha = mixup_alpha  # For mixup (0 = disabled)\n",
=======
   "id": "f56d7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 16: Dataset & DataLoaders for Fine-Tuning ----------\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class PreprocessedVideoDataset(Dataset):\n",
    "    \"\"\"Dataset for loading preprocessed NPZ video files with augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, manifest_df, augment=False):\n",
    "        self.records = manifest_df.to_dict(\"records\")\n",
    "        self.augment = augment\n",
>>>>>>> khaled
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        npz_path = record[\"save_path\"]\n",
    "        label = record[\"label\"]\n",
<<<<<<< HEAD
    "        gloss = record[\"gloss\"]\n",
    "        \n",
    "        data = np.load(npz_path)\n",
    "        frames = data[\"frames\"].astype(np.float32) / 255.0  # (T, H, W, C)\n",
    "        \n",
    "        if self.augment:\n",
    "            sample_count = self.class_sample_counts.get(gloss, self.median_count)\n",
    "            augment_intensity = min(2.5, self.median_count / max(sample_count, 1))\n",
    "            frames = self._augment_robust(frames, intensity=augment_intensity)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        frames = (frames - 0.5) * 2.0\n",
    "        # (T, H, W, C) -> (C, T, H, W)\n",
    "        frames = np.transpose(frames, (3, 0, 1, 2))\n",
    "        \n",
    "        return torch.from_numpy(frames.copy()).float(), label\n",
    "    \n",
    "    def _augment_robust(self, frames, intensity=1.0):\n",
    "        \"\"\"Apply robust augmentations with adaptive intensity.\"\"\"\n",
    "        T, H, W, C = frames.shape\n",
    "        \n",
    "        # 1. Horizontal flip (50-70% based on intensity)\n",
    "        if random.random() < min(0.7, 0.5 * intensity):\n",
    "            frames = frames[:, :, ::-1, :].copy()\n",
    "        \n",
    "        # 2. Temporal speed variation (NEW) - ¬±25% speed change\n",
    "        if random.random() < 0.5:\n",
    "            speed_factor = random.uniform(0.75, 1.25)\n",
    "            new_T = int(T * speed_factor)\n",
    "            new_T = max(20, min(48, new_T))  # Clamp to reasonable range\n",
    "            indices_src = np.linspace(0, T - 1, new_T, dtype=int)\n",
    "            frames_resampled = frames[indices_src]\n",
    "            # Resample back to 32 frames\n",
    "            indices_dst = np.linspace(0, len(frames_resampled) - 1, 32, dtype=int)\n",
    "            frames = frames_resampled[indices_dst]\n",
    "            T = 32\n",
    "        \n",
    "        # 3. Temporal crop (existing, improved)\n",
    "        if random.random() < min(0.6, 0.4 * intensity) and T >= 28:\n",
    "            crop_len = random.randint(26, min(T, 32))\n",
    "            start = random.randint(0, T - crop_len)\n",
    "            frames = frames[start:start + crop_len]\n",
    "            if len(frames) != 32:\n",
    "                indices = np.linspace(0, len(frames) - 1, 32, dtype=int)\n",
    "                frames = frames[indices]\n",
    "            T = 32\n",
    "        \n",
    "        # 4. Rotation (NEW) - ¬±12 degrees\n",
    "        if random.random() < min(0.4, 0.3 * intensity):\n",
    "            angle = random.uniform(-12, 12)\n",
    "            frames_rotated = []\n",
    "            for f in frames:\n",
    "                # Rotate around center\n",
    "                center = (W // 2, H // 2)\n",
    "                M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "                f_rot = cv2.warpAffine(f, M, (W, H), borderMode=cv2.BORDER_REFLECT)\n",
    "                frames_rotated.append(f_rot)\n",
    "            frames = np.stack(frames_rotated)\n",
    "        \n",
    "        # 5. Spatial crop/zoom (existing, improved)\n",
    "        if random.random() < min(0.5, 0.35 * intensity):\n",
    "            scale = random.uniform(max(0.7, 0.8 - 0.1 * (intensity - 1)), 0.95)\n",
    "            new_size = int(H * scale)\n",
    "            y = random.randint(0, H - new_size)\n",
    "            x = random.randint(0, W - new_size)\n",
    "            frames = frames[:, y:y + new_size, x:x + new_size, :]\n",
    "            frames = np.stack([cv2.resize(f, (W, H)) for f in frames])\n",
    "        \n",
    "        # 6. Enhanced color jitter with HSV (IMPROVED)\n",
    "        if random.random() < min(0.6, 0.4 * intensity):\n",
    "            # Brightness and contrast (existing)\n",
    "            brightness = random.uniform(0.7, 1.3)\n",
    "            contrast = random.uniform(-0.2, 0.2)\n",
    "            frames = frames * brightness + contrast\n",
    "            \n",
    "            # HSV shift (NEW) - subtle hue/saturation changes\n",
    "            if random.random() < 0.3:\n",
    "                for i in range(len(frames)):\n",
    "                    f_uint8 = (np.clip(frames[i], 0, 1) * 255).astype(np.uint8)\n",
    "                    hsv = cv2.cvtColor(f_uint8, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "                    hsv[:, :, 0] = (hsv[:, :, 0] + random.uniform(-10, 10)) % 180  # Hue\n",
    "                    hsv[:, :, 1] = np.clip(hsv[:, :, 1] * random.uniform(0.8, 1.2), 0, 255)  # Saturation\n",
    "                    hsv = hsv.astype(np.uint8)\n",
    "                    frames[i] = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB).astype(np.float32) / 255.0\n",
    "            \n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        \n",
    "        # 7. Cutout / Random erasing (NEW)\n",
    "        if random.random() < min(0.4, 0.3 * intensity):\n",
    "            # Erase a random rectangle for some frames\n",
    "            num_cutouts = random.randint(1, 2)\n",
    "            for _ in range(num_cutouts):\n",
    "                # Random patch size (10-30% of image)\n",
    "                cut_h = random.randint(int(H * 0.1), int(H * 0.3))\n",
    "                cut_w = random.randint(int(W * 0.1), int(W * 0.3))\n",
    "                cut_y = random.randint(0, H - cut_h)\n",
    "                cut_x = random.randint(0, W - cut_w)\n",
    "                # Apply to random subset of frames (30-70% of frames)\n",
    "                start_frame = random.randint(0, max(0, T - int(T * 0.5)))\n",
    "                end_frame = min(T, start_frame + random.randint(int(T * 0.3), int(T * 0.7)))\n",
    "                # Fill with mean color or black\n",
    "                fill_value = random.choice([0.0, 0.5, frames.mean()])\n",
    "                frames[start_frame:end_frame, cut_y:cut_y + cut_h, cut_x:cut_x + cut_w, :] = fill_value\n",
    "        \n",
    "        # 8. Frame dropout (NEW) - drop 2-4 frames and duplicate neighbors\n",
    "        if random.random() < min(0.3, 0.2 * intensity):\n",
    "            num_drop = random.randint(2, 4)\n",
    "            drop_indices = sorted(random.sample(range(1, T - 1), min(num_drop, T - 2)))\n",
    "            for idx in drop_indices:\n",
    "                # Replace with previous frame\n",
    "                frames[idx] = frames[idx - 1]\n",
    "        \n",
    "        # 9. Stronger Gaussian noise (IMPROVED)\n",
    "        if random.random() < min(0.5, 0.35 * intensity):\n",
    "            noise_std = random.uniform(0.02, 0.05) * intensity\n",
    "            noise = np.random.normal(0, noise_std, frames.shape).astype(np.float32)\n",
=======
    "        \n",
    "        # Load preprocessed frames\n",
    "        data = np.load(npz_path)\n",
    "        frames = data[\"frames\"]  # (T, H, W, 3), uint8\n",
    "        \n",
    "        # Convert to float [0, 1]\n",
    "        frames = frames.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Apply augmentations (training only)\n",
    "        if self.augment:\n",
    "            frames = self._augment(frames)\n",
    "        \n",
    "        # Normalize to [-1, 1] (matching I3D training)\n",
    "        frames = (frames - 0.5) * 2.0\n",
    "        \n",
    "        # Convert to (C, T, H, W) for I3D\n",
    "        frames = np.transpose(frames, (3, 0, 1, 2))  # (3, T, H, W)\n",
    "        \n",
    "        return torch.from_numpy(frames), label\n",
    "    \n",
    "    def _augment(self, frames):\n",
    "        \"\"\"Apply augmentations matching our 87.6% training.\"\"\"\n",
    "        T, H, W, C = frames.shape\n",
    "        \n",
    "        # Horizontal flip (50%)\n",
    "        if random.random() < 0.5:\n",
    "            frames = frames[:, :, ::-1, :].copy()\n",
    "        \n",
    "        # Temporal jitter: random crop of 28-32 frames, then resize back to 32\n",
    "        if random.random() < 0.5 and T >= 28:\n",
    "            crop_len = random.randint(28, min(T, 32))\n",
    "            start = random.randint(0, T - crop_len)\n",
    "            frames = frames[start:start+crop_len]\n",
    "            # Resize back to 32 frames\n",
    "            if len(frames) != 32:\n",
    "                indices = np.linspace(0, len(frames)-1, 32, dtype=int)\n",
    "                frames = frames[indices]\n",
    "        \n",
    "        # Brightness/contrast jitter\n",
    "        if random.random() < 0.3:\n",
    "            brightness = random.uniform(0.8, 1.2)\n",
    "            contrast = random.uniform(0.8, 1.2)\n",
    "            frames = frames * contrast + (brightness - 1.0)\n",
    "            frames = np.clip(frames, 0, 1)\n",
    "        \n",
    "        # Spatial crop (random 80-100% crop, resize back)\n",
    "        if random.random() < 0.3:\n",
    "            scale = random.uniform(0.8, 1.0)\n",
    "            new_size = int(H * scale)\n",
    "            y = random.randint(0, H - new_size)\n",
    "            x = random.randint(0, W - new_size)\n",
    "            frames = frames[:, y:y+new_size, x:x+new_size, :]\n",
    "            # Resize back (using simple nearest for speed)\n",
    "            import cv2\n",
    "            resized = []\n",
    "            for f in frames:\n",
    "                resized.append(cv2.resize(f, (W, H), interpolation=cv2.INTER_LINEAR))\n",
    "            frames = np.stack(resized)\n",
    "        \n",
    "        # Gaussian noise\n",
    "        if random.random() < 0.2:\n",
    "            noise = np.random.normal(0, 0.02, frames.shape).astype(np.float32)\n",
>>>>>>> khaled
    "            frames = np.clip(frames + noise, 0, 1)\n",
    "        \n",
    "        return frames\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "# ============================================================================\n",
    "# Mixup collate function (optional, for DataLoader)\n",
    "# ============================================================================\n",
    "def mixup_collate(batch, alpha=0.2, num_classes=55):\n",
    "    \"\"\"\n",
    "    Mixup: blend pairs of samples with soft labels.\n",
    "    Returns: (mixed_frames, soft_labels)\n",
    "    \"\"\"\n",
    "    frames, labels = zip(*batch)\n",
    "    frames = torch.stack(frames)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    if alpha > 0:\n",
    "        # Sample lambda from Beta distribution\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam = max(lam, 1 - lam)  # Ensure lam >= 0.5\n",
    "        \n",
    "        # Random permutation for mixing pairs\n",
    "        batch_size = frames.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "        # Mix frames\n",
    "        mixed_frames = lam * frames + (1 - lam) * frames[index]\n",
    "        \n",
    "        # Create soft labels (one-hot)\n",
    "        labels_onehot = torch.zeros(batch_size, num_classes)\n",
    "        labels_onehot.scatter_(1, labels.unsqueeze(1), lam)\n",
    "        labels_onehot.scatter_(1, labels[index].unsqueeze(1), 1 - lam)\n",
    "        \n",
    "        return mixed_frames, labels_onehot\n",
    "    else:\n",
    "        return frames, labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Create Datasets and DataLoaders\n",
    "# ============================================================================\n",
    "\n",
    "# Load preprocessed manifest\n",
    "df_preprocessed = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_preprocessed_manifest.csv\"))\n",
    "train_df = df_preprocessed[df_preprocessed[\"split\"] == \"train\"]\n",
    "val_df = df_preprocessed[df_preprocessed[\"split\"] == \"val\"]\n",
    "\n",
    "print(f\"\\nüìä Dataset: {len(train_df)} train, {len(val_df)} val\")\n",
    "\n",
    "# Class sample counts for adaptive augmentation\n",
    "class_sample_counts = train_df.groupby(\"gloss\").size().to_dict()\n",
    "print(f\"   Min: {min(class_sample_counts.values())} ({min(class_sample_counts, key=class_sample_counts.get)})\")\n",
    "print(f\"   Max: {max(class_sample_counts.values())}\")\n",
    "\n",
    "# Create datasets with robust augmentation\n",
    "train_dataset = RobustVideoDataset(train_df, augment=True, class_sample_counts=class_sample_counts)\n",
    "val_dataset = RobustVideoDataset(val_df, augment=False)\n",
    "\n",
    "# Weighted sampler (same as before)\n",
    "label_counts = train_df.groupby(\"label\").size()\n",
    "sample_weights = torch.DoubleTensor([\n",
    "    len(train_df) / (NUM_CLASSES_DEMO * label_counts[row[\"label\"]]) \n",
    "    for _, row in train_df.iterrows()\n",
    "])\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_df), replacement=True)\n",
    "\n",
    "# DataLoaders with smaller batch size AND MixUp\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler,\n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True, \n",
    "    drop_last=True,\n",
    "    # ENABLE MIXUP HERE:\n",
    "    collate_fn=lambda b: mixup_collate(b, alpha=0.4, num_classes=NUM_CLASSES_DEMO)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n",
    "print(f\"   (More batches = more gradient updates per epoch)\")\n",
    "print(f\"   ‚úÖ MixUp enabled (alpha=0.4)\")\n",
    "\n",
    "# Class weights (sqrt-scaled)\n",
    "class_weights = torch.zeros(NUM_CLASSES_DEMO)\n",
    "for label, count in label_counts.items():\n",
    "    class_weights[label] = np.sqrt(len(train_df) / (NUM_CLASSES_DEMO * count))\n",
    "class_weights = torch.clamp(class_weights, min=0.5, max=5.0)\n",
    "\n",
    "print(f\"\\nüéØ v6 Augmentations enabled:\")\n",
    "print(f\"   ‚úÖ Horizontal flip\")\n",
    "print(f\"   ‚úÖ Temporal speed variation (¬±25%)\")\n",
    "print(f\"   ‚úÖ Temporal crop\")\n",
    "print(f\"   ‚úÖ Rotation (¬±12¬∞)\")\n",
    "print(f\"   ‚úÖ Spatial crop/zoom\")\n",
    "print(f\"   ‚úÖ Enhanced color jitter (HSV)\")\n",
    "print(f\"   ‚úÖ Cutout/Random erasing\")\n",
    "print(f\"   ‚úÖ Frame dropout\")\n",
    "print(f\"   ‚úÖ Stronger Gaussian noise\")\n",
    "print(f\"\\n‚úÖ Ready for v6 training on {DEVICE}!\")"
=======
    "# Load preprocessed manifest\n",
    "df_preprocessed = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_preprocessed_manifest.csv\"))\n",
    "\n",
    "train_df = df_preprocessed[df_preprocessed[\"split\"] == \"train\"]\n",
    "val_df = df_preprocessed[df_preprocessed[\"split\"] == \"val\"]\n",
    "\n",
    "print(f\"üìä Train samples: {len(train_df)}, Val samples: {len(val_df)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PreprocessedVideoDataset(train_df, augment=True)\n",
    "val_dataset = PreprocessedVideoDataset(val_df, augment=False)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 8  # Adjust based on GPU memory (8 works for T4)\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Compute class weights for imbalanced classes (using dynamic NUM_CLASSES_DEMO)\n",
    "class_counts = train_df.groupby(\"label\").size()\n",
    "total_samples = len(train_df)\n",
    "\n",
    "# Inverse frequency weighting with smoothing\n",
    "class_weights = torch.zeros(NUM_CLASSES_DEMO)\n",
    "for label, count in class_counts.items():\n",
    "    class_weights[label] = total_samples / (NUM_CLASSES_DEMO * count)\n",
    "\n",
    "# Cap extreme weights (max 10x)\n",
    "class_weights = torch.clamp(class_weights, max=10.0)\n",
    "print(f\"\\nüìä Class weights (top 5 highest):\")\n",
    "top_weights = torch.topk(class_weights, 5)\n",
    "for idx, weight in zip(top_weights.indices.tolist(), top_weights.values.tolist()):\n",
    "    gloss = label_to_gloss_demo[idx]\n",
    "    print(f\"   {gloss}: {weight:.2f}x\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset & DataLoaders ready for {NUM_CLASSES_DEMO} classes!\")"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "08807fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T22:56:31.091348Z",
     "iopub.status.busy": "2025-12-09T22:56:31.091097Z",
     "iopub.status.idle": "2025-12-10T00:30:31.560662Z",
     "shell.execute_reply": "2025-12-10T00:30:31.559517Z"
    },
    "papermill": {
     "duration": 5640.53138,
     "end_time": "2025-12-10T00:30:31.561880",
     "exception": false,
     "start_time": "2025-12-09T22:56:31.030500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Device: cuda\n",
      "\n",
      "üìã Training config:\n",
      "   Model: 55 classes\n",
      "   Epochs: 30\n",
      "   Backbone LR: 1e-05, Classifier LR: 0.0001\n",
      "   Label smoothing: 0.1\n",
      "   Early stopping patience: 7\n",
      "\n",
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/30 | Train: 53.9% (loss=2.6778) | Val: 73.1% (loss=2.1561) | LR: 9.97e-06 | Time: 210s\n",
      "   üíæ New best model saved! Val acc: 73.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/30 | Train: 76.8% (loss=1.8808) | Val: 82.9% (loss=1.6692) | LR: 9.89e-06 | Time: 208s\n",
      "   üíæ New best model saved! Val acc: 82.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/30 | Train: 84.3% (loss=1.5675) | Val: 87.8% (loss=1.4825) | LR: 9.76e-06 | Time: 209s\n",
      "   üíæ New best model saved! Val acc: 87.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/30 | Train: 87.1% (loss=1.4153) | Val: 87.2% (loss=1.3990) | LR: 9.57e-06 | Time: 209s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/30 | Train: 89.1% (loss=1.3098) | Val: 88.4% (loss=1.3313) | LR: 9.34e-06 | Time: 209s\n",
      "   üíæ New best model saved! Val acc: 88.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/30 | Train: 90.7% (loss=1.2588) | Val: 88.4% (loss=1.3021) | LR: 9.05e-06 | Time: 209s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/30 | Train: 90.5% (loss=1.2411) | Val: 89.6% (loss=1.2755) | LR: 8.73e-06 | Time: 209s\n",
      "   üíæ New best model saved! Val acc: 89.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/30 | Train: 92.3% (loss=1.1959) | Val: 89.9% (loss=1.2598) | LR: 8.36e-06 | Time: 208s\n",
      "   üíæ New best model saved! Val acc: 89.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/30 | Train: 93.5% (loss=1.1703) | Val: 89.0% (loss=1.2519) | LR: 7.96e-06 | Time: 209s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train: 94.3% (loss=1.1454) | Val: 90.2% (loss=1.2268) | LR: 7.53e-06 | Time: 209s\n",
      "   üíæ New best model saved! Val acc: 90.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train: 95.0% (loss=1.1309) | Val: 89.9% (loss=1.2253) | LR: 7.06e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train: 94.7% (loss=1.1111) | Val: 89.3% (loss=1.2218) | LR: 6.58e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train: 95.3% (loss=1.1050) | Val: 89.9% (loss=1.2083) | LR: 6.08e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train: 95.3% (loss=1.0941) | Val: 89.6% (loss=1.2010) | LR: 5.57e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train: 96.6% (loss=1.0744) | Val: 90.5% (loss=1.1960) | LR: 5.05e-06 | Time: 209s\n",
      "   üíæ New best model saved! Val acc: 90.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train: 96.6% (loss=1.0724) | Val: 89.9% (loss=1.1904) | LR: 4.53e-06 | Time: 209s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train: 95.5% (loss=1.0666) | Val: 90.8% (loss=1.1808) | LR: 4.02e-06 | Time: 209s\n",
      "   üíæ New best model saved! Val acc: 90.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train: 97.5% (loss=1.0448) | Val: 90.8% (loss=1.1791) | LR: 3.52e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train: 97.3% (loss=1.0373) | Val: 90.2% (loss=1.1841) | LR: 3.04e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train: 96.2% (loss=1.0531) | Val: 91.1% (loss=1.1777) | LR: 2.58e-06 | Time: 209s\n",
      "   üíæ New best model saved! Val acc: 91.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train: 96.9% (loss=1.0509) | Val: 91.1% (loss=1.1735) | LR: 2.14e-06 | Time: 209s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train: 97.0% (loss=1.0311) | Val: 90.8% (loss=1.1715) | LR: 1.74e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train: 97.1% (loss=1.0372) | Val: 90.8% (loss=1.1689) | LR: 1.37e-06 | Time: 209s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train: 97.1% (loss=1.0374) | Val: 91.1% (loss=1.1669) | LR: 1.05e-06 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train: 97.4% (loss=1.0274) | Val: 90.8% (loss=1.1668) | LR: 7.63e-07 | Time: 208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train: 97.7% (loss=1.0236) | Val: 90.5% (loss=1.1660) | LR: 5.28e-07 | Time: 209s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train: 97.4% (loss=1.0268) | Val: 91.1% (loss=1.1661) | LR: 3.42e-07 | Time: 209s\n",
      "\n",
      "‚èπÔ∏è Early stopping triggered at epoch 27\n",
      "\n",
      "‚úÖ Training complete!\n",
      "   Total time: 93.9 minutes\n",
      "   Best validation accuracy: 91.13%\n",
      "   Best model saved to: /kaggle/working/SignBridge_demo/checkpoints/best_demo_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ---------- Cell 17: Training Loop v6 (Anti-Overfitting Config) ----------\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "import types\n",
    "\n",
    "# ===========================================================================\n",
    "# üöÄ TRAINING CONFIGURATION v6 - Anti-Overfitting\n",
    "# ===========================================================================\n",
    "# Changes from v5:\n",
    "# 1. ADD dropout (0.5) - stronger dropout\n",
    "# 2. Smaller batch size (8) - done in Cell 16\n",
    "# 3. Robust augmentations - done in Cell 16\n",
    "# 4. Slightly lower LR - helps with smaller batches\n",
    "# 5. More patience - let augmentations do their work\n",
    "# 6. MixUp enabled - requires handling soft labels\n",
    "# ===========================================================================\n",
    "\n",
    "# v6 CONFIG - Anti-overfitting\n",
    "EPOCHS = 75                  # ‚¨ÜÔ∏è Increased epoch count for MixUp\n",
    "LR = 5e-5                    # ‚¨áÔ∏è Lower LR for smaller batches\n",
    "WEIGHT_DECAY = 1e-5          # ‚¨ÜÔ∏è Slightly more regularization\n",
    "ADAM_EPS = 1e-3\n",
    "LABEL_SMOOTHING = 0.15       # ‚¨ÜÔ∏è Slightly more smoothing\n",
    "PATIENCE = 15                # ‚¨ÜÔ∏è More patience (robust aug = slower convergence)\n",
    "LR_PATIENCE = 4             # ‚¨ÜÔ∏è More patience before LR reduction\n",
    "LR_FACTOR = 0.5\n",
    "GRAD_CLIP = 1.0\n",
    "DROPOUT = 0.5                # ‚¨ÜÔ∏è Stronger dropout (was 0.3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ TRAINING CONFIGURATION v6 - Anti-Overfitting\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Changes from v5:\")\n",
    "print(f\"   ‚Ä¢ Dropout: {DROPOUT} (was 0.3)\")\n",
    "print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE} (was 16)\")\n",
    "print(f\"   ‚Ä¢ Learning rate: {LR:.0e} (was 1e-4)\")\n",
    "print(f\"   ‚Ä¢ Weight decay: {WEIGHT_DECAY:.0e} (was 1e-8)\")\n",
    "print(f\"   ‚Ä¢ Label smoothing: {LABEL_SMOOTHING} (was 0.1)\")\n",
    "print(f\"   ‚Ä¢ Epochs: {EPOCHS} (was 50)\")\n",
    "print(f\"   ‚Ä¢ Patience: {PATIENCE} (was 10)\")\n",
    "print(f\"   ‚Ä¢ Robust augmentations: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ MixUp regularization: ‚úÖ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===========================================================================\n",
    "# CREATE FRESH MODEL & LOAD 87.6% CHECKPOINT\n",
    "# ===========================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîß CREATING MODEL & LOADING 87.6% CHECKPOINT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear any previous GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"   ‚úÖ Cleared GPU cache\")\n",
    "\n",
    "# Create fresh model WITH DROPOUT (v6 change!)\n",
    "print(f\"\\n1Ô∏è‚É£ Creating InceptionI3d model with dropout={DROPOUT}...\")\n",
    "model_demo = InceptionI3d(\n",
    "    num_classes=NUM_CLASSES_DEMO, \n",
    "    in_channels=3, \n",
    "    dropout_keep_prob=1.0 - DROPOUT  # 0.5 = 50% dropout\n",
    ")\n",
    "print(f\"   ‚úÖ Model created with {NUM_CLASSES_DEMO} classes, dropout={DROPOUT}\")\n",
    "\n",
    "# Load checkpoint\n",
    "# Pointing to the 87.6% model (trained on WLASL + Citizen) for better robustness\n",
    "CHECKPOINT_PATH = \"/kaggle/input/best-model-87/best_model_citizen100_87pct.pth\"\n",
    "print(f\"\\n2Ô∏è‚É£ Loading 87.6% checkpoint from: {CHECKPOINT_PATH}...\")\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"‚ö†Ô∏è Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    # Fallback to the variable if defined elsewhere, or error out\n",
    "    try:\n",
    "        # Try the variable from earlier cells if it differs\n",
    "        from_cell_9 = \"/kaggle/input/wlasl-finetuned-full-model/wlasl100_best_model_75.15pct_FULL.pth\"\n",
    "        if os.path.exists(from_cell_9):\n",
    "            print(f\"   Using fallback 75.15% model: {from_cell_9}\")\n",
    "            CHECKPOINT_PATH = from_cell_9\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No checkpoint found!\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create temporary CV module structure for pickle compatibility\n",
    "cv_module = types.ModuleType('CV')\n",
    "cv_models = types.ModuleType('CV.models')\n",
    "cv_i3d = types.ModuleType('CV.models.i3d')\n",
    "\n",
    "cv_i3d.Unit3D = Unit3D\n",
    "cv_i3d.MaxPool3dSamePadding = MaxPool3dSamePadding\n",
    "cv_i3d.InceptionModule = InceptionModule\n",
    "cv_i3d.InceptionI3d = InceptionI3d\n",
    "\n",
    "sys.modules['CV'] = cv_module\n",
    "sys.modules['CV.models'] = cv_models\n",
    "sys.modules['CV.models.i3d'] = cv_i3d\n",
    "cv_module.models = cv_models\n",
    "cv_models.i3d = cv_i3d\n",
    "print(f\"   ‚úÖ Created CV module structure\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)\n",
    "print(f\"   ‚úÖ Loaded checkpoint from disk\")\n",
    "\n",
    "# Extract state dict\n",
    "if isinstance(checkpoint, nn.Module):\n",
    "    state_dict = checkpoint.state_dict()\n",
    "elif isinstance(checkpoint, dict):\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        if isinstance(checkpoint['model'], nn.Module):\n",
    "            state_dict = checkpoint['model'].state_dict()\n",
    "        else:\n",
    "            state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "else:\n",
    "    raise ValueError(f\"Unknown checkpoint format: {type(checkpoint)}\")\n",
    "\n",
    "print(f\"   üìä Checkpoint has {len(state_dict)} keys\")\n",
    "\n",
    "# Load backbone weights (exclude logits - different num_classes)\n",
    "print(f\"\\n3Ô∏è‚É£ Loading backbone weights...\")\n",
    "model_dict = model_demo.state_dict()\n",
    "backbone_dict = {k: v for k, v in state_dict.items() \n",
    "                 if k in model_dict and 'logits' not in k and v.shape == model_dict[k].shape}\n",
    "model_dict.update(backbone_dict)\n",
    "model_demo.load_state_dict(model_dict)\n",
    "print(f\"   ‚úÖ Loaded {len(backbone_dict)}/{len(state_dict)} backbone layers\")\n",
    "\n",
    "# Initialize classifier\n",
    "print(f\"\\n4Ô∏è‚É£ Initializing fresh classifier...\")\n",
    "nn.init.xavier_uniform_(model_demo.logits.conv3d.weight)\n",
    "nn.init.zeros_(model_demo.logits.conv3d.bias)\n",
    "print(f\"   ‚úÖ Classifier initialized\")\n",
    "\n",
    "# Clean up\n",
    "del checkpoint\n",
    "del state_dict\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Move to GPU\n",
    "model_demo = model_demo.to(DEVICE)\n",
    "print(f\"\\n5Ô∏è‚É£ Model ready on {DEVICE}\")\n",
    "\n",
    "# Parameter count\n",
    "backbone_params = sum(p.numel() for n, p in model_demo.named_parameters() if 'logits' not in n)\n",
    "classifier_params = sum(p.numel() for n, p in model_demo.named_parameters() if 'logits' in n)\n",
    "print(f\"\\nüìä Parameters: {backbone_params:,} backbone + {classifier_params:,} classifier\")\n",
    "\n",
    "# ===========================================================================\n",
    "# SETUP TRAINING COMPONENTS\n",
    "# ===========================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚öôÔ∏è SETTING UP TRAINING COMPONENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Move class weights to device\n",
    "class_weights_gpu = class_weights.to(DEVICE)\n",
    "\n",
    "# Loss with label smoothing\n",
    "# Note: CrossEntropyLoss expects class indices, but with MixUp we have soft labels\n",
    "# so we'll handle that in the train_epoch function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_gpu, label_smoothing=LABEL_SMOOTHING)\n",
    "print(f\"   ‚úÖ Loss: CrossEntropyLoss (smoothing={LABEL_SMOOTHING})\")\n",
    "\n",
    "# Optimizer with increased weight decay\n",
    "optimizer = optim.AdamW(  # AdamW is better for weight decay\n",
    "    model_demo.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eps=ADAM_EPS\n",
    ")\n",
    "print(f\"   ‚úÖ Optimizer: AdamW (lr={LR:.0e}, wd={WEIGHT_DECAY:.0e})\")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=LR_FACTOR,\n",
    "    patience=LR_PATIENCE,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "print(f\"   ‚úÖ Scheduler: ReduceLROnPlateau (patience={LR_PATIENCE})\")\n",
    "\n",
    "# Mixed precision\n",
    "scaler = GradScaler('cuda', enabled=USE_AMP)\n",
    "print(f\"   ‚úÖ Mixed Precision: {'Enabled' if USE_AMP else 'Disabled'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ v6 ANTI-OVERFITTING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Batch size: {BATCH_SIZE} (2x more updates/epoch)\")\n",
    "print(f\"   Dropout: {DROPOUT} (prevents memorization)\")\n",
    "print(f\"   Augmentations: 9 robust transforms\")\n",
    "print(f\"   MixUp: Enabled (interpolates samples)\")\n",
    "print(f\"   Weight decay: {WEIGHT_DECAY:.0e} (L2 regularization)\")\n",
    "print(f\"   Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüéØ Target: Reduce train-val gap while maintaining >70% val accuracy!\")\n",
    "\n",
    "# ===========================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ===========================================================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scaler, device, use_amp=True, grad_clip=1.0):\n",
    "    \"\"\"Train one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for frames, labels in pbar:\n",
    "        frames = frames.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast('cuda', enabled=use_amp):\n",
    "            outputs = model(frames)\n",
    "            \n",
    "            # Handle MixUp soft labels\n",
    "            if labels.dim() > 1:  # Soft labels (one-hot)\n",
    "                # CrossEntropyLoss expects class indices, so we use soft target calculation manually or\n",
    "                # simply use the fact that CE(pred, target) = -sum(target * log_softmax(pred))\n",
    "                log_probs = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "                loss = -(labels * log_probs).sum(dim=1).mean()\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item() * frames.size(0)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        if labels.dim() > 1:\n",
    "            _, targets = labels.max(1)\n",
    "        else:\n",
    "            targets = labels\n",
    "            \n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.3f}\", \n",
    "            \"acc\": f\"{100*correct/total:.1f}%\"\n",
    "        })\n",
=======
   "id": "8e829da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 17: Training Loop with Fine-Tuning Strategy ----------\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# Training Configuration\n",
    "# ============================================================================\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Device: {DEVICE}\")\n",
    "\n",
    "# Fine-tuning hyperparameters\n",
    "EPOCHS = 30\n",
    "LR = 1e-5  # Low LR for fine-tuning (was 1e-4 for scratch)\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTHING = 0.1\n",
    "PATIENCE = 7  # Early stopping patience\n",
    "\n",
    "# Move model to device\n",
    "model_demo = model_demo.to(DEVICE)\n",
    "class_weights = class_weights.to(DEVICE)\n",
    "\n",
    "# Loss function with label smoothing and class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# Optimizer - use lower LR for backbone, higher for new classifier\n",
    "backbone_params = [p for n, p in model_demo.named_parameters() if not n.startswith(\"logits\")]\n",
    "classifier_params = [p for n, p in model_demo.named_parameters() if n.startswith(\"logits\")]\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": LR},\n",
    "    {\"params\": classifier_params, \"lr\": LR * 10}  # 10x LR for new classifier head\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Cosine annealing scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LR * 0.01)\n",
    "\n",
    "print(f\"\\nüìã Training config:\")\n",
    "print(f\"   Model: {NUM_CLASSES_DEMO} classes\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Backbone LR: {LR}, Classifier LR: {LR * 10}\")\n",
    "print(f\"   Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"   Early stopping patience: {PATIENCE}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Training Functions\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for frames, labels in pbar:\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * frames.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100*correct/total:.1f}%\"})\n",
>>>>>>> khaled
    "    \n",
    "    return total_loss / total, 100 * correct / total\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "def validate(model, loader, criterion, device, use_amp=True):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            frames = frames.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast('cuda', enabled=use_amp):\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
=======
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
>>>>>>> khaled
    "            \n",
    "            total_loss += loss.item() * frames.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
<<<<<<< HEAD
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / total, 100 * correct / total, all_preds, all_labels\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# TRAINING LOOP\n",
    "# ===========================================================================\n",
=======
    "    \n",
    "    return total_loss / total, 100 * correct / total\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop\n",
    "# ============================================================================\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
>>>>>>> khaled
    "\n",
    "CHECKPOINT_DIR = os.path.join(BASE_OUTPUT_DIR, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
<<<<<<< HEAD
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"lr\": [], \"gap\": []}\n",
    "\n",
    "print(f\"\\nüöÄ Starting v6 training (anti-overfitting config)...\")\n",
    "print(f\"   Train: {len(train_df)} samples, {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_df)} samples, {len(val_loader)} batches\")\n",
    "print(f\"   Target: <10% train-val gap, >75% val accuracy\")\n",
=======
    "print(f\"\\nüöÄ Starting training...\")\n",
>>>>>>> khaled
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
<<<<<<< HEAD
    "    train_loss, train_acc = train_epoch(\n",
    "        model_demo, train_loader, criterion, optimizer, scaler, DEVICE, USE_AMP, GRAD_CLIP\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(\n",
    "        model_demo, val_loader, criterion, DEVICE, USE_AMP\n",
    "    )\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Calculate gap\n",
    "    gap = train_acc - val_acc\n",
=======
    "    train_loss, train_acc = train_epoch(model_demo, train_loader, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model_demo, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
>>>>>>> khaled
    "    \n",
    "    # Record history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
<<<<<<< HEAD
    "    history[\"lr\"].append(current_lr)\n",
    "    history[\"gap\"].append(gap)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Status indicators\n",
    "    gap_status = \"‚úÖ\" if gap < 10 else (\"‚ö†Ô∏è\" if gap < 15 else \"üî¥\")\n",
    "    best_marker = \"üìà\" if val_acc > best_val_acc else \"  \"\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "          f\"Train: {train_acc:5.1f}% | Val: {val_acc:5.1f}% | \"\n",
    "          f\"Gap: {gap:+5.1f}% {gap_status} | LR: {current_lr:.1e} | {epoch_time:.0f}s {best_marker}\")\n",
=======
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
    "          f\"Train: {train_acc:.1f}% (loss={train_loss:.4f}) | \"\n",
    "          f\"Val: {val_acc:.1f}% (loss={val_loss:.4f}) | \"\n",
    "          f\"LR: {current_lr:.2e} | Time: {epoch_time:.0f}s\")\n",
>>>>>>> khaled
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model_demo.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
<<<<<<< HEAD
    "            \"val_acc\": val_acc,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"gap\": gap,\n",
    "            \"num_classes\": NUM_CLASSES_DEMO,\n",
    "            \"label_mapping\": label_mapping_demo,\n",
    "            \"demo_labels\": DEMO_LABELS,\n",
    "            \"config\": \"v6_anti_overfitting\",\n",
    "            \"dropout\": DROPOUT\n",
    "        }\n",
    "        best_path = os.path.join(CHECKPOINT_DIR, \"best_demo_model_v6.pth\")\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"   üíæ New best: {val_acc:.2f}% (gap: {gap:+.1f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ v6 TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Final train-val gap: {history['gap'][-1]:+.1f}%\")\n",
    "print(f\"   Model saved: {best_path}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick gap analysis\n",
    "avg_gap = np.mean(history['gap'][-10:]) if len(history['gap']) >= 10 else np.mean(history['gap'])\n",
    "if avg_gap < 10:\n",
    "    print(f\"üéâ Excellent! Gap < 10% = good generalization\")\n",
    "elif avg_gap < 15:\n",
    "    print(f\"‚úÖ Good! Gap < 15% = acceptable generalization\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Gap still high. Consider: more data, stronger augmentation, or simpler model\")"
=======
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"history\": history,\n",
    "            \"num_classes\": NUM_CLASSES_DEMO,\n",
    "            \"label_mapping\": label_mapping_demo\n",
    "        }\n",
    "        \n",
    "        best_path = os.path.join(CHECKPOINT_DIR, \"best_demo_model.pth\")\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"   üíæ New best model saved! Val acc: {val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Save final model\n",
    "final_path = os.path.join(CHECKPOINT_DIR, \"final_demo_model.pth\")\n",
    "torch.save({\n",
    "    \"epoch\": epoch + 1,\n",
    "    \"model_state_dict\": model_demo.state_dict(),\n",
    "    \"val_acc\": val_acc,\n",
    "    \"num_classes\": NUM_CLASSES_DEMO,\n",
    "    \"label_mapping\": label_mapping_demo\n",
    "}, final_path)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Best model saved to: {best_path}\")"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "e638d006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T00:30:33.010355Z",
     "iopub.status.busy": "2025-12-10T00:30:33.009768Z",
     "iopub.status.idle": "2025-12-10T00:30:33.850533Z",
     "shell.execute_reply": "2025-12-10T00:30:33.849557Z"
    },
    "papermill": {
     "duration": 1.515346,
     "end_time": "2025-12-10T00:30:33.851718",
     "exception": false,
     "start_time": "2025-12-10T00:30:32.336372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä TRAINING SUMMARY\n",
      "============================================================\n",
      "   Model: InceptionI3d (55 classes)\n",
      "   Training samples: 2093\n",
      "   Validation samples: 327\n",
      "   Best validation accuracy: 91.13%\n",
      "   Epochs trained: 27\n",
      "   Checkpoint saved: /kaggle/working/SignBridge_demo/checkpoints/best_demo_model.pth\n",
      "============================================================\n",
      "\n",
      "üíæ Model copied to: /kaggle/working/demo_model_55class.pth (ready for download)\n"
     ]
    }
   ],
=======
   "id": "131a66ae",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 18: Training Curves & Summary ----------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
<<<<<<< HEAD
    "import os\n",
    "\n",
    "# Plot training curves (3 plots)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\", marker=\"o\", markersize=3)\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\", marker=\"s\", markersize=3)\n",
=======
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "axes[0].plot(history[\"val_loss\"], label=\"Val Loss\", marker=\"s\")\n",
>>>>>>> khaled
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training & Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
<<<<<<< HEAD
    "axes[1].plot(history[\"train_acc\"], label=\"Train Acc\", marker=\"o\", markersize=3)\n",
    "axes[1].plot(history[\"val_acc\"], label=\"Val Acc\", marker=\"s\", markersize=3)\n",
    "axes[1].axhline(y=best_val_acc, color=\"r\", linestyle=\"--\", alpha=0.5)\n",
=======
    "axes[1].plot(history[\"train_acc\"], label=\"Train Acc\", marker=\"o\")\n",
    "axes[1].plot(history[\"val_acc\"], label=\"Val Acc\", marker=\"s\")\n",
>>>>>>> khaled
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "axes[1].set_title(\"Training & Validation Accuracy\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
<<<<<<< HEAD
    "axes[1].annotate(f\"Best: {best_val_acc:.1f}%\", xy=(len(history[\"val_acc\"])-1, best_val_acc),\n",
    "                 xytext=(5, 5), textcoords=\"offset points\", fontsize=9, color=\"red\")\n",
    "\n",
    "# Learning Rate\n",
    "axes[2].plot(history[\"lr\"], label=\"Classifier LR\", color=\"green\", marker=\".\", markersize=2)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Learning Rate\")\n",
    "axes[2].set_title(\"Learning Rate Schedule (OneCycleLR)\")\n",
    "axes[2].set_yscale(\"log\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
=======
    "axes[1].axhline(y=best_val_acc, color=\"r\", linestyle=\"--\", alpha=0.5, label=f\"Best: {best_val_acc:.1f}%\")\n",
>>>>>>> khaled
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(BASE_OUTPUT_DIR, \"training_curves.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
<<<<<<< HEAD
    "# Define baseline constant for display if not present\n",
    "USE_WLASL2000_BASELINE = False\n",
    "\n",
=======
>>>>>>> khaled
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Model: InceptionI3d ({NUM_CLASSES_DEMO} classes)\")\n",
<<<<<<< HEAD
    "print(f\"   Baseline: {'WLASL-2000 (75.15%)' if USE_WLASL2000_BASELINE else '87.6% model'}\")\n",
=======
>>>>>>> khaled
    "print(f\"   Training samples: {len(train_df)}\")\n",
    "print(f\"   Validation samples: {len(val_df)}\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Epochs trained: {len(history['train_loss'])}\")\n",
    "print(f\"   Checkpoint saved: {best_path}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Copy best model to easy download location\n",
    "import shutil\n",
    "final_model_path = f\"/kaggle/working/demo_model_{NUM_CLASSES_DEMO}class.pth\"\n",
    "shutil.copy(best_path, final_model_path)\n",
<<<<<<< HEAD
    "print(f\"\\nüíæ Model copied to: {final_model_path} (ready for download)\")\n",
    "print(f\"\\nüìã Next: Run Cell 20 for HELD-OUT TEST SET evaluation!\")"
=======
    "print(f\"\\nüíæ Model copied to: {final_model_path} (ready for download)\")"
>>>>>>> khaled
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "id": "c98c4d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T00:30:35.330501Z",
     "iopub.status.busy": "2025-12-10T00:30:35.329672Z",
     "iopub.status.idle": "2025-12-10T00:30:48.876065Z",
     "shell.execute_reply": "2025-12-10T00:30:48.874991Z"
    },
    "papermill": {
     "duration": 14.260224,
     "end_time": "2025-12-10T00:30:48.877730",
     "exception": false,
     "start_time": "2025-12-10T00:30:34.617506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loaded best model (epoch 20, val acc 91.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:13<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä PER-CLASS ACCURACY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üîµ OLD CLASSES (from 100-class model): 25 classes\n",
      "   Average accuracy: 90.2%\n",
      "\n",
      "   Gloss          Acc  Samples\n",
      "   ------------------------------\n",
      "   need          50.0%       4   ‚ö†Ô∏è\n",
      "   who           71.4%       7   \n",
      "   no            80.0%       5   \n",
      "   like          80.0%       5   \n",
      "   work          80.0%       5   \n",
      "   before        83.3%       6   \n",
      "   go            83.3%       6   \n",
      "   time          83.3%       6   \n",
      "   later         87.5%       8   \n",
      "   fine          88.9%       9   \n",
      "   eat           88.9%       9   \n",
      "   want          88.9%       9   \n",
      "   woman         90.0%      10   \n",
      "   drink        100.0%      12   \n",
      "   help         100.0%       7   \n",
      "   yes          100.0%       7   \n",
      "   hot          100.0%       6   \n",
      "   mother       100.0%       4   \n",
      "   now          100.0%       6   \n",
      "   what         100.0%      10   \n",
      "   family       100.0%       4   \n",
      "   man          100.0%       6   \n",
      "   pizza        100.0%       9   \n",
      "   school       100.0%       5   \n",
      "   how          100.0%       7   \n",
      "\n",
      "üÜï NEW CLASSES (learned from scratch): 30 classes\n",
      "   Average accuracy: 88.4%\n",
      "\n",
      "   Gloss          Acc  Samples\n",
      "   ------------------------------\n",
      "   i              0.0%       1   ‚ö†Ô∏è\n",
      "   tomorrow      50.0%       4   ‚ö†Ô∏è\n",
      "   happy         60.0%       5   ‚ö†Ô∏è\n",
      "   father        75.0%       4   \n",
      "   friend        80.0%       5   \n",
      "   your          80.0%       5   \n",
      "   water         83.3%       6   \n",
      "   why           83.3%       6   \n",
      "   home          83.3%       6   \n",
      "   stay          83.3%       6   \n",
      "   cold          85.7%       7   \n",
      "   they          87.5%       8   \n",
      "   sick         100.0%       5   \n",
      "   good         100.0%       5   \n",
      "   know         100.0%       6   \n",
      "   you          100.0%       6   \n",
      "   day          100.0%       5   \n",
      "   where        100.0%       6   \n",
      "   tired        100.0%       5   \n",
      "   please       100.0%       5   \n",
      "   when         100.0%       5   \n",
      "   hello        100.0%       5   \n",
      "   sorry        100.0%       4   \n",
      "   my           100.0%       6   \n",
      "   ok           100.0%       6   \n",
      "   hungry       100.0%       5   \n",
      "   see          100.0%       5   \n",
      "   we           100.0%       5   \n",
      "   he           100.0%       4   \n",
      "   have         100.0%       4   \n",
      "\n",
      "============================================================\n",
      "üìà SUMMARY\n",
      "============================================================\n",
      "   OLD classes (25): 90.2% average\n",
      "   NEW classes (30): 88.4% average\n",
      "   Gap: 1.8%\n",
      "\n",
      "   Overall validation accuracy: 91.13%\n",
      "   Total classes: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "4744e527",
   "metadata": {},
   "outputs": [],
>>>>>>> khaled
   "source": [
    "# ---------- Cell 19: Per-Class Accuracy Analysis ----------\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load best checkpoint\n",
    "best_ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "model_demo.load_state_dict(best_ckpt[\"model_state_dict\"])\n",
    "print(f\"üì¶ Loaded best model (epoch {best_ckpt['epoch']}, val acc {best_ckpt['val_acc']:.2f}%)\")\n",
    "\n",
    "# Evaluate per-class accuracy\n",
    "model_demo.eval()\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for frames, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        frames = frames.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        outputs = model_demo(frames)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        for pred, label in zip(predicted.cpu().numpy(), labels.cpu().numpy()):\n",
    "            gloss = label_to_gloss_demo[label]\n",
    "            class_total[gloss] += 1\n",
    "            if pred == label:\n",
    "                class_correct[gloss] += 1\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "per_class_acc = {}\n",
    "for gloss in class_total:\n",
    "    acc = 100 * class_correct[gloss] / class_total[gloss]\n",
    "    per_class_acc[gloss] = (acc, class_total[gloss])\n",
    "\n",
    "# Separate OLD (from 100-class) vs NEW classes\n",
    "old_classes = {g: v for g, v in per_class_acc.items() if g in CURRENT_100_LABELS}\n",
    "new_classes = {g: v for g, v in per_class_acc.items() if g not in CURRENT_100_LABELS}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä PER-CLASS ACCURACY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüîµ OLD CLASSES (from 100-class model): {len(old_classes)} classes\")\n",
    "old_avg = sum(v[0] for v in old_classes.values()) / len(old_classes) if old_classes else 0\n",
    "print(f\"   Average accuracy: {old_avg:.1f}%\")\n",
    "print(f\"\\n   {'Gloss':<12} {'Acc':>5}  {'Samples':>7}\")\n",
    "print(f\"   {'-'*30}\")\n",
    "for gloss, (acc, count) in sorted(old_classes.items(), key=lambda x: x[1][0]):\n",
    "    marker = \"‚ö†Ô∏è\" if acc < 70 else \"\"\n",
    "    print(f\"   {gloss:<12} {acc:>5.1f}%  {count:>6}   {marker}\")\n",
    "\n",
    "print(f\"\\nüÜï NEW CLASSES (learned from scratch): {len(new_classes)} classes\")\n",
    "new_avg = sum(v[0] for v in new_classes.values()) / len(new_classes) if new_classes else 0\n",
    "print(f\"   Average accuracy: {new_avg:.1f}%\")\n",
    "print(f\"\\n   {'Gloss':<12} {'Acc':>5}  {'Samples':>7}\")\n",
    "print(f\"   {'-'*30}\")\n",
    "for gloss, (acc, count) in sorted(new_classes.items(), key=lambda x: x[1][0]):\n",
    "    marker = \"‚ö†Ô∏è\" if acc < 70 else \"\"\n",
    "    print(f\"   {gloss:<12} {acc:>5.1f}%  {count:>6}   {marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   OLD classes ({len(old_classes)}): {old_avg:.1f}% average\")\n",
    "print(f\"   NEW classes ({len(new_classes)}): {new_avg:.1f}% average\")\n",
    "print(f\"   Gap: {abs(old_avg - new_avg):.1f}%\")\n",
    "print(f\"\\n   Overall validation accuracy: {best_ckpt['val_acc']:.2f}%\")\n",
    "print(f\"   Total classes: {NUM_CLASSES_DEMO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "3e23b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 20: HELD-OUT TEST SET Evaluation (MS-ASL Test Split) ----------\n",
    "\n",
    "# ===========================================================================\n",
    "# üéØ FINAL EVALUATION on MS-ASL TEST SPLIT\n",
    "# ===========================================================================\n",
    "# This is the TRUE measure of generalization:\n",
    "# - MS-ASL test set was NEVER touched during training or validation\n",
    "# - These are completely unseen signers\n",
    "# - This accuracy should match real-world inference performance\n",
    "# ===========================================================================\n",
    "\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ HELD-OUT TEST SET EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load preprocessed manifest\n",
    "preprocessed_df = pd.read_csv(os.path.join(MANIFESTS_DIR, \"demo_preprocessed_manifest.csv\"))\n",
    "\n",
    "# Filter for test split ONLY\n",
    "test_df = preprocessed_df[preprocessed_df[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"\\nüì¶ Test Set Statistics:\")\n",
    "print(f\"   Total samples: {len(test_df)}\")\n",
    "print(f\"   Classes covered: {test_df['gloss'].nunique()} / {NUM_CLASSES_DEMO}\")\n",
    "\n",
    "if len(test_df) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No test samples found in preprocessed manifest!\")\n",
    "    print(\"   This may happen if MS-ASL test split wasn't preprocessed.\")\n",
    "    print(\"   The validation accuracy is your best estimate of performance.\")\n",
    "else:\n",
    "    # Show test set distribution\n",
    "    test_class_counts = test_df.groupby(\"gloss\").size().sort_values()\n",
    "    print(f\"\\n   Class sample distribution:\")\n",
    "    print(f\"   Min: {test_class_counts.min()} ({test_class_counts.idxmin()})\")\n",
    "    print(f\"   Max: {test_class_counts.max()} ({test_class_counts.idxmax()})\")\n",
    "    print(f\"   Median: {test_class_counts.median():.0f}\")\n",
    "    \n",
    "    # Check for missing classes\n",
    "    missing_classes = set(DEMO_LABELS) - set(test_df[\"gloss\"].unique())\n",
    "    if missing_classes:\n",
    "        print(f\"\\n   ‚ö†Ô∏è Classes NOT in test set: {missing_classes}\")\n",
    "    \n",
    "    # Create test dataset and loader\n",
    "    # Using RobustVideoDataset with augment=False for test set\n",
    "    test_dataset = RobustVideoDataset(test_df, augment=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    print(f\"\\n   DataLoader: {len(test_loader)} batches\")\n",
    "    \n",
    "    # Load best model checkpoint\n",
    "    best_ckpt = torch.load(best_path, map_location=DEVICE, weights_only=False)\n",
    "    model_demo.load_state_dict(best_ckpt[\"model_state_dict\"])\n",
    "    print(f\"\\nüì¶ Loaded best model:\")\n",
    "    print(f\"   Epoch: {best_ckpt['epoch']}\")\n",
    "    print(f\"   Validation accuracy: {best_ckpt['val_acc']:.2f}%\")\n",
    "    print(f\"   Train accuracy: {best_ckpt.get('train_acc', 'N/A')}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model_demo.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    class_correct = defaultdict(int)\n",
    "    class_total = defaultdict(int)\n",
    "    all_test_preds = []\n",
    "    all_test_labels = []\n",
    "    all_test_glosses = []\n",
    "    \n",
    "    # Use label_to_gloss_demo from Cell 7\n",
    "    print(\"\\nüß™ Evaluating on HELD-OUT TEST SET...\")\n",
    "    with torch.no_grad():\n",
    "        for idx, (frames, labels) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            frames = frames.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            with autocast('cuda', enabled=USE_AMP):\n",
    "                outputs = model_demo(frames)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            \n",
    "            # Track per-class accuracy\n",
    "            for pred, label in zip(predicted.cpu().numpy(), labels.cpu().numpy()):\n",
    "                gloss = label_to_gloss_demo[label]\n",
    "                class_total[gloss] += 1\n",
    "                if pred == label:\n",
    "                    class_correct[gloss] += 1\n",
    "                all_test_preds.append(pred)\n",
    "                all_test_labels.append(label)\n",
    "                all_test_glosses.append(gloss)\n",
    "    \n",
    "    # Calculate overall test accuracy\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    per_class_acc = {}\n",
    "    for gloss in class_total:\n",
    "        acc = 100 * class_correct[gloss] / class_total[gloss]\n",
    "        per_class_acc[gloss] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"correct\": class_correct[gloss],\n",
    "            \"total\": class_total[gloss]\n",
    "        }\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_classes = sorted(per_class_acc.items(), key=lambda x: x[1][\"accuracy\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä TEST SET RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nüéØ OVERALL TEST ACCURACY: {test_accuracy:.2f}%\")\n",
    "    print(f\"   ({test_correct}/{test_total} samples correct)\")\n",
    "    \n",
    "    print(f\"\\nüìà Comparison:\")\n",
    "    print(f\"   Train accuracy: {best_ckpt.get('train_acc', history['train_acc'][-1]):.2f}%\")\n",
    "    print(f\"   Val accuracy:   {best_ckpt['val_acc']:.2f}%\")\n",
    "    print(f\"   Test accuracy:  {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Gap analysis\n",
    "    val_test_gap = best_ckpt['val_acc'] - test_accuracy\n",
    "    if abs(val_test_gap) < 3:\n",
    "        print(f\"\\n   ‚úÖ Val-Test gap: {val_test_gap:+.1f}% (excellent!)\")\n",
    "    elif abs(val_test_gap) < 7:\n",
    "        print(f\"\\n   ‚ö†Ô∏è Val-Test gap: {val_test_gap:+.1f}% (acceptable)\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚ùå Val-Test gap: {val_test_gap:+.1f}% (concerning)\")\n",
    "    \n",
    "    # Per-class breakdown\n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    print(\"üìã PER-CLASS TEST ACCURACY\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"   {'Gloss':<15} {'Accuracy':>8}  {'Correct':>8}  {'Total':>6}  Status\")\n",
    "    print(f\"   {'-'*55}\")\n",
    "    \n",
    "    for gloss, stats in sorted_classes:\n",
    "        acc = stats[\"accuracy\"]\n",
    "        correct = stats[\"correct\"]\n",
    "        total = stats[\"total\"]\n",
    "        \n",
    "        if acc >= 80:\n",
    "            status = \"‚úÖ\"\n",
    "        elif acc >= 60:\n",
    "            status = \"‚ö†Ô∏è\"\n",
    "        elif acc >= 40:\n",
    "            status = \"üî∂\"\n",
    "        else:\n",
    "            status = \"‚ùå\"\n",
    "        \n",
    "        print(f\"   {gloss:<15} {acc:>7.1f}%  {correct:>8}  {total:>6}  {status}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    accuracies = [s[\"accuracy\"] for s in per_class_acc.values()]\n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    print(\"üìä CLASS ACCURACY STATISTICS\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"   Mean:    {np.mean(accuracies):.1f}%\")\n",
    "    print(f\"   Median:  {np.median(accuracies):.1f}%\")\n",
    "    print(f\"   Std:     {np.std(accuracies):.1f}%\")\n",
    "    print(f\"   Min:     {min(accuracies):.1f}% ({sorted_classes[0][0]})\")\n",
    "    print(f\"   Max:     {max(accuracies):.1f}% ({sorted_classes[-1][0]})\")\n",
    "    \n",
    "    # Classes needing attention\n",
    "    weak_classes = [(g, s) for g, s in sorted_classes if s[\"accuracy\"] < 50]\n",
    "    if weak_classes:\n",
    "        print(f\"\\n‚ö†Ô∏è WEAK CLASSES (< 50% accuracy): {len(weak_classes)}\")\n",
    "        for gloss, stats in weak_classes:\n",
    "            print(f\"   - {gloss}: {stats['accuracy']:.1f}% ({stats['correct']}/{stats['total']})\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèÜ FINAL SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"   Model: InceptionI3d fine-tuned from WLASL-2000 baseline\")\n",
    "    print(f\"   Classes: {NUM_CLASSES_DEMO}\")\n",
    "    print(f\"   Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"   Saved at: {best_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save test results\n",
    "    test_results = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"val_accuracy\": best_ckpt['val_acc'],\n",
    "        \"per_class_accuracy\": per_class_acc,\n",
    "        \"num_classes\": NUM_CLASSES_DEMO,\n",
    "        \"test_samples\": test_total,\n",
    "        \"test_correct\": test_correct\n",
    "    }\n",
    "    results_path = os.path.join(CHECKPOINT_DIR, \"test_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        # Convert to JSON-serializable format\n",
    "        json_results = {\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"val_accuracy\": best_ckpt['val_acc'],\n",
    "            \"num_classes\": NUM_CLASSES_DEMO,\n",
    "            \"test_samples\": test_total,\n",
    "            \"test_correct\": test_correct,\n",
    "            \"per_class_accuracy\": {k: float(v[\"accuracy\"]) for k, v in per_class_acc.items()}\n",
    "        }\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    print(f\"\\nüìÅ Test results saved to: {results_path}\")"
=======
   "id": "fbaa7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Cell 20: Test Model on Video File ----------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess_video_for_inference(video_path, num_frames=32, image_size=224):\n",
    "    \"\"\"Preprocess a video file for model inference.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "    \n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(\"Video has no frames\")\n",
    "    \n",
    "    # Sample frames uniformly\n",
    "    total = len(frames)\n",
    "    indices = np.linspace(0, total - 1, num_frames, dtype=int)\n",
    "    sampled = [frames[i] for i in indices]\n",
    "    \n",
    "    # Resize and convert BGR -> RGB\n",
    "    processed = []\n",
    "    for f in sampled:\n",
    "        f_resized = cv2.resize(f, (image_size, image_size))\n",
    "        f_rgb = cv2.cvtColor(f_resized, cv2.COLOR_BGR2RGB)\n",
    "        processed.append(f_rgb)\n",
    "    \n",
    "    # Stack and normalize\n",
    "    frames_np = np.stack(processed).astype(np.float32) / 255.0\n",
    "    frames_np = (frames_np - 0.5) * 2.0  # [-1, 1]\n",
    "    frames_np = np.transpose(frames_np, (3, 0, 1, 2))  # (C, T, H, W)\n",
    "    \n",
    "    return torch.from_numpy(frames_np).unsqueeze(0)  # (1, C, T, H, W)\n",
    "\n",
    "\n",
    "def predict_video(model, video_path, label_to_gloss, device, top_k=5):\n",
    "    \"\"\"Run inference on a video file.\"\"\"\n",
    "    # Preprocess\n",
    "    video_tensor = preprocess_video_for_inference(video_path)\n",
    "    video_tensor = video_tensor.to(device)\n",
    "    \n",
    "    # Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(video_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs[0].cpu().numpy(), top_indices[0].cpu().numpy()):\n",
    "        gloss = label_to_gloss[int(idx)]\n",
    "        results.append((gloss, float(prob)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Test on a specific video file\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Test on a video from the validation set\n",
    "print(\"üé¨ Testing on a random validation video...\")\n",
    "\n",
    "val_sample = val_df.sample(1).iloc[0]\n",
    "test_video_path = val_sample[\"video_path\"].replace(\"/kaggle/working/SignBridge_demo/preprocessed/val/\", \"\")\n",
    "test_video_path = test_video_path.replace(\".npz\", \".mp4\")\n",
    "\n",
    "# Try to find the original video\n",
    "possible_paths = [\n",
    "    f\"/kaggle/input/wlasl-processed/videos/{val_sample['video_id']}.mp4\",\n",
    "    f\"/kaggle/input/asl-citizen/ASL_Citizen/videos/{val_sample['video_id']}.mp4\",\n",
    "]\n",
    "\n",
    "original_video = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p):\n",
    "        original_video = p\n",
    "        break\n",
    "\n",
    "if original_video:\n",
    "    print(f\"   Video: {original_video}\")\n",
    "    print(f\"   True label: {val_sample['gloss']}\")\n",
    "    \n",
    "    predictions = predict_video(model_demo, original_video, label_to_gloss_demo, DEVICE)\n",
    "    \n",
    "    print(f\"\\nüìä Top-5 Predictions:\")\n",
    "    for i, (gloss, prob) in enumerate(predictions, 1):\n",
    "        marker = \"‚úÖ\" if gloss == val_sample['gloss'] else \"  \"\n",
    "        print(f\"   {marker} {i}. {gloss:12s} ({prob*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Could not find original video file\")\n",
    "\n",
    "# ============================================================================\n",
    "# Option 2: Test on a custom video path\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì§ TO TEST YOUR OWN VIDEO:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. Upload your video to Kaggle:\n",
    "   - Click 'Add Data' ‚Üí 'Upload' ‚Üí select your .mp4 file\n",
    "   - Or use: from google.colab import files; files.upload()\n",
    "\n",
    "2. Then run this code with your video path:\n",
    "\n",
    "   VIDEO_PATH = \"/kaggle/input/your-dataset/your_video.mp4\"\n",
    "   predictions = predict_video(model_demo, VIDEO_PATH, label_to_gloss_demo, DEVICE)\n",
    "   \n",
    "   print(\"Predictions:\")\n",
    "   for gloss, prob in predictions:\n",
    "       print(f\"  {gloss}: {prob*100:.1f}%\")\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# Quick test function for easy reuse\n",
    "# ============================================================================\n",
    "\n",
    "def test_video(video_path):\n",
    "    \"\"\"Quick function to test any video.\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"‚ùå File not found: {video_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üé¨ Testing: {video_path}\")\n",
    "    predictions = predict_video(model_demo, video_path, label_to_gloss_demo, DEVICE)\n",
    "    \n",
    "    print(f\"\\nüìä Predictions:\")\n",
    "    for i, (gloss, prob) in enumerate(predictions, 1):\n",
    "        conf = \"üü¢\" if prob > 0.7 else \"üü°\" if prob > 0.3 else \"üî¥\"\n",
    "        print(f\"   {conf} {i}. {gloss:12s} ({prob*100:.1f}%)\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"\\nüí° Quick test: test_video('/path/to/your/video.mp4')\")"
>>>>>>> khaled
   ]
  }
 ],
 "metadata": {
<<<<<<< HEAD
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1589971,
     "sourceId": 2632847,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4178608,
     "sourceId": 7219693,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8959148,
     "sourceId": 14074338,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8959125,
     "sourceId": 14074302,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
=======
>>>>>>> khaled
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
<<<<<<< HEAD
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6001.224915,
   "end_time": "2025-12-10T00:30:52.322904",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-09T22:50:51.097989",
   "version": "2.6.0"
=======
   "name": "python",
   "pygments_lexer": "ipython3"
>>>>>>> khaled
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
