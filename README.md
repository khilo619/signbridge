# SignBridge

A **real-time sign language video call platform** that bridges communication between deaf/hard-of-hearing users and hearing users.

---

## Features

| Feature | Description |
|---------|-------------|
| **Video Call (WebRTC)** | Peer-to-peer video/audio with TURN relay fallback |
| **Sign Language Recognition** | I3D model (100 classes, ~87% accuracy) via HuggingFace |
| **Gloss â†’ Sentence Refinement** | LLM (OpenAI/Gemini) converts glosses to proper English |
| **Speech-to-Text** | Browser Web Speech API for hearing users â†’ text for deaf users |
| **Real-time Sync** | Pusher Channels for signaling + translation events |

---

## Overview & Goals

- **G1 â€“ Build a strong word-level sign recognizer** (ASL, 100 classes) â†’ **~87.6% top-1** with Inception I3D on Citizen + WLASL100.
- **G2 â€“ Deliver a deployable app**: FastAPI backend + Docker + HF Space for the model; Next.js frontend for calls.
- **G3 â€“ Clean API surface**: HTTP endpoints for other clients (e.g., Flutter) and web API routes in the Next.js app.
- **G4 â€“ Explore extensions**: Speech-to-text (browser), gloss-to-sentence LLM refinement, and real-time prototypes (archived).
- **G5 â€“ Reproducible pipeline**: Training code in `CV/training`, checkpoints in LFS, experiments and W&B logs retained.

Status: G1â€“G5 achieved for the 100-class offline pipeline; real-time and multimodal remain experimental.

---

## Repository Structure (Monorepo)

```text
signbridge/ (root)
â”œâ”€â”€ .github/workflows/              # CI/CD
â”‚   â”œâ”€â”€ deploy.yml                  # Sync to Hugging Face
â”‚   â””â”€â”€ test.yml                    # Pytest on push
â”‚
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ Web/                        # Next.js web app
â”‚       â”œâ”€â”€ pages/                  # Next.js pages + API routes
â”‚       â”‚   â”œâ”€â”€ index.js
â”‚       â”‚   â”œâ”€â”€ room/[roomId].js
â”‚       â”‚   â””â”€â”€ api/
â”‚       â”‚       â”œâ”€â”€ pusher/         # auth.js, trigger.js
â”‚       â”‚       â””â”€â”€ sign/           # predict.js, refine.js
â”‚       â”œâ”€â”€ styles/                 # globals.css
â”‚       â”œâ”€â”€ package.json
â”‚       â”œâ”€â”€ package-lock.json
â”‚       â”œâ”€â”€ next.config.js
â”‚       â”œâ”€â”€ tailwind.config.js
â”‚       â”œâ”€â”€ netlify.toml
â”‚       â”œâ”€â”€ jsconfig.json
â”‚       â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ api/                            # FastAPI backend
â”‚   â”œâ”€â”€ common/                     # health, schemas, video_io
â”‚   â”œâ”€â”€ sign_full/                  # 100-class API (main, routers, config, deps)
â”‚   â””â”€â”€ sign_demo/                  # 55-class demo API
â”‚
â”œâ”€â”€ CV/                             # Computer Vision module
â”‚   â”œâ”€â”€ assets/                     # label mappings
â”‚   â”œâ”€â”€ checkpoints/                # model weights (.pth) via LFS
â”‚   â”œâ”€â”€ data/                       # video reader, transforms
â”‚   â”œâ”€â”€ models/                     # I3D architecture
â”‚   â”œâ”€â”€ inference/                  # SignRecognizer wrapper
â”‚   â”œâ”€â”€ training/                   # training scripts
â”‚   â””â”€â”€ scripts/                    # webcam test, utilities
â”‚
â”œâ”€â”€ notebooks/                      # Primary notebooks
â”‚   â”œâ”€â”€ 01_sign_to_text.ipynb
â”‚   â”œâ”€â”€ 01_speech_to_text.ipynb
â”‚   â”œâ”€â”€ 02_conversational_demo_seed.ipynb
â”‚   â”œâ”€â”€ 02_streaming_speech_to_text.ipynb
â”‚   â””â”€â”€ 05_msasl_downloader.ipynb
â”‚
â”œâ”€â”€ experiments/                    # Archived research
â”‚   â”œâ”€â”€ notebooks/                  # real-time / ISLR experiments
â”‚   â””â”€â”€ wandb/                      # W&B logs
â”‚
â”œâ”€â”€ tests/                          # Python tests
â”‚   â”œâ”€â”€ integration/                # test_api_demo.py
â”‚   â””â”€â”€ unit/                       # test_model.py, test_sign_recognizer.py, test_transforms.py, test_types.py
â”‚
â”œâ”€â”€ configs/                        # JSON configs
â”‚   â”œâ”€â”€ data_config.json
â”‚   â””â”€â”€ train_config.json
â”‚
â”œâ”€â”€ docs/                           # Project docs (MD/PDF/Tex)
â”œâ”€â”€ manifests/                      # (empty placeholder)
â”œâ”€â”€ requirements.txt                # Full Python env
â”œâ”€â”€ requirements-api.txt            # Minimal API deps
â”œâ”€â”€ pyproject.toml                  # Python project config
â”œâ”€â”€ Dockerfile                      # API container
â”œâ”€â”€ setup.sh / setup.bat            # Environment setup
â””â”€â”€ README.md                       # This file
```

---

## Installation (Python)

Choose a full environment (notebooks + training + API) or the minimal API stack.

```bash
# Clone
git clone https://github.com/khilo619/signbridge.git
cd signbridge

# Python env
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Option A: full stack (notebooks + training + API)
pip install -r requirements.txt

# Option B: minimal API only
pip install -r requirements-api.txt
```

Notes:
- Python 3.10+ recommended.
- Git LFS is required to pull the checkpoint: `git lfs install && git lfs pull`.

---

## 1) Web App (Next.js)

### 1.1 Requirements
- **Node.js** 18+
- **Pusher** account (free tier works)
- **Metered TURN** credentials (recommended for cross-network calls)

### 1.2 Setup

```bash
cd signbridge/apps/Web
npm install
```

### 1.3 Environment Variables

Copy `.env.example` to `.env.local`:

```env
# Pusher (required)
PUSHER_APP_ID=
PUSHER_KEY=
PUSHER_SECRET=
PUSHER_CLUSTER=
NEXT_PUBLIC_PUSHER_KEY=
NEXT_PUBLIC_PUSHER_CLUSTER=

# LLM for sentence refinement (optional)
OPENAI_API_KEY=
# or
GEMINI_API_KEY=
```

### 1.4 Run

```bash
npm run dev
```

Open: `http://localhost:3000`

### 1.5 How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deaf User  â”‚â—„â”€â”€â”€â”€â”€â”€ WebRTC Video â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚Hearing User â”‚
â”‚    ğŸ¤Ÿ       â”‚                              â”‚     ğŸ‘‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                            â”‚
       â”‚ Signs â†’ 32-frame clip                      â”‚ Speech
       â–¼                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HuggingFace â”‚                              â”‚ Web Speech  â”‚
â”‚  I3D Model  â”‚                              â”‚    API      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Gloss                                      â”‚ Text
       â–¼                                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  LLM Refine â”‚ (OpenAI/Gemini)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
       â”‚ Sentence                                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Pusher â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    Both users see text
```

---

## 2) Python API (FastAPI)

### 2.1 Requirements
- **Python** 3.10+
- **PyTorch** (GPU recommended)

### 2.2 Setup (API-only)

```bash
cd signbridge
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements-api.txt
```

### 2.3 Run

```bash
# 100-class model
uvicorn api.sign_full.main:app --reload --host 0.0.0.0 --port 8000

# or 55-class demo
uvicorn api.sign_demo.main:app --reload --host 0.0.0.0 --port 8001
```

Open: `http://localhost:8000/docs`

### 2.4 Docker

```bash
docker build -t signbridge-api .
docker run --rm -p 8000:8000 signbridge-api
```

---

## 3) CV Module (I3D Sign Recognition)

### 3.1 Model Info
| Property | Value |
|----------|-------|
| Architecture | Inception I3D |
| Classes | 100 (Citizen + WLASL subset) |
| Top-1 Accuracy | ~87.6% |
| Input | 32 frames @ 25fps |

### 3.2 Assets & Checkpoints
- **Label map**: `CV/assets/label_mapping.json`
- **Checkpoint**: `CV/checkpoints/best_model_citizen100_87pct.pth` (Git LFS)
- **Config**: `CV/config.py` (paths, num_classes=100, frames=32, image_size=224)

### 3.3 Training (reproducible)
- Data: Citizen + WLASL100 cleaned/filtered manifests (see datasets below).
- Preprocessing: 32 frames, 224Ã—224 RGB, uniform sampling; augmentation mirrors notebook (flip, temporal crop/resample, brightness/contrast, small rotations, noise).
- Scripts: `CV/training/train_i3d.py` + `CV/training/datasets.py` for CLI training with JSON manifests.
- Tracking: W&B run for final model (`bj3s5cle`) with ~87.6% top-1.

### 3.2 Usage

```python
from CV.inference import SignRecognizer

recognizer = SignRecognizer()
result = recognizer.predict_clip(frames, topk=5)

print(result.gloss, result.probability)
```

### 3.3 Assets
- **Label mapping**: `CV/assets/label_mapping.json`
- **Checkpoint**: `CV/checkpoints/best_model_citizen100_87pct.pth`

---

## 4) Notebooks

| Notebook | Purpose |
|----------|---------|
| `01_sign_to_text.ipynb` | Offline sign video â†’ text |
| `01_speech_to_text.ipynb` | Speech recognition (experimental) |
| `02_streaming_speech_to_text.ipynb` | Streaming STT prototype |
| `02_conversational_demo_seed.ipynb` | Conversational demo seed |
| `05_msasl_downloader.ipynb` | MS-ASL dataset helper |

---

## 5) Deployment

### 5.1 Web App (Vercel/Netlify)

For **Vercel**:
- Set **Root Directory** = `apps/Web`
- Add environment variables in project settings

For **Netlify**:
- `netlify.toml` is in `apps/Web`
- Set **Base directory** = `apps/Web`

### 5.2 Python API (HuggingFace Spaces / Docker)

The I3D model is deployed on HuggingFace:
`https://khalood619-signbridge-api.hf.space`

---

## 6) Datasets

| Dataset | Modality / size | Role |
|---------|-----------------|------|
| [WLASL2000 â†’ WLASL100](https://www.kaggle.com/datasets/ngphmng/wlasl2000-dataset) | RGB video, 100-gloss subset (~2038 videos, 1013 available) | Core training data for the 100-class I3D |
| [ASL Citizen](https://www.kaggle.com/datasets/abd0kamel/asl-citizen) | RGB video, crowdsourced 100-gloss overlap | Augments WLASL100 for the final 100-class dataset |
| [Google ASL Signs](https://www.kaggle.com/competitions/asl-signs) | Landmark sequences (pose/hands/face) | ISLR/landmark experiments (archived) |

---

## 7) Project Scope & Limitations
- Vocabulary: 100 word-level glosses (ASL); not full sentence translation.
- Modality: RGB I3D pipeline is production; speech/real-time are experimental.
- Language: ASL-focused; other sign languages not covered yet.
- Real-time: Prototypes exist in `experiments/`, not productionized.
- Dataset bias: Trained on Citizen + WLASL; may not cover all dialects/demographics.

---

## 8) Experiments & Research

- `experiments/notebooks/`: real-time prototypes, ISLR (landmark) runs, Colab helpers.
- `experiments/wandb/`: tracked training runs (final 100-class I3D ~87.6%: https://wandb.ai/Sign_Bridge/Sign_Bridge/runs/bj3s5cle).
- Landmark-based models (ASL Signs) and Hyso/TGCN explorations are archived; not production.

---

## 9) Future Work (from original plan)
- Multimodal fusion: combine I3D outputs with speech (late fusion or cross-modal models).
- Landmark model based sign recognition: lightweight pose/landmark pipelines (MediaPipe/OpenPose + BiLSTM/ST-GCN/TGCN).
- Larger vocabularies and more languages: extend beyond 100 ASL glosses.
- Training framework: richer config-driven experiments, more augmentation/ablation support in `CV/training`.

---

## 10) Acknowledgements
- Built on PyTorch, FastAPI, OpenCV, MediaPipe, and related open-source libraries.
- Datasets: WLASL, ASL Citizen, Google ASL Signs (Kaggle).
- Thanks to the broader sign language research community for architectures and baselines that informed the I3D setup.

---

## 11) Troubleshooting

| Issue | Solution |
|-------|----------|
| **STT not working** | Use Chrome, allow mic permission |
| **WebRTC fails** | Check TURN credentials in `[roomId].js` |
| **LLM refine fails** | Set `OPENAI_API_KEY` or `GEMINI_API_KEY` |
| **Sign recognition slow** | Ensure HuggingFace Space is awake |

---

## 12) Tech Stack

| Layer | Technology |
|-------|------------|
| **Frontend** | Next.js 14, React, Tailwind CSS |
| **Real-time** | WebRTC, Pusher Channels |
| **Sign Model** | PyTorch I3D, HuggingFace Spaces |
| **LLM** | OpenAI GPT-3.5 / Google Gemini |
| **STT** | Web Speech API (browser) |
| **Backend** | FastAPI, Uvicorn |
| **Deploy** | Vercel, Netlify, Docker |

---

## License

See individual dataset licenses for training data. Code is provided as-is for educational purposes.
